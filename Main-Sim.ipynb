{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9773169f",
   "metadata": {},
   "source": [
    "<font color='green'>***Installation and Libraries Import***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2b60f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install flwr\n",
    "%pip install ray \n",
    "%pip install --upgrade pip\n",
    "%pip install torch torchvision matplotlib\n",
    "%pip install async-timeout\n",
    "%pip install async-numpy\n",
    "%pip install pandas\n",
    "%pip install datasets\n",
    "%pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2230f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import flwr as fl\n",
    "import torch, ray, pandas, sklearn\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "\n",
    "print(\"All modules loaded successfully!\")\n",
    "print(\"FLWR version:\", fl.__version__)\n",
    "print(\"Ray version:\", ray.__version__)\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "libraries_to_uninstall = [\n",
    "    \"tb-nightly==2.18.0a20240701\",\n",
    "    \"tensorboard==2.16.2\",\n",
    "    \"tensorboard-data-server==0.7.2\",\n",
    "    \"tensorboard-plugin-wit==1.8.1\",\n",
    "    \"tensorflow==2.16.2\",\n",
    "    \"tensorflow-io-gcs-filesystem==0.37.0\",\n",
    "    \"termcolor==2.4.0\",\n",
    "    \"terminado==0.18.1\",\n",
    "    \"tf-estimator-nightly==2.8.0.dev2021122109\",\n",
    "    \"tf_keras-nightly==2.18.0.dev2024070109\",\n",
    "    \"tf-nightly==2.18.0.dev20240626\"\n",
    "]\n",
    "for library in libraries_to_uninstall:\n",
    "    os.system(f\"pip uninstall -y {library}\")\n",
    "print(\"All modules loaded successfully!\")\n",
    "print(\"FLWR version:\", fl.__version__)\n",
    "print(\"Ray version:\", ray.__version__)\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e632d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, WeightedRandomSampler\n",
    "import flwr as fl\n",
    "from flwr.common import Metrics\n",
    "# from flwr_datasets import FederatedDataset\n",
    "from sklearn.preprocessing import MinMaxScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import csv\n",
    "import copy\n",
    "print(fl.__version__)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e02604",
   "metadata": {},
   "source": [
    "<font color='Brown'>***Constants***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE\n",
    "NUM_CLIENTS = 48 #48\n",
    "ROUNDS = 40 #40\n",
    "BATCH_SIZE = 64 #C\n",
    "LEARNING_RATE = 0.0025 #C\n",
    "EPOCHS = 3 #C\n",
    "DATA_GROUPS = 40\n",
    "BATCH_ROUND = 6\n",
    "SIZE_ROUND = int(BATCH_ROUND * BATCH_SIZE * NUM_CLIENTS)\n",
    "NUM_ATCKS = 5\n",
    "FL = True # FL or CENT\n",
    "MODE = 'DNN' #DNN or TT\n",
    "PATH = f'{MODE}-FL-{FL}-{NUM_CLIENTS}-clients-{NUM_ATCKS}-atk-{ROUNDS}-rounds-{EPOCHS}-epochs-{LEARNING_RATE}-lr-{DATA_GROUPS}-groups'\n",
    "G = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6c986d",
   "metadata": {},
   "source": [
    "<font color='Light Blue'>***Dataset Preparations***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49adbeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrafficData = {}\n",
    "TrafficData['Dataset']={}\n",
    "sets_names = ['30','100','70','50','120']\n",
    "for  DATA_NUM in sets_names:\n",
    "    TrafficData['Dataset'][DATA_NUM]=pd.read_csv(f'data/2_Dataset_{NUM_ATCKS}_Attack_{DATA_NUM}_normal.csv', low_memory=False, quoting=csv.QUOTE_NONE, on_bad_lines='skip')\n",
    "    print(DATA_NUM, TrafficData['Dataset'][DATA_NUM].shape)\n",
    "for DATA_NUM in TrafficData['Dataset']:\n",
    "    TrafficData['Dataset'][DATA_NUM]=TrafficData['Dataset'][DATA_NUM].sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf47e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrafficData['Split'] = {}\n",
    "sets_training =  ['30','100','70','50']\n",
    "for DATA_NUM in sets_training:\n",
    "    TrafficData['Split'][DATA_NUM] = np.array_split(TrafficData['Dataset'][DATA_NUM],DATA_GROUPS)\n",
    "\n",
    "TrafficData['Combined'] = pd.concat([TrafficData['Split']['30'][0], TrafficData['Split']['100'][0], TrafficData['Split']['70'][0], TrafficData['Split']['50'][0]]).reset_index(drop=True)\n",
    "for GROUP in range(1, DATA_GROUPS):\n",
    "    TrafficData['Combined'] = pd.concat([TrafficData['Combined'], TrafficData['Split']['30'][GROUP], TrafficData['Split']['100'][GROUP], TrafficData['Split']['70'][GROUP], TrafficData['Split']['50'][GROUP]]).reset_index(drop=True)\n",
    "print(TrafficData['Combined'].shape)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14ed445",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrafficData['Train'] = {}\n",
    "TrafficData['Train']['X'] = TrafficData['Combined'].iloc[:, 0:-1]\n",
    "TrafficData['Train']['y'] = TrafficData['Combined'].iloc[:, -1]\n",
    "print(TrafficData['Train']['X'].shape)\n",
    "print(TrafficData['Train']['y'].shape)\n",
    "\n",
    "TrafficData['Test'] = {}\n",
    "TrafficData['Test']['X']=TrafficData['Dataset']['120'].iloc[:, 0:-1]\n",
    "TrafficData['Test']['y']=TrafficData['Dataset']['120'].iloc[:, -1]\n",
    "print(TrafficData['Test']['X'].shape)\n",
    "print(TrafficData['Test']['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b8b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "model = scaler.fit(TrafficData['Train']['X'])\n",
    "TrafficData['Train']['X'] = model.transform(TrafficData['Train']['X'])\n",
    "TrafficData['Test']['X'] = model.transform(TrafficData['Test']['X'])\n",
    "\n",
    "TrafficData['Train']['X'], TrafficData['Train']['y']= np.array(TrafficData['Train']['X']), np.array(TrafficData['Train']['y'])\n",
    "print(type(TrafficData['Train']['X']),type(TrafficData['Train']['y']))\n",
    "print(TrafficData['Train']['X'].shape,TrafficData['Train']['y'].shape)\n",
    "TrafficData['Test']['X'], TrafficData['Test']['y']= np.array(TrafficData['Test']['X']), np.array(TrafficData['Test']['y'])\n",
    "print(type(TrafficData['Test']['X']),type(TrafficData['Test']['y']))\n",
    "print(TrafficData['Test']['X'].shape,TrafficData['Test']['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b55bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrafficData['ROUNDS']={}\n",
    "for ROUND in range(1, ROUNDS+1):\n",
    "    TrafficData['ROUNDS'][ROUND]={}\n",
    "\n",
    "SIZE_Demo = SIZE_ROUND\n",
    "for ROUND in range(1,ROUNDS+1):\n",
    "    if ROUND == 1:\n",
    "        TrafficData['ROUNDS'][ROUND]['X']= TrafficData['Train']['X'][:SIZE_Demo]\n",
    "        TrafficData['ROUNDS'][ROUND]['y']= TrafficData['Train']['y'][:SIZE_Demo]\n",
    "    else:\n",
    "        print((SIZE_Demo - SIZE_ROUND),SIZE_Demo)\n",
    "        TrafficData['ROUNDS'][ROUND]['X']= TrafficData['Train']['X'][(SIZE_Demo - SIZE_ROUND):SIZE_Demo]\n",
    "        TrafficData['ROUNDS'][ROUND]['y']= TrafficData['Train']['y'][(SIZE_Demo - SIZE_ROUND):SIZE_Demo]\n",
    "    SIZE_Demo = SIZE_Demo + SIZE_ROUND\n",
    "for ROUND in TrafficData['ROUNDS']:\n",
    "    print(\"ROUND: \", ROUND, TrafficData['ROUNDS'][ROUND]['X'].shape, TrafficData['ROUNDS'][ROUND]['y'].shape)\n",
    "print(SIZE_Demo, SIZE_ROUND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d57c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrafficData['trainsets']={}\n",
    "for ROUND in range(1, ROUNDS+1):\n",
    "    TrafficData['trainsets'][ROUND]= ClassifierDataset(torch.from_numpy(TrafficData['ROUNDS'][ROUND]['X']).float(), torch.from_numpy(TrafficData['ROUNDS'][ROUND]['y']).long())\n",
    "TrafficData['testset'] = ClassifierDataset(torch.from_numpy(TrafficData['Test']['X']).float(), torch.from_numpy(TrafficData['Test']['y']).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa457c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train(numberofclients, ROUND):    \n",
    "    # Use the actual dataset length to compute balanced portions per client\n",
    "    dataset_len = len(TrafficData['trainsets'][ROUND])\n",
    "    if dataset_len == 0:\n",
    "        # Return an empty list of loaders to avoid indexing errors downstream\n",
    "        return [DataLoader(Subset(TrafficData['trainsets'][ROUND], []), batch_size=BATCH_SIZE, shuffle=False) for _ in range(numberofclients)]\n",
    "    \n",
    "    # Distribute samples as evenly as possible across clients (handle remainders)\n",
    "    num_portions = int(numberofclients)\n",
    "    base_portion = dataset_len // num_portions\n",
    "    remainder = dataset_len % num_portions\n",
    "    portion_indices = []\n",
    "    start_idx = 0\n",
    "    for i in range(num_portions):\n",
    "        # First `remainder` clients receive one extra sample\n",
    "        sz = base_portion + (1 if i < remainder else 0)\n",
    "        end_idx = start_idx + sz\n",
    "        if sz > 0:\n",
    "            portion_indices.append(list(range(start_idx, end_idx)))\n",
    "        else:\n",
    "            portion_indices.append([])\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    # Build Subset and DataLoader for each client (safe: indices are within [0, dataset_len))\n",
    "    portion_datasets = [Subset(TrafficData['trainsets'][ROUND], indices) for indices in portion_indices]\n",
    "    portion_loaders = [DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False) for dataset in portion_datasets]            \n",
    "    return portion_loaders\n",
    "def load_test(numberofclients):    \n",
    "    testloader = DataLoader(TrafficData['testset'], batch_size=BATCH_SIZE, shuffle=False)\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataloaders = {}\n",
    "for ROUND in range(1, ROUNDS+1):\n",
    "    Dataloaders[ROUND] = load_train(NUM_CLIENTS, ROUND)\n",
    "Dataloaders['Test'] = load_test(NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedad2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: print per-client dataset sizes for the first few rounds to verify splits\n",
    "for ROUND in range(1, min(6, ROUNDS+1)):\n",
    "    loaders = Dataloaders.get(ROUND, None)\n",
    "    if loaders is None:\n",
    "        print(f\"Dataloaders for Round {ROUND} not found (run the rebuild cell).\")\n",
    "        continue\n",
    "    sizes = [len(loader.dataset) for loader in loaders]\n",
    "    print(f\"Round {ROUND}: per-client sizes (first 12): {sizes[:12]}\")\n",
    "    print(f\"  Total samples this round: {sum(sizes)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe4c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(Dataloaders['Test']):\n",
    "    batch_size = len(batch[0])  # Assuming the first element of the batch is the data\n",
    "    print(f\"Batch {i+1} size: {batch_size}\")\n",
    "    if batch_size != 64:\n",
    "        print(f\"Batch {i+1} does not contain 64 records.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87850767",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(Dataloaders[5][0]):\n",
    "    batch_size = len(batch[0])  # Assuming the first element of the batch is the data\n",
    "    print(f\"Batch {i+1} size: {batch_size}\")\n",
    "    if batch_size != 64:\n",
    "        print(f\"Batch {i+1} does not contain 64 records.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa528c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "for CLUSTER in range (1, 9):\n",
    "    DEVICE_PERCENTAGE = []\n",
    "    for DEVICE__ in range(0,NUM_CLIENTS):\n",
    "        for i, batch in enumerate(Dataloaders[CLUSTER][DEVICE__]):\n",
    "            _, labels = batch\n",
    "            class_counts = Counter(labels.numpy())\n",
    "            total_records = sum(class_counts.values())\n",
    "            class_0_count = class_counts.get(0, 0)\n",
    "            percentage_class_0 = (class_0_count / total_records) * 100\n",
    "            DEVICE_PERCENTAGE.append(percentage_class_0)\n",
    "            # print(f\"Batch {i+1}: {dict(class_counts)}\")\n",
    "            # print(f\"Percentage of class 0: {percentage_class_0:.2f}%\\n\")\n",
    "    # print(DEVICE_PERCENTAGE)        \n",
    "    chunk_size = 6\n",
    "    averages = [sum(DEVICE_PERCENTAGE[i:i + chunk_size]) / chunk_size for i in range(0, len(DEVICE_PERCENTAGE), chunk_size)]\n",
    "    # print(\"Averages of every device:\")\n",
    "    # print(averages)\n",
    "    chunk_size_4 = 4\n",
    "    averages = [sum(averages[i:i + chunk_size_4]) / chunk_size_4 for i in range(0, len(averages), chunk_size_4)]\n",
    "    # print(\"Averages of every 4 devices:\")\n",
    "    print(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99dc5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del TrafficData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ddabb9",
   "metadata": {},
   "source": [
    "<font color='Red'>***Neural Network***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b762f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()        \n",
    "        self.layer_1 = nn.Linear(98, 64)\n",
    "        self.layer_2 = nn.Linear(64, 32)\n",
    "        self.layer_out = nn.Linear(32, 15) \n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_out(x)        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b3b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random  = Net()\n",
    "# for param_tensor in Random.state_dict():\n",
    "#     print(param_tensor, \"\\t\", Random.state_dict()[param_tensor].size())\n",
    "# torch.save(Random.state_dict(), \"0_Input_Random_model_Net.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e424200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, epochs: int, verbose=True):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    net.train()\n",
    "    prediction_matrix = []\n",
    "    actual_matrix = []\n",
    "    acc_matrix = []\n",
    "    loss_matrix = []\n",
    "    # Safeguard: if the trainloader has no samples, skip training\n",
    "    try:\n",
    "        dataset_len = len(trainloader.dataset)\n",
    "    except Exception:\n",
    "        dataset_len = 0\n",
    "    if dataset_len == 0:\n",
    "        if verbose:\n",
    "            print(\"Warning: trainloader has 0 samples, skipping training.\")\n",
    "        return prediction_matrix, actual_matrix, acc_matrix, loss_matrix\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Accumulate loss as scalar sum over samples\n",
    "            epoch_loss += loss.item() * labels.size(0)\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "            predictions = torch.max(outputs.data, 1)[1]\n",
    "            prediction_matrix.append(predictions.tolist())\n",
    "            actual_matrix.append(labels.tolist())\n",
    "        # Avoid division by zero if total==0 for safety\n",
    "        if total == 0:\n",
    "            epoch_loss_val = 0.0\n",
    "            epoch_acc = 0.0\n",
    "        else:\n",
    "            epoch_loss_val = epoch_loss / total\n",
    "            epoch_acc = correct / total\n",
    "        loss_matrix.append(epoch_loss_val)\n",
    "        acc_matrix.append(epoch_acc)\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss_val:.6f} | Acc: {epoch_acc:.4f}\")\n",
    "    return prediction_matrix, actual_matrix, acc_matrix, loss_matrix\n",
    "def test(net, testloader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct, total, loss_sum = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    prediction_matrix = []\n",
    "    actual_matrix = []\n",
    "    acc_matrix = []\n",
    "    loss_matrix = []\n",
    "    try:\n",
    "        test_len = len(testloader.dataset)\n",
    "    except Exception:\n",
    "        test_len = 0\n",
    "    if test_len == 0:\n",
    "        print(\"Warning: testloader has 0 samples, skipping evaluation.\")\n",
    "        return 0.0, 0.0, prediction_matrix, actual_matrix, acc_matrix, loss_matrix\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss_sum += criterion(outputs, labels).item() * labels.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            prediction_matrix.append(predicted.tolist())\n",
    "            actual_matrix.append(labels.tolist())\n",
    "    if total == 0:\n",
    "        avg_loss = 0.0\n",
    "        accuracy = 0.0\n",
    "    else:\n",
    "        avg_loss = loss_sum / total\n",
    "        accuracy = correct / total\n",
    "    loss_matrix.append(avg_loss)\n",
    "    acc_matrix.append(accuracy)\n",
    "    print(f\"Evaluation: eval loss {avg_loss}, eval accuracy {accuracy}\")\n",
    "    return avg_loss, accuracy, prediction_matrix, actual_matrix, acc_matrix, loss_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f4a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dict= {}\n",
    "actual_dict= {}\n",
    "accuracy_dict= {}\n",
    "loss_dict= {}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

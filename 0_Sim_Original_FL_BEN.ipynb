{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcoA2oGB6Kyi",
   "metadata": {
    "editable": true,
    "id": "bcoA2oGB6Kyi",
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<font color='green'>***Installation and Libraries Import***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b0b1a7-d1b9-403f-8c98-84e601860c8f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install flwr\n",
    "# %pip install ray \n",
    "# %pip install --upgrade pip\n",
    "# %pip install torch torchvision matplotlib\n",
    "# %pip install timeout\n",
    "# %pip install numpy\n",
    "# %pip install pandas\n",
    "# %pip install datasets\n",
    "# %pip install scikit-learn\n",
    "# %pip install requests\n",
    "# %pip install transformers accelerate bitsandbytes\n",
    "# %pip install peft datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8316c4a3-e1c6-44ac-b41b-0da26527d7cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import flwr as fl\n",
    "import torch, ray, pandas, sklearn\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# print(\"All modules loaded successfully!\")\n",
    "# print(\"FLWR version:\", fl.__version__)\n",
    "# print(\"Ray version:\", ray.__version__)\n",
    "# print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "# libraries_to_uninstall = [\n",
    "#     \"tb-nightly==2.18.0a20240701\",\n",
    "#     \"tensorboard==2.16.2\",\n",
    "#     \"tensorboard-data-server==0.7.2\",\n",
    "#     \"tensorboard-plugin-wit==1.8.1\",\n",
    "#     \"tensorflow==2.16.2\",\n",
    "#     \"tensorflow-io-gcs-filesystem==0.37.0\",\n",
    "#     \"termcolor==2.4.0\",\n",
    "#     \"terminado==0.18.1\",\n",
    "#     \"tf-estimator-nightly==2.8.0.dev2021122109\",\n",
    "#     \"tf_keras-nightly==2.18.0.dev2024070109\",\n",
    "#     \"tf-nightly==2.18.0.dev20240626\"\n",
    "# ]\n",
    "# for library in libraries_to_uninstall:\n",
    "#     os.system(f\"pip uninstall -y {library}\")\n",
    "# print(\"All modules loaded successfully!\")\n",
    "# print(\"FLWR version:\", fl.__version__)\n",
    "# print(\"Ray version:\", ray.__version__)\n",
    "# print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e677784-3ccb-49de-9aa0-339fdfa926ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, WeightedRandomSampler\n",
    "import flwr as fl\n",
    "from flwr.common import Metrics\n",
    "# from flwr_datasets import FederatedDataset\n",
    "from sklearn.preprocessing import MinMaxScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import csv\n",
    "import copy\n",
    "print(fl.__version__)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fg6LBRPK-E6_",
   "metadata": {
    "id": "Fg6LBRPK-E6_",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='Brown'>***FL Constants***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZFQQhlvc-c4P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 181,
     "status": "ok",
     "timestamp": 1674161131349,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "ZFQQhlvc-c4P",
    "outputId": "b14bdc45-179f-4a11-fb12-49e6e350ee3f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DEVICE\n",
    "NUM_CLIENTS = 10 #48\n",
    "ROUNDS = 40 #40 \n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0018\n",
    "EPOCHS = 4\n",
    "DATA_GROUPS = 120\n",
    "BATCH_ROUND = 6\n",
    "SIZE_ROUND = int(BATCH_ROUND * BATCH_SIZE * NUM_CLIENTS)\n",
    "NUM_ATKS = 4\n",
    "PATH = f'LLM{NUM_ATKS}atk_{ROUNDS}_rounds_{NUM_CLIENTS}_clients_{EPOCHS}_epochs_{BATCH_SIZE}_batch_{LEARNING_RATE}_lr_{DATA_GROUPS}_data_groups/'\n",
    "G=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7b6b263-9d7b-495d-b7ec-db47c7bb4a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Quiet mode enabled — only print() outputs and LLM feedback will appear.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ✅ Quiet Mode Setup — Silence Background Logs Completely\n",
    "# ============================================================\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# -- 1️⃣ Disable parallel tokenizer & Ray telemetry noise --\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"RAY_USAGE_STATS_ENABLED\"] = \"0\"\n",
    "os.environ[\"RAY_USAGE_STATS_OUTPUT_ENABLED\"] = \"0\"\n",
    "os.environ[\"RAY_DISABLE_DASHBOARD\"] = \"1\"\n",
    "os.environ[\"RAY_BACKEND_LOG_LEVEL\"] = \"error\"\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"  # silence Hugging Face pipeline info\n",
    "\n",
    "# -- 2️⃣ Suppress all non-critical Python warnings --\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# -- 3️⃣ Silence common noisy libraries --\n",
    "for lib in [\n",
    "    \"flwr\",          # Flower federated server/client\n",
    "    \"ray\",           # Ray backend\n",
    "    \"transformers\",  # HF transformers logs\n",
    "    \"torch\",         # PyTorch initialization logs\n",
    "    \"datasets\",      # HF datasets\n",
    "    \"urllib3\",       # Networking / SSL warnings\n",
    "    \"filelock\",      # FileLock for model caching\n",
    "    \"absl\",          # TensorFlow-style logs if imported\n",
    "]:\n",
    "    logging.getLogger(lib).setLevel(logging.CRITICAL)\n",
    "\n",
    "# -- 4️⃣ Optional: quiet matplotlib/font/system noise (if present)\n",
    "logging.getLogger(\"matplotlib\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"PIL\").setLevel(logging.CRITICAL)\n",
    "\n",
    "# -- 5️⃣ Confirm quiet mode\n",
    "print(\"✅ Quiet mode enabled — only print() outputs and LLM feedback will appear.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gjVnC-rj9gC4",
   "metadata": {
    "editable": true,
    "id": "gjVnC-rj9gC4",
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<font color='Light Blue'>***Dataset Preparations***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aa6e75b-01b6-4bc4-a0e3-af41e9e15d50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 (184320, 99)\n",
      "100 (184320, 99)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m sets_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m30\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m70\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m50\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m  DATA_NUM \u001b[38;5;129;01min\u001b[39;00m sets_names:\n\u001b[0;32m----> 5\u001b[0m     TrafficData[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m][DATA_NUM]\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/2_Dataset_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mNUM_ATKS\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_Attack_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATA_NUM\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_normal.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQUOTE_NONE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_bad_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(DATA_NUM, TrafficData[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m][DATA_NUM]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m DATA_NUM \u001b[38;5;129;01min\u001b[39;00m TrafficData[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:239\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TrafficData = {}\n",
    "TrafficData['Dataset']={}\n",
    "sets_names = ['30','100','70','50','testing']\n",
    "for  DATA_NUM in sets_names:\n",
    "    TrafficData['Dataset'][DATA_NUM]=pd.read_csv(f'data/2_Dataset_{NUM_ATKS}_Attack_{DATA_NUM}_normal.csv', low_memory=False, quoting=csv.QUOTE_NONE, on_bad_lines='skip')\n",
    "    print(DATA_NUM, TrafficData['Dataset'][DATA_NUM].shape)\n",
    "for DATA_NUM in TrafficData['Dataset']:\n",
    "    TrafficData['Dataset'][DATA_NUM]=TrafficData['Dataset'][DATA_NUM].sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2857fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined shape: (737280, 99)\n"
     ]
    }
   ],
   "source": [
    "# === Combine TrafficData splits using transpose for interleaving ===\n",
    "TrafficData['Split'] = {}\n",
    "sets_training = ['30', '100', '70', '50']\n",
    "\n",
    "# Split each dataset into DATA_GROUPS\n",
    "for DATA_NUM in sets_training:\n",
    "    TrafficData['Split'][DATA_NUM] = np.array_split(TrafficData['Dataset'][DATA_NUM], DATA_GROUPS)\n",
    "\n",
    "# Transpose the grouped splits (interleave across datasets)\n",
    "# Each element of `group_set` is a tuple of slices, one from each dataset\n",
    "interleaved_groups = list(zip(\n",
    "    TrafficData['Split']['30'],\n",
    "    TrafficData['Split']['100'],\n",
    "    TrafficData['Split']['70'],\n",
    "    TrafficData['Split']['50']\n",
    "))\n",
    "\n",
    "# Concatenate groups vertically\n",
    "TrafficData['Combined'] = pd.concat(\n",
    "    [pd.concat(group_set, axis=0).reset_index(drop=True) for group_set in interleaved_groups],\n",
    "    axis=0\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"Combined shape:\", TrafficData['Combined'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d74ea7-0942-4de4-aa49-cb0c9032e148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(737280, 98)\n",
      "(737280,)\n",
      "(120000, 98)\n",
      "(120000,)\n"
     ]
    }
   ],
   "source": [
    "TrafficData['Train'] = {}\n",
    "TrafficData['Train']['X'] = TrafficData['Combined'].iloc[:, 0:-1]\n",
    "TrafficData['Train']['y'] = TrafficData['Combined'].iloc[:, -1]\n",
    "print(TrafficData['Train']['X'].shape)\n",
    "print(TrafficData['Train']['y'].shape)\n",
    "\n",
    "TrafficData['Test'] = {}\n",
    "TrafficData['Test']['X']=TrafficData['Dataset']['testing'].iloc[:, 0:-1]\n",
    "TrafficData['Test']['y']=TrafficData['Dataset']['testing'].iloc[:, -1]\n",
    "print(TrafficData['Test']['X'].shape)\n",
    "print(TrafficData['Test']['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f406c44d-9a2b-4831-a713-ce5a95134832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(737280, 98) (737280,)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(120000, 98) (120000,)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "model = scaler.fit(TrafficData['Train']['X'])\n",
    "TrafficData['Train']['X'] = model.transform(TrafficData['Train']['X'])\n",
    "TrafficData['Test']['X'] = model.transform(TrafficData['Test']['X'])\n",
    "\n",
    "TrafficData['Train']['X'], TrafficData['Train']['y']= np.array(TrafficData['Train']['X']), np.array(TrafficData['Train']['y'])\n",
    "print(type(TrafficData['Train']['X']),type(TrafficData['Train']['y']))\n",
    "print(TrafficData['Train']['X'].shape,TrafficData['Train']['y'].shape)\n",
    "TrafficData['Test']['X'], TrafficData['Test']['y']= np.array(TrafficData['Test']['X']), np.array(TrafficData['Test']['y'])\n",
    "print(type(TrafficData['Test']['X']),type(TrafficData['Test']['y']))\n",
    "print(TrafficData['Test']['X'].shape,TrafficData['Test']['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b2901-2954-4d8c-9185-8e6ac28ad965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3840 7680\n",
      "7680 11520\n",
      "11520 15360\n",
      "15360 19200\n",
      "19200 23040\n",
      "23040 26880\n",
      "26880 30720\n",
      "ROUND:  1 (3840, 98) (3840,)\n",
      "ROUND:  2 (3840, 98) (3840,)\n",
      "ROUND:  3 (3840, 98) (3840,)\n",
      "ROUND:  4 (3840, 98) (3840,)\n",
      "ROUND:  5 (3840, 98) (3840,)\n",
      "ROUND:  6 (3840, 98) (3840,)\n",
      "ROUND:  7 (3840, 98) (3840,)\n",
      "ROUND:  8 (3840, 98) (3840,)\n",
      "34560 3840\n"
     ]
    }
   ],
   "source": [
    "TrafficData['ROUNDS']={}\n",
    "for ROUND in range(1, ROUNDS+1):\n",
    "    TrafficData['ROUNDS'][ROUND]={}\n",
    "\n",
    "SIZE_Demo = SIZE_ROUND\n",
    "for ROUND in range(1,ROUNDS+1):\n",
    "    if ROUND == 1:\n",
    "        TrafficData['ROUNDS'][ROUND]['X']= TrafficData['Train']['X'][:SIZE_Demo]\n",
    "        TrafficData['ROUNDS'][ROUND]['y']= TrafficData['Train']['y'][:SIZE_Demo]\n",
    "    else:\n",
    "        print((SIZE_Demo - SIZE_ROUND),SIZE_Demo)\n",
    "        TrafficData['ROUNDS'][ROUND]['X']= TrafficData['Train']['X'][(SIZE_Demo - SIZE_ROUND):SIZE_Demo]\n",
    "        TrafficData['ROUNDS'][ROUND]['y']= TrafficData['Train']['y'][(SIZE_Demo - SIZE_ROUND):SIZE_Demo]\n",
    "    SIZE_Demo = SIZE_Demo + SIZE_ROUND\n",
    "for ROUND in TrafficData['ROUNDS']:\n",
    "    print(\"ROUND: \", ROUND, TrafficData['ROUNDS'][ROUND]['X'].shape, TrafficData['ROUNDS'][ROUND]['y'].shape)\n",
    "print(SIZE_Demo, SIZE_ROUND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f73dc-662e-441e-9361-32fbca07af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ade83a-b7fd-450d-82cb-0649af87fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrafficData['trainsets']={}\n",
    "for ROUND in range(1, ROUNDS+1):\n",
    "    TrafficData['trainsets'][ROUND]= ClassifierDataset(torch.from_numpy(TrafficData['ROUNDS'][ROUND]['X']).float(), torch.from_numpy(TrafficData['ROUNDS'][ROUND]['y']).long())\n",
    "TrafficData['testset'] = ClassifierDataset(torch.from_numpy(TrafficData['Test']['X']).float(), torch.from_numpy(TrafficData['Test']['y']).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d6a1f-95bc-4173-a8b4-8961e3d2875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train(numberofclients, ROUND):    \n",
    "    portion_size = int(BATCH_ROUND*BATCH_SIZE)\n",
    "    num_portions = int(NUM_CLIENTS)\n",
    "    portion_indices = []\n",
    "    for i in range(num_portions):\n",
    "        start_idx = i * portion_size\n",
    "        end_idx = (i + 1) * portion_size\n",
    "        portion_indices.append(list(range(start_idx, min(end_idx, SIZE_ROUND))))\n",
    "    portion_datasets = [Subset(TrafficData['trainsets'][ROUND], indices) for indices in portion_indices]\n",
    "    portion_loaders = [DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False) for dataset in portion_datasets]             \n",
    "    return portion_loaders\n",
    "def load_test(numberofclients):    \n",
    "    testloader = DataLoader(TrafficData['testset'], batch_size=BATCH_SIZE, shuffle=False)\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e2e5d8-60da-4260-80bc-4850460279c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataloaders = {}\n",
    "for ROUND in range(1, ROUNDS+1):\n",
    "    Dataloaders[ROUND] = load_train(NUM_CLIENTS, ROUND)\n",
    "Dataloaders['Test'] = load_test(NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6455bfc-e8b5-4d01-84d6-3f484a2c8ad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 size: 64\n",
      "Batch 2 size: 64\n",
      "Batch 3 size: 64\n",
      "Batch 4 size: 64\n",
      "Batch 5 size: 64\n",
      "Batch 6 size: 64\n",
      "Batch 7 size: 64\n",
      "Batch 8 size: 64\n",
      "Batch 9 size: 64\n",
      "Batch 10 size: 64\n",
      "Batch 11 size: 64\n",
      "Batch 12 size: 64\n",
      "Batch 13 size: 64\n",
      "Batch 14 size: 64\n",
      "Batch 15 size: 64\n",
      "Batch 16 size: 64\n",
      "Batch 17 size: 64\n",
      "Batch 18 size: 64\n",
      "Batch 19 size: 64\n",
      "Batch 20 size: 64\n",
      "Batch 21 size: 64\n",
      "Batch 22 size: 64\n",
      "Batch 23 size: 64\n",
      "Batch 24 size: 64\n",
      "Batch 25 size: 64\n",
      "Batch 26 size: 64\n",
      "Batch 27 size: 64\n",
      "Batch 28 size: 64\n",
      "Batch 29 size: 64\n",
      "Batch 30 size: 64\n",
      "Batch 31 size: 64\n",
      "Batch 32 size: 64\n",
      "Batch 33 size: 64\n",
      "Batch 34 size: 64\n",
      "Batch 35 size: 64\n",
      "Batch 36 size: 64\n",
      "Batch 37 size: 64\n",
      "Batch 38 size: 64\n",
      "Batch 39 size: 64\n",
      "Batch 40 size: 64\n",
      "Batch 41 size: 64\n",
      "Batch 42 size: 64\n",
      "Batch 43 size: 64\n",
      "Batch 44 size: 64\n",
      "Batch 45 size: 64\n",
      "Batch 46 size: 64\n",
      "Batch 47 size: 64\n",
      "Batch 48 size: 64\n",
      "Batch 49 size: 64\n",
      "Batch 50 size: 64\n",
      "Batch 51 size: 64\n",
      "Batch 52 size: 64\n",
      "Batch 53 size: 64\n",
      "Batch 54 size: 64\n",
      "Batch 55 size: 64\n",
      "Batch 56 size: 64\n",
      "Batch 57 size: 64\n",
      "Batch 58 size: 64\n",
      "Batch 59 size: 64\n",
      "Batch 60 size: 64\n",
      "Batch 61 size: 64\n",
      "Batch 62 size: 64\n",
      "Batch 63 size: 64\n",
      "Batch 64 size: 64\n",
      "Batch 65 size: 64\n",
      "Batch 66 size: 64\n",
      "Batch 67 size: 64\n",
      "Batch 68 size: 64\n",
      "Batch 69 size: 64\n",
      "Batch 70 size: 64\n",
      "Batch 71 size: 64\n",
      "Batch 72 size: 64\n",
      "Batch 73 size: 64\n",
      "Batch 74 size: 64\n",
      "Batch 75 size: 64\n",
      "Batch 76 size: 64\n",
      "Batch 77 size: 64\n",
      "Batch 78 size: 64\n",
      "Batch 79 size: 64\n",
      "Batch 80 size: 64\n",
      "Batch 81 size: 64\n",
      "Batch 82 size: 64\n",
      "Batch 83 size: 64\n",
      "Batch 84 size: 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 85 size: 64\n",
      "Batch 86 size: 64\n",
      "Batch 87 size: 64\n",
      "Batch 88 size: 64\n",
      "Batch 89 size: 64\n",
      "Batch 90 size: 64\n",
      "Batch 91 size: 64\n",
      "Batch 92 size: 64\n",
      "Batch 93 size: 64\n",
      "Batch 94 size: 64\n",
      "Batch 95 size: 64\n",
      "Batch 96 size: 64\n",
      "Batch 97 size: 64\n",
      "Batch 98 size: 64\n",
      "Batch 99 size: 64\n",
      "Batch 100 size: 64\n",
      "Batch 101 size: 64\n",
      "Batch 102 size: 64\n",
      "Batch 103 size: 64\n",
      "Batch 104 size: 64\n",
      "Batch 105 size: 64\n",
      "Batch 106 size: 64\n",
      "Batch 107 size: 64\n",
      "Batch 108 size: 64\n",
      "Batch 109 size: 64\n",
      "Batch 110 size: 64\n",
      "Batch 111 size: 64\n",
      "Batch 112 size: 64\n",
      "Batch 113 size: 64\n",
      "Batch 114 size: 64\n",
      "Batch 115 size: 64\n",
      "Batch 116 size: 64\n",
      "Batch 117 size: 64\n",
      "Batch 118 size: 64\n",
      "Batch 119 size: 64\n",
      "Batch 120 size: 64\n",
      "Batch 121 size: 64\n",
      "Batch 122 size: 64\n",
      "Batch 123 size: 64\n",
      "Batch 124 size: 64\n",
      "Batch 125 size: 64\n",
      "Batch 126 size: 64\n",
      "Batch 127 size: 64\n",
      "Batch 128 size: 64\n",
      "Batch 129 size: 64\n",
      "Batch 130 size: 64\n",
      "Batch 131 size: 64\n",
      "Batch 132 size: 64\n",
      "Batch 133 size: 64\n",
      "Batch 134 size: 64\n",
      "Batch 135 size: 64\n",
      "Batch 136 size: 64\n",
      "Batch 137 size: 64\n",
      "Batch 138 size: 64\n",
      "Batch 139 size: 64\n",
      "Batch 140 size: 64\n",
      "Batch 141 size: 64\n",
      "Batch 142 size: 64\n",
      "Batch 143 size: 64\n",
      "Batch 144 size: 64\n",
      "Batch 145 size: 64\n",
      "Batch 146 size: 64\n",
      "Batch 147 size: 64\n",
      "Batch 148 size: 64\n",
      "Batch 149 size: 64\n",
      "Batch 150 size: 64\n",
      "Batch 151 size: 64\n",
      "Batch 152 size: 64\n",
      "Batch 153 size: 64\n",
      "Batch 154 size: 64\n",
      "Batch 155 size: 64\n",
      "Batch 156 size: 64\n",
      "Batch 157 size: 64\n",
      "Batch 158 size: 64\n",
      "Batch 159 size: 64\n",
      "Batch 160 size: 64\n",
      "Batch 161 size: 64\n",
      "Batch 162 size: 64\n",
      "Batch 163 size: 64\n",
      "Batch 164 size: 64\n",
      "Batch 165 size: 64\n",
      "Batch 166 size: 64\n",
      "Batch 167 size: 64\n",
      "Batch 168 size: 64\n",
      "Batch 169 size: 64\n",
      "Batch 170 size: 64\n",
      "Batch 171 size: 64\n",
      "Batch 172 size: 64\n",
      "Batch 173 size: 64\n",
      "Batch 174 size: 64\n",
      "Batch 175 size: 64\n",
      "Batch 176 size: 64\n",
      "Batch 177 size: 64\n",
      "Batch 178 size: 64\n",
      "Batch 179 size: 64\n",
      "Batch 180 size: 64\n",
      "Batch 181 size: 64\n",
      "Batch 182 size: 64\n",
      "Batch 183 size: 64\n",
      "Batch 184 size: 64\n",
      "Batch 185 size: 64\n",
      "Batch 186 size: 64\n",
      "Batch 187 size: 64\n",
      "Batch 188 size: 64\n",
      "Batch 189 size: 64\n",
      "Batch 190 size: 64\n",
      "Batch 191 size: 64\n",
      "Batch 192 size: 64\n",
      "Batch 193 size: 64\n",
      "Batch 194 size: 64\n",
      "Batch 195 size: 64\n",
      "Batch 196 size: 64\n",
      "Batch 197 size: 64\n",
      "Batch 198 size: 64\n",
      "Batch 199 size: 64\n",
      "Batch 200 size: 64\n",
      "Batch 201 size: 64\n",
      "Batch 202 size: 64\n",
      "Batch 203 size: 64\n",
      "Batch 204 size: 64\n",
      "Batch 205 size: 64\n",
      "Batch 206 size: 64\n",
      "Batch 207 size: 64\n",
      "Batch 208 size: 64\n",
      "Batch 209 size: 64\n",
      "Batch 210 size: 64\n",
      "Batch 211 size: 64\n",
      "Batch 212 size: 64\n",
      "Batch 213 size: 64\n",
      "Batch 214 size: 64\n",
      "Batch 215 size: 64\n",
      "Batch 216 size: 64\n",
      "Batch 217 size: 64\n",
      "Batch 218 size: 64\n",
      "Batch 219 size: 64\n",
      "Batch 220 size: 64\n",
      "Batch 221 size: 64\n",
      "Batch 222 size: 64\n",
      "Batch 223 size: 64\n",
      "Batch 224 size: 64\n",
      "Batch 225 size: 64\n",
      "Batch 226 size: 64\n",
      "Batch 227 size: 64\n",
      "Batch 228 size: 64\n",
      "Batch 229 size: 64\n",
      "Batch 230 size: 64\n",
      "Batch 231 size: 64\n",
      "Batch 232 size: 64\n",
      "Batch 233 size: 64\n",
      "Batch 234 size: 64\n",
      "Batch 235 size: 64\n",
      "Batch 236 size: 64\n",
      "Batch 237 size: 64\n",
      "Batch 238 size: 64\n",
      "Batch 239 size: 64\n",
      "Batch 240 size: 64\n",
      "Batch 241 size: 64\n",
      "Batch 242 size: 64\n",
      "Batch 243 size: 64\n",
      "Batch 244 size: 64\n",
      "Batch 245 size: 64\n",
      "Batch 246 size: 64\n",
      "Batch 247 size: 64\n",
      "Batch 248 size: 64\n",
      "Batch 249 size: 64\n",
      "Batch 250 size: 64\n",
      "Batch 251 size: 64\n",
      "Batch 252 size: 64\n",
      "Batch 253 size: 64\n",
      "Batch 254 size: 64\n",
      "Batch 255 size: 64\n",
      "Batch 256 size: 64\n",
      "Batch 257 size: 64\n",
      "Batch 258 size: 64\n",
      "Batch 259 size: 64\n",
      "Batch 260 size: 64\n",
      "Batch 261 size: 64\n",
      "Batch 262 size: 64\n",
      "Batch 263 size: 64\n",
      "Batch 264 size: 64\n",
      "Batch 265 size: 64\n",
      "Batch 266 size: 64\n",
      "Batch 267 size: 64\n",
      "Batch 268 size: 64\n",
      "Batch 269 size: 64\n",
      "Batch 270 size: 64\n",
      "Batch 271 size: 64\n",
      "Batch 272 size: 64\n",
      "Batch 273 size: 64\n",
      "Batch 274 size: 64\n",
      "Batch 275 size: 64\n",
      "Batch 276 size: 64\n",
      "Batch 277 size: 64\n",
      "Batch 278 size: 64\n",
      "Batch 279 size: 64\n",
      "Batch 280 size: 64\n",
      "Batch 281 size: 64\n",
      "Batch 282 size: 64\n",
      "Batch 283 size: 64\n",
      "Batch 284 size: 64\n",
      "Batch 285 size: 64\n",
      "Batch 286 size: 64\n",
      "Batch 287 size: 64\n",
      "Batch 288 size: 64\n",
      "Batch 289 size: 64\n",
      "Batch 290 size: 64\n",
      "Batch 291 size: 64\n",
      "Batch 292 size: 64\n",
      "Batch 293 size: 64\n",
      "Batch 294 size: 64\n",
      "Batch 295 size: 64\n",
      "Batch 296 size: 64\n",
      "Batch 297 size: 64\n",
      "Batch 298 size: 64\n",
      "Batch 299 size: 64\n",
      "Batch 300 size: 64\n",
      "Batch 301 size: 64\n",
      "Batch 302 size: 64\n",
      "Batch 303 size: 64\n",
      "Batch 304 size: 64\n",
      "Batch 305 size: 64\n",
      "Batch 306 size: 64\n",
      "Batch 307 size: 64\n",
      "Batch 308 size: 64\n",
      "Batch 309 size: 64\n",
      "Batch 310 size: 64\n",
      "Batch 311 size: 64\n",
      "Batch 312 size: 64\n",
      "Batch 313 size: 64\n",
      "Batch 314 size: 64\n",
      "Batch 315 size: 64\n",
      "Batch 316 size: 64\n",
      "Batch 317 size: 64\n",
      "Batch 318 size: 64\n",
      "Batch 319 size: 64\n",
      "Batch 320 size: 64\n",
      "Batch 321 size: 64\n",
      "Batch 322 size: 64\n",
      "Batch 323 size: 64\n",
      "Batch 324 size: 64\n",
      "Batch 325 size: 64\n",
      "Batch 326 size: 64\n",
      "Batch 327 size: 64\n",
      "Batch 328 size: 64\n",
      "Batch 329 size: 64\n",
      "Batch 330 size: 64\n",
      "Batch 331 size: 64\n",
      "Batch 332 size: 64\n",
      "Batch 333 size: 64\n",
      "Batch 334 size: 64\n",
      "Batch 335 size: 64\n",
      "Batch 336 size: 64\n",
      "Batch 337 size: 64\n",
      "Batch 338 size: 64\n",
      "Batch 339 size: 64\n",
      "Batch 340 size: 64\n",
      "Batch 341 size: 64\n",
      "Batch 342 size: 64\n",
      "Batch 343 size: 64\n",
      "Batch 344 size: 64\n",
      "Batch 345 size: 64\n",
      "Batch 346 size: 64\n",
      "Batch 347 size: 64\n",
      "Batch 348 size: 64\n",
      "Batch 349 size: 64\n",
      "Batch 350 size: 64\n",
      "Batch 351 size: 64\n",
      "Batch 352 size: 64\n",
      "Batch 353 size: 64\n",
      "Batch 354 size: 64\n",
      "Batch 355 size: 64\n",
      "Batch 356 size: 64\n",
      "Batch 357 size: 64\n",
      "Batch 358 size: 64\n",
      "Batch 359 size: 64\n",
      "Batch 360 size: 64\n",
      "Batch 361 size: 64\n",
      "Batch 362 size: 64\n",
      "Batch 363 size: 64\n",
      "Batch 364 size: 64\n",
      "Batch 365 size: 64\n",
      "Batch 366 size: 64\n",
      "Batch 367 size: 64\n",
      "Batch 368 size: 64\n",
      "Batch 369 size: 64\n",
      "Batch 370 size: 64\n",
      "Batch 371 size: 64\n",
      "Batch 372 size: 64\n",
      "Batch 373 size: 64\n",
      "Batch 374 size: 64\n",
      "Batch 375 size: 64\n",
      "Batch 376 size: 64\n",
      "Batch 377 size: 64\n",
      "Batch 378 size: 64\n",
      "Batch 379 size: 64\n",
      "Batch 380 size: 64\n",
      "Batch 381 size: 64\n",
      "Batch 382 size: 64\n",
      "Batch 383 size: 64\n",
      "Batch 384 size: 64\n",
      "Batch 385 size: 64\n",
      "Batch 386 size: 64\n",
      "Batch 387 size: 64\n",
      "Batch 388 size: 64\n",
      "Batch 389 size: 64\n",
      "Batch 390 size: 64\n",
      "Batch 391 size: 64\n",
      "Batch 392 size: 64\n",
      "Batch 393 size: 64\n",
      "Batch 394 size: 64\n",
      "Batch 395 size: 64\n",
      "Batch 396 size: 64\n",
      "Batch 397 size: 64\n",
      "Batch 398 size: 64\n",
      "Batch 399 size: 64\n",
      "Batch 400 size: 64\n",
      "Batch 401 size: 64\n",
      "Batch 402 size: 64\n",
      "Batch 403 size: 64\n",
      "Batch 404 size: 64\n",
      "Batch 405 size: 64\n",
      "Batch 406 size: 64\n",
      "Batch 407 size: 64\n",
      "Batch 408 size: 64\n",
      "Batch 409 size: 64\n",
      "Batch 410 size: 64\n",
      "Batch 411 size: 64\n",
      "Batch 412 size: 64\n",
      "Batch 413 size: 64\n",
      "Batch 414 size: 64\n",
      "Batch 415 size: 64\n",
      "Batch 416 size: 64\n",
      "Batch 417 size: 64\n",
      "Batch 418 size: 64\n",
      "Batch 419 size: 64\n",
      "Batch 420 size: 64\n",
      "Batch 421 size: 64\n",
      "Batch 422 size: 64\n",
      "Batch 423 size: 64\n",
      "Batch 424 size: 64\n",
      "Batch 425 size: 64\n",
      "Batch 426 size: 64\n",
      "Batch 427 size: 64\n",
      "Batch 428 size: 64\n",
      "Batch 429 size: 64\n",
      "Batch 430 size: 64\n",
      "Batch 431 size: 64\n",
      "Batch 432 size: 64\n",
      "Batch 433 size: 64\n",
      "Batch 434 size: 64\n",
      "Batch 435 size: 64\n",
      "Batch 436 size: 64\n",
      "Batch 437 size: 64\n",
      "Batch 438 size: 64\n",
      "Batch 439 size: 64\n",
      "Batch 440 size: 64\n",
      "Batch 441 size: 64\n",
      "Batch 442 size: 64\n",
      "Batch 443 size: 64\n",
      "Batch 444 size: 64\n",
      "Batch 445 size: 64\n",
      "Batch 446 size: 64\n",
      "Batch 447 size: 64\n",
      "Batch 448 size: 64\n",
      "Batch 449 size: 64\n",
      "Batch 450 size: 64\n",
      "Batch 451 size: 64\n",
      "Batch 452 size: 64\n",
      "Batch 453 size: 64\n",
      "Batch 454 size: 64\n",
      "Batch 455 size: 64\n",
      "Batch 456 size: 64\n",
      "Batch 457 size: 64\n",
      "Batch 458 size: 64\n",
      "Batch 459 size: 64\n",
      "Batch 460 size: 64\n",
      "Batch 461 size: 64\n",
      "Batch 462 size: 64\n",
      "Batch 463 size: 64\n",
      "Batch 464 size: 64\n",
      "Batch 465 size: 64\n",
      "Batch 466 size: 64\n",
      "Batch 467 size: 64\n",
      "Batch 468 size: 64\n",
      "Batch 469 size: 64\n",
      "Batch 470 size: 64\n",
      "Batch 471 size: 64\n",
      "Batch 472 size: 64\n",
      "Batch 473 size: 64\n",
      "Batch 474 size: 64\n",
      "Batch 475 size: 64\n",
      "Batch 476 size: 64\n",
      "Batch 477 size: 64\n",
      "Batch 478 size: 64\n",
      "Batch 479 size: 64\n",
      "Batch 480 size: 64\n",
      "Batch 481 size: 64\n",
      "Batch 482 size: 64\n",
      "Batch 483 size: 64\n",
      "Batch 484 size: 64\n",
      "Batch 485 size: 64\n",
      "Batch 486 size: 64\n",
      "Batch 487 size: 64\n",
      "Batch 488 size: 64\n",
      "Batch 489 size: 64\n",
      "Batch 490 size: 64\n",
      "Batch 491 size: 64\n",
      "Batch 492 size: 64\n",
      "Batch 493 size: 64\n",
      "Batch 494 size: 64\n",
      "Batch 495 size: 64\n",
      "Batch 496 size: 64\n",
      "Batch 497 size: 64\n",
      "Batch 498 size: 64\n",
      "Batch 499 size: 64\n",
      "Batch 500 size: 64\n",
      "Batch 501 size: 64\n",
      "Batch 502 size: 64\n",
      "Batch 503 size: 64\n",
      "Batch 504 size: 64\n",
      "Batch 505 size: 64\n",
      "Batch 506 size: 64\n",
      "Batch 507 size: 64\n",
      "Batch 508 size: 64\n",
      "Batch 509 size: 64\n",
      "Batch 510 size: 64\n",
      "Batch 511 size: 64\n",
      "Batch 512 size: 64\n",
      "Batch 513 size: 64\n",
      "Batch 514 size: 64\n",
      "Batch 515 size: 64\n",
      "Batch 516 size: 64\n",
      "Batch 517 size: 64\n",
      "Batch 518 size: 64\n",
      "Batch 519 size: 64\n",
      "Batch 520 size: 64\n",
      "Batch 521 size: 64\n",
      "Batch 522 size: 64\n",
      "Batch 523 size: 64\n",
      "Batch 524 size: 64\n",
      "Batch 525 size: 64\n",
      "Batch 526 size: 64\n",
      "Batch 527 size: 64\n",
      "Batch 528 size: 64\n",
      "Batch 529 size: 64\n",
      "Batch 530 size: 64\n",
      "Batch 531 size: 64\n",
      "Batch 532 size: 64\n",
      "Batch 533 size: 64\n",
      "Batch 534 size: 64\n",
      "Batch 535 size: 64\n",
      "Batch 536 size: 64\n",
      "Batch 537 size: 64\n",
      "Batch 538 size: 64\n",
      "Batch 539 size: 64\n",
      "Batch 540 size: 64\n",
      "Batch 541 size: 64\n",
      "Batch 542 size: 64\n",
      "Batch 543 size: 64\n",
      "Batch 544 size: 64\n",
      "Batch 545 size: 64\n",
      "Batch 546 size: 64\n",
      "Batch 547 size: 64\n",
      "Batch 548 size: 64\n",
      "Batch 549 size: 64\n",
      "Batch 550 size: 64\n",
      "Batch 551 size: 64\n",
      "Batch 552 size: 64\n",
      "Batch 553 size: 64\n",
      "Batch 554 size: 64\n",
      "Batch 555 size: 64\n",
      "Batch 556 size: 64\n",
      "Batch 557 size: 64\n",
      "Batch 558 size: 64\n",
      "Batch 559 size: 64\n",
      "Batch 560 size: 64\n",
      "Batch 561 size: 64\n",
      "Batch 562 size: 64\n",
      "Batch 563 size: 64\n",
      "Batch 564 size: 64\n",
      "Batch 565 size: 64\n",
      "Batch 566 size: 64\n",
      "Batch 567 size: 64\n",
      "Batch 568 size: 64\n",
      "Batch 569 size: 64\n",
      "Batch 570 size: 64\n",
      "Batch 571 size: 64\n",
      "Batch 572 size: 64\n",
      "Batch 573 size: 64\n",
      "Batch 574 size: 64\n",
      "Batch 575 size: 64\n",
      "Batch 576 size: 64\n",
      "Batch 577 size: 64\n",
      "Batch 578 size: 64\n",
      "Batch 579 size: 64\n",
      "Batch 580 size: 64\n",
      "Batch 581 size: 64\n",
      "Batch 582 size: 64\n",
      "Batch 583 size: 64\n",
      "Batch 584 size: 64\n",
      "Batch 585 size: 64\n",
      "Batch 586 size: 64\n",
      "Batch 587 size: 64\n",
      "Batch 588 size: 64\n",
      "Batch 589 size: 64\n",
      "Batch 590 size: 64\n",
      "Batch 591 size: 64\n",
      "Batch 592 size: 64\n",
      "Batch 593 size: 64\n",
      "Batch 594 size: 64\n",
      "Batch 595 size: 64\n",
      "Batch 596 size: 64\n",
      "Batch 597 size: 64\n",
      "Batch 598 size: 64\n",
      "Batch 599 size: 64\n",
      "Batch 600 size: 64\n",
      "Batch 601 size: 64\n",
      "Batch 602 size: 64\n",
      "Batch 603 size: 64\n",
      "Batch 604 size: 64\n",
      "Batch 605 size: 64\n",
      "Batch 606 size: 64\n",
      "Batch 607 size: 64\n",
      "Batch 608 size: 64\n",
      "Batch 609 size: 64\n",
      "Batch 610 size: 64\n",
      "Batch 611 size: 64\n",
      "Batch 612 size: 64\n",
      "Batch 613 size: 64\n",
      "Batch 614 size: 64\n",
      "Batch 615 size: 64\n",
      "Batch 616 size: 64\n",
      "Batch 617 size: 64\n",
      "Batch 618 size: 64\n",
      "Batch 619 size: 64\n",
      "Batch 620 size: 64\n",
      "Batch 621 size: 64\n",
      "Batch 622 size: 64\n",
      "Batch 623 size: 64\n",
      "Batch 624 size: 64\n",
      "Batch 625 size: 64\n",
      "Batch 626 size: 64\n",
      "Batch 627 size: 64\n",
      "Batch 628 size: 64\n",
      "Batch 629 size: 64\n",
      "Batch 630 size: 64\n",
      "Batch 631 size: 64\n",
      "Batch 632 size: 64\n",
      "Batch 633 size: 64\n",
      "Batch 634 size: 64\n",
      "Batch 635 size: 64\n",
      "Batch 636 size: 64\n",
      "Batch 637 size: 64\n",
      "Batch 638 size: 64\n",
      "Batch 639 size: 64\n",
      "Batch 640 size: 64\n",
      "Batch 641 size: 64\n",
      "Batch 642 size: 64\n",
      "Batch 643 size: 64\n",
      "Batch 644 size: 64\n",
      "Batch 645 size: 64\n",
      "Batch 646 size: 64\n",
      "Batch 647 size: 64\n",
      "Batch 648 size: 64\n",
      "Batch 649 size: 64\n",
      "Batch 650 size: 64\n",
      "Batch 651 size: 64\n",
      "Batch 652 size: 64\n",
      "Batch 653 size: 64\n",
      "Batch 654 size: 64\n",
      "Batch 655 size: 64\n",
      "Batch 656 size: 64\n",
      "Batch 657 size: 64\n",
      "Batch 658 size: 64\n",
      "Batch 659 size: 64\n",
      "Batch 660 size: 64\n",
      "Batch 661 size: 64\n",
      "Batch 662 size: 64\n",
      "Batch 663 size: 64\n",
      "Batch 664 size: 64\n",
      "Batch 665 size: 64\n",
      "Batch 666 size: 64\n",
      "Batch 667 size: 64\n",
      "Batch 668 size: 64\n",
      "Batch 669 size: 64\n",
      "Batch 670 size: 64\n",
      "Batch 671 size: 64\n",
      "Batch 672 size: 64\n",
      "Batch 673 size: 64\n",
      "Batch 674 size: 64\n",
      "Batch 675 size: 64\n",
      "Batch 676 size: 64\n",
      "Batch 677 size: 64\n",
      "Batch 678 size: 64\n",
      "Batch 679 size: 64\n",
      "Batch 680 size: 64\n",
      "Batch 681 size: 64\n",
      "Batch 682 size: 64\n",
      "Batch 683 size: 64\n",
      "Batch 684 size: 64\n",
      "Batch 685 size: 64\n",
      "Batch 686 size: 64\n",
      "Batch 687 size: 64\n",
      "Batch 688 size: 64\n",
      "Batch 689 size: 64\n",
      "Batch 690 size: 64\n",
      "Batch 691 size: 64\n",
      "Batch 692 size: 64\n",
      "Batch 693 size: 64\n",
      "Batch 694 size: 64\n",
      "Batch 695 size: 64\n",
      "Batch 696 size: 64\n",
      "Batch 697 size: 64\n",
      "Batch 698 size: 64\n",
      "Batch 699 size: 64\n",
      "Batch 700 size: 64\n",
      "Batch 701 size: 64\n",
      "Batch 702 size: 64\n",
      "Batch 703 size: 64\n",
      "Batch 704 size: 64\n",
      "Batch 705 size: 64\n",
      "Batch 706 size: 64\n",
      "Batch 707 size: 64\n",
      "Batch 708 size: 64\n",
      "Batch 709 size: 64\n",
      "Batch 710 size: 64\n",
      "Batch 711 size: 64\n",
      "Batch 712 size: 64\n",
      "Batch 713 size: 64\n",
      "Batch 714 size: 64\n",
      "Batch 715 size: 64\n",
      "Batch 716 size: 64\n",
      "Batch 717 size: 64\n",
      "Batch 718 size: 64\n",
      "Batch 719 size: 64\n",
      "Batch 720 size: 64\n",
      "Batch 721 size: 64\n",
      "Batch 722 size: 64\n",
      "Batch 723 size: 64\n",
      "Batch 724 size: 64\n",
      "Batch 725 size: 64\n",
      "Batch 726 size: 64\n",
      "Batch 727 size: 64\n",
      "Batch 728 size: 64\n",
      "Batch 729 size: 64\n",
      "Batch 730 size: 64\n",
      "Batch 731 size: 64\n",
      "Batch 732 size: 64\n",
      "Batch 733 size: 64\n",
      "Batch 734 size: 64\n",
      "Batch 735 size: 64\n",
      "Batch 736 size: 64\n",
      "Batch 737 size: 64\n",
      "Batch 738 size: 64\n",
      "Batch 739 size: 64\n",
      "Batch 740 size: 64\n",
      "Batch 741 size: 64\n",
      "Batch 742 size: 64\n",
      "Batch 743 size: 64\n",
      "Batch 744 size: 64\n",
      "Batch 745 size: 64\n",
      "Batch 746 size: 64\n",
      "Batch 747 size: 64\n",
      "Batch 748 size: 64\n",
      "Batch 749 size: 64\n",
      "Batch 750 size: 64\n",
      "Batch 751 size: 64\n",
      "Batch 752 size: 64\n",
      "Batch 753 size: 64\n",
      "Batch 754 size: 64\n",
      "Batch 755 size: 64\n",
      "Batch 756 size: 64\n",
      "Batch 757 size: 64\n",
      "Batch 758 size: 64\n",
      "Batch 759 size: 64\n",
      "Batch 760 size: 64\n",
      "Batch 761 size: 64\n",
      "Batch 762 size: 64\n",
      "Batch 763 size: 64\n",
      "Batch 764 size: 64\n",
      "Batch 765 size: 64\n",
      "Batch 766 size: 64\n",
      "Batch 767 size: 64\n",
      "Batch 768 size: 64\n",
      "Batch 769 size: 64\n",
      "Batch 770 size: 64\n",
      "Batch 771 size: 64\n",
      "Batch 772 size: 64\n",
      "Batch 773 size: 64\n",
      "Batch 774 size: 64\n",
      "Batch 775 size: 64\n",
      "Batch 776 size: 64\n",
      "Batch 777 size: 64\n",
      "Batch 778 size: 64\n",
      "Batch 779 size: 64\n",
      "Batch 780 size: 64\n",
      "Batch 781 size: 64\n",
      "Batch 782 size: 64\n",
      "Batch 783 size: 64\n",
      "Batch 784 size: 64\n",
      "Batch 785 size: 64\n",
      "Batch 786 size: 64\n",
      "Batch 787 size: 64\n",
      "Batch 788 size: 64\n",
      "Batch 789 size: 64\n",
      "Batch 790 size: 64\n",
      "Batch 791 size: 64\n",
      "Batch 792 size: 64\n",
      "Batch 793 size: 64\n",
      "Batch 794 size: 64\n",
      "Batch 795 size: 64\n",
      "Batch 796 size: 64\n",
      "Batch 797 size: 64\n",
      "Batch 798 size: 64\n",
      "Batch 799 size: 64\n",
      "Batch 800 size: 64\n",
      "Batch 801 size: 64\n",
      "Batch 802 size: 64\n",
      "Batch 803 size: 64\n",
      "Batch 804 size: 64\n",
      "Batch 805 size: 64\n",
      "Batch 806 size: 64\n",
      "Batch 807 size: 64\n",
      "Batch 808 size: 64\n",
      "Batch 809 size: 64\n",
      "Batch 810 size: 64\n",
      "Batch 811 size: 64\n",
      "Batch 812 size: 64\n",
      "Batch 813 size: 64\n",
      "Batch 814 size: 64\n",
      "Batch 815 size: 64\n",
      "Batch 816 size: 64\n",
      "Batch 817 size: 64\n",
      "Batch 818 size: 64\n",
      "Batch 819 size: 64\n",
      "Batch 820 size: 64\n",
      "Batch 821 size: 64\n",
      "Batch 822 size: 64\n",
      "Batch 823 size: 64\n",
      "Batch 824 size: 64\n",
      "Batch 825 size: 64\n",
      "Batch 826 size: 64\n",
      "Batch 827 size: 64\n",
      "Batch 828 size: 64\n",
      "Batch 829 size: 64\n",
      "Batch 830 size: 64\n",
      "Batch 831 size: 64\n",
      "Batch 832 size: 64\n",
      "Batch 833 size: 64\n",
      "Batch 834 size: 64\n",
      "Batch 835 size: 64\n",
      "Batch 836 size: 64\n",
      "Batch 837 size: 64\n",
      "Batch 838 size: 64\n",
      "Batch 839 size: 64\n",
      "Batch 840 size: 64\n",
      "Batch 841 size: 64\n",
      "Batch 842 size: 64\n",
      "Batch 843 size: 64\n",
      "Batch 844 size: 64\n",
      "Batch 845 size: 64\n",
      "Batch 846 size: 64\n",
      "Batch 847 size: 64\n",
      "Batch 848 size: 64\n",
      "Batch 849 size: 64\n",
      "Batch 850 size: 64\n",
      "Batch 851 size: 64\n",
      "Batch 852 size: 64\n",
      "Batch 853 size: 64\n",
      "Batch 854 size: 64\n",
      "Batch 855 size: 64\n",
      "Batch 856 size: 64\n",
      "Batch 857 size: 64\n",
      "Batch 858 size: 64\n",
      "Batch 859 size: 64\n",
      "Batch 860 size: 64\n",
      "Batch 861 size: 64\n",
      "Batch 862 size: 64\n",
      "Batch 863 size: 64\n",
      "Batch 864 size: 64\n",
      "Batch 865 size: 64\n",
      "Batch 866 size: 64\n",
      "Batch 867 size: 64\n",
      "Batch 868 size: 64\n",
      "Batch 869 size: 64\n",
      "Batch 870 size: 64\n",
      "Batch 871 size: 64\n",
      "Batch 872 size: 64\n",
      "Batch 873 size: 64\n",
      "Batch 874 size: 64\n",
      "Batch 875 size: 64\n",
      "Batch 876 size: 64\n",
      "Batch 877 size: 64\n",
      "Batch 878 size: 64\n",
      "Batch 879 size: 64\n",
      "Batch 880 size: 64\n",
      "Batch 881 size: 64\n",
      "Batch 882 size: 64\n",
      "Batch 883 size: 64\n",
      "Batch 884 size: 64\n",
      "Batch 885 size: 64\n",
      "Batch 886 size: 64\n",
      "Batch 887 size: 64\n",
      "Batch 888 size: 64\n",
      "Batch 889 size: 64\n",
      "Batch 890 size: 64\n",
      "Batch 891 size: 64\n",
      "Batch 892 size: 64\n",
      "Batch 893 size: 64\n",
      "Batch 894 size: 64\n",
      "Batch 895 size: 64\n",
      "Batch 896 size: 64\n",
      "Batch 897 size: 64\n",
      "Batch 898 size: 64\n",
      "Batch 899 size: 64\n",
      "Batch 900 size: 64\n",
      "Batch 901 size: 64\n",
      "Batch 902 size: 64\n",
      "Batch 903 size: 64\n",
      "Batch 904 size: 64\n",
      "Batch 905 size: 64\n",
      "Batch 906 size: 64\n",
      "Batch 907 size: 64\n",
      "Batch 908 size: 64\n",
      "Batch 909 size: 64\n",
      "Batch 910 size: 64\n",
      "Batch 911 size: 64\n",
      "Batch 912 size: 64\n",
      "Batch 913 size: 64\n",
      "Batch 914 size: 64\n",
      "Batch 915 size: 64\n",
      "Batch 916 size: 64\n",
      "Batch 917 size: 64\n",
      "Batch 918 size: 64\n",
      "Batch 919 size: 64\n",
      "Batch 920 size: 64\n",
      "Batch 921 size: 64\n",
      "Batch 922 size: 64\n",
      "Batch 923 size: 64\n",
      "Batch 924 size: 64\n",
      "Batch 925 size: 64\n",
      "Batch 926 size: 64\n",
      "Batch 927 size: 64\n",
      "Batch 928 size: 64\n",
      "Batch 929 size: 64\n",
      "Batch 930 size: 64\n",
      "Batch 931 size: 64\n",
      "Batch 932 size: 64\n",
      "Batch 933 size: 64\n",
      "Batch 934 size: 64\n",
      "Batch 935 size: 64\n",
      "Batch 936 size: 64\n",
      "Batch 937 size: 64\n",
      "Batch 938 size: 64\n",
      "Batch 939 size: 64\n",
      "Batch 940 size: 64\n",
      "Batch 941 size: 64\n",
      "Batch 942 size: 64\n",
      "Batch 943 size: 64\n",
      "Batch 944 size: 64\n",
      "Batch 945 size: 64\n",
      "Batch 946 size: 64\n",
      "Batch 947 size: 64\n",
      "Batch 948 size: 64\n",
      "Batch 949 size: 64\n",
      "Batch 950 size: 64\n",
      "Batch 951 size: 64\n",
      "Batch 952 size: 64\n",
      "Batch 953 size: 64\n",
      "Batch 954 size: 64\n",
      "Batch 955 size: 64\n",
      "Batch 956 size: 64\n",
      "Batch 957 size: 64\n",
      "Batch 958 size: 64\n",
      "Batch 959 size: 64\n",
      "Batch 960 size: 64\n",
      "Batch 961 size: 64\n",
      "Batch 962 size: 64\n",
      "Batch 963 size: 64\n",
      "Batch 964 size: 64\n",
      "Batch 965 size: 64\n",
      "Batch 966 size: 64\n",
      "Batch 967 size: 64\n",
      "Batch 968 size: 64\n",
      "Batch 969 size: 64\n",
      "Batch 970 size: 64\n",
      "Batch 971 size: 64\n",
      "Batch 972 size: 64\n",
      "Batch 973 size: 64\n",
      "Batch 974 size: 64\n",
      "Batch 975 size: 64\n",
      "Batch 976 size: 64\n",
      "Batch 977 size: 64\n",
      "Batch 978 size: 64\n",
      "Batch 979 size: 64\n",
      "Batch 980 size: 64\n",
      "Batch 981 size: 64\n",
      "Batch 982 size: 64\n",
      "Batch 983 size: 64\n",
      "Batch 984 size: 64\n",
      "Batch 985 size: 64\n",
      "Batch 986 size: 64\n",
      "Batch 987 size: 64\n",
      "Batch 988 size: 64\n",
      "Batch 989 size: 64\n",
      "Batch 990 size: 64\n",
      "Batch 991 size: 64\n",
      "Batch 992 size: 64\n",
      "Batch 993 size: 64\n",
      "Batch 994 size: 64\n",
      "Batch 995 size: 64\n",
      "Batch 996 size: 64\n",
      "Batch 997 size: 64\n",
      "Batch 998 size: 64\n",
      "Batch 999 size: 64\n",
      "Batch 1000 size: 64\n",
      "Batch 1001 size: 64\n",
      "Batch 1002 size: 64\n",
      "Batch 1003 size: 64\n",
      "Batch 1004 size: 64\n",
      "Batch 1005 size: 64\n",
      "Batch 1006 size: 64\n",
      "Batch 1007 size: 64\n",
      "Batch 1008 size: 64\n",
      "Batch 1009 size: 64\n",
      "Batch 1010 size: 64\n",
      "Batch 1011 size: 64\n",
      "Batch 1012 size: 64\n",
      "Batch 1013 size: 64\n",
      "Batch 1014 size: 64\n",
      "Batch 1015 size: 64\n",
      "Batch 1016 size: 64\n",
      "Batch 1017 size: 64\n",
      "Batch 1018 size: 64\n",
      "Batch 1019 size: 64\n",
      "Batch 1020 size: 64\n",
      "Batch 1021 size: 64\n",
      "Batch 1022 size: 64\n",
      "Batch 1023 size: 64\n",
      "Batch 1024 size: 64\n",
      "Batch 1025 size: 64\n",
      "Batch 1026 size: 64\n",
      "Batch 1027 size: 64\n",
      "Batch 1028 size: 64\n",
      "Batch 1029 size: 64\n",
      "Batch 1030 size: 64\n",
      "Batch 1031 size: 64\n",
      "Batch 1032 size: 64\n",
      "Batch 1033 size: 64\n",
      "Batch 1034 size: 64\n",
      "Batch 1035 size: 64\n",
      "Batch 1036 size: 64\n",
      "Batch 1037 size: 64\n",
      "Batch 1038 size: 64\n",
      "Batch 1039 size: 64\n",
      "Batch 1040 size: 64\n",
      "Batch 1041 size: 64\n",
      "Batch 1042 size: 64\n",
      "Batch 1043 size: 64\n",
      "Batch 1044 size: 64\n",
      "Batch 1045 size: 64\n",
      "Batch 1046 size: 64\n",
      "Batch 1047 size: 64\n",
      "Batch 1048 size: 64\n",
      "Batch 1049 size: 64\n",
      "Batch 1050 size: 64\n",
      "Batch 1051 size: 64\n",
      "Batch 1052 size: 64\n",
      "Batch 1053 size: 64\n",
      "Batch 1054 size: 64\n",
      "Batch 1055 size: 64\n",
      "Batch 1056 size: 64\n",
      "Batch 1057 size: 64\n",
      "Batch 1058 size: 64\n",
      "Batch 1059 size: 64\n",
      "Batch 1060 size: 64\n",
      "Batch 1061 size: 64\n",
      "Batch 1062 size: 64\n",
      "Batch 1063 size: 64\n",
      "Batch 1064 size: 64\n",
      "Batch 1065 size: 64\n",
      "Batch 1066 size: 64\n",
      "Batch 1067 size: 64\n",
      "Batch 1068 size: 64\n",
      "Batch 1069 size: 64\n",
      "Batch 1070 size: 64\n",
      "Batch 1071 size: 64\n",
      "Batch 1072 size: 64\n",
      "Batch 1073 size: 64\n",
      "Batch 1074 size: 64\n",
      "Batch 1075 size: 64\n",
      "Batch 1076 size: 64\n",
      "Batch 1077 size: 64\n",
      "Batch 1078 size: 64\n",
      "Batch 1079 size: 64\n",
      "Batch 1080 size: 64\n",
      "Batch 1081 size: 64\n",
      "Batch 1082 size: 64\n",
      "Batch 1083 size: 64\n",
      "Batch 1084 size: 64\n",
      "Batch 1085 size: 64\n",
      "Batch 1086 size: 64\n",
      "Batch 1087 size: 64\n",
      "Batch 1088 size: 64\n",
      "Batch 1089 size: 64\n",
      "Batch 1090 size: 64\n",
      "Batch 1091 size: 64\n",
      "Batch 1092 size: 64\n",
      "Batch 1093 size: 64\n",
      "Batch 1094 size: 64\n",
      "Batch 1095 size: 64\n",
      "Batch 1096 size: 64\n",
      "Batch 1097 size: 64\n",
      "Batch 1098 size: 64\n",
      "Batch 1099 size: 64\n",
      "Batch 1100 size: 64\n",
      "Batch 1101 size: 64\n",
      "Batch 1102 size: 64\n",
      "Batch 1103 size: 64\n",
      "Batch 1104 size: 64\n",
      "Batch 1105 size: 64\n",
      "Batch 1106 size: 64\n",
      "Batch 1107 size: 64\n",
      "Batch 1108 size: 64\n",
      "Batch 1109 size: 64\n",
      "Batch 1110 size: 64\n",
      "Batch 1111 size: 64\n",
      "Batch 1112 size: 64\n",
      "Batch 1113 size: 64\n",
      "Batch 1114 size: 64\n",
      "Batch 1115 size: 64\n",
      "Batch 1116 size: 64\n",
      "Batch 1117 size: 64\n",
      "Batch 1118 size: 64\n",
      "Batch 1119 size: 64\n",
      "Batch 1120 size: 64\n",
      "Batch 1121 size: 64\n",
      "Batch 1122 size: 64\n",
      "Batch 1123 size: 64\n",
      "Batch 1124 size: 64\n",
      "Batch 1125 size: 64\n",
      "Batch 1126 size: 64\n",
      "Batch 1127 size: 64\n",
      "Batch 1128 size: 64\n",
      "Batch 1129 size: 64\n",
      "Batch 1130 size: 64\n",
      "Batch 1131 size: 64\n",
      "Batch 1132 size: 64\n",
      "Batch 1133 size: 64\n",
      "Batch 1134 size: 64\n",
      "Batch 1135 size: 64\n",
      "Batch 1136 size: 64\n",
      "Batch 1137 size: 64\n",
      "Batch 1138 size: 64\n",
      "Batch 1139 size: 64\n",
      "Batch 1140 size: 64\n",
      "Batch 1141 size: 64\n",
      "Batch 1142 size: 64\n",
      "Batch 1143 size: 64\n",
      "Batch 1144 size: 64\n",
      "Batch 1145 size: 64\n",
      "Batch 1146 size: 64\n",
      "Batch 1147 size: 64\n",
      "Batch 1148 size: 64\n",
      "Batch 1149 size: 64\n",
      "Batch 1150 size: 64\n",
      "Batch 1151 size: 64\n",
      "Batch 1152 size: 64\n",
      "Batch 1153 size: 64\n",
      "Batch 1154 size: 64\n",
      "Batch 1155 size: 64\n",
      "Batch 1156 size: 64\n",
      "Batch 1157 size: 64\n",
      "Batch 1158 size: 64\n",
      "Batch 1159 size: 64\n",
      "Batch 1160 size: 64\n",
      "Batch 1161 size: 64\n",
      "Batch 1162 size: 64\n",
      "Batch 1163 size: 64\n",
      "Batch 1164 size: 64\n",
      "Batch 1165 size: 64\n",
      "Batch 1166 size: 64\n",
      "Batch 1167 size: 64\n",
      "Batch 1168 size: 64\n",
      "Batch 1169 size: 64\n",
      "Batch 1170 size: 64\n",
      "Batch 1171 size: 64\n",
      "Batch 1172 size: 64\n",
      "Batch 1173 size: 64\n",
      "Batch 1174 size: 64\n",
      "Batch 1175 size: 64\n",
      "Batch 1176 size: 64\n",
      "Batch 1177 size: 64\n",
      "Batch 1178 size: 64\n",
      "Batch 1179 size: 64\n",
      "Batch 1180 size: 64\n",
      "Batch 1181 size: 64\n",
      "Batch 1182 size: 64\n",
      "Batch 1183 size: 64\n",
      "Batch 1184 size: 64\n",
      "Batch 1185 size: 64\n",
      "Batch 1186 size: 64\n",
      "Batch 1187 size: 64\n",
      "Batch 1188 size: 64\n",
      "Batch 1189 size: 64\n",
      "Batch 1190 size: 64\n",
      "Batch 1191 size: 64\n",
      "Batch 1192 size: 64\n",
      "Batch 1193 size: 64\n",
      "Batch 1194 size: 64\n",
      "Batch 1195 size: 64\n",
      "Batch 1196 size: 64\n",
      "Batch 1197 size: 64\n",
      "Batch 1198 size: 64\n",
      "Batch 1199 size: 64\n",
      "Batch 1200 size: 64\n",
      "Batch 1201 size: 64\n",
      "Batch 1202 size: 64\n",
      "Batch 1203 size: 64\n",
      "Batch 1204 size: 64\n",
      "Batch 1205 size: 64\n",
      "Batch 1206 size: 64\n",
      "Batch 1207 size: 64\n",
      "Batch 1208 size: 64\n",
      "Batch 1209 size: 64\n",
      "Batch 1210 size: 64\n",
      "Batch 1211 size: 64\n",
      "Batch 1212 size: 64\n",
      "Batch 1213 size: 64\n",
      "Batch 1214 size: 64\n",
      "Batch 1215 size: 64\n",
      "Batch 1216 size: 64\n",
      "Batch 1217 size: 64\n",
      "Batch 1218 size: 64\n",
      "Batch 1219 size: 64\n",
      "Batch 1220 size: 64\n",
      "Batch 1221 size: 64\n",
      "Batch 1222 size: 64\n",
      "Batch 1223 size: 64\n",
      "Batch 1224 size: 64\n",
      "Batch 1225 size: 64\n",
      "Batch 1226 size: 64\n",
      "Batch 1227 size: 64\n",
      "Batch 1228 size: 64\n",
      "Batch 1229 size: 64\n",
      "Batch 1230 size: 64\n",
      "Batch 1231 size: 64\n",
      "Batch 1232 size: 64\n",
      "Batch 1233 size: 64\n",
      "Batch 1234 size: 64\n",
      "Batch 1235 size: 64\n",
      "Batch 1236 size: 64\n",
      "Batch 1237 size: 64\n",
      "Batch 1238 size: 64\n",
      "Batch 1239 size: 64\n",
      "Batch 1240 size: 64\n",
      "Batch 1241 size: 64\n",
      "Batch 1242 size: 64\n",
      "Batch 1243 size: 64\n",
      "Batch 1244 size: 64\n",
      "Batch 1245 size: 64\n",
      "Batch 1246 size: 64\n",
      "Batch 1247 size: 64\n",
      "Batch 1248 size: 64\n",
      "Batch 1249 size: 64\n",
      "Batch 1250 size: 64\n",
      "Batch 1251 size: 64\n",
      "Batch 1252 size: 64\n",
      "Batch 1253 size: 64\n",
      "Batch 1254 size: 64\n",
      "Batch 1255 size: 64\n",
      "Batch 1256 size: 64\n",
      "Batch 1257 size: 64\n",
      "Batch 1258 size: 64\n",
      "Batch 1259 size: 64\n",
      "Batch 1260 size: 64\n",
      "Batch 1261 size: 64\n",
      "Batch 1262 size: 64\n",
      "Batch 1263 size: 64\n",
      "Batch 1264 size: 64\n",
      "Batch 1265 size: 64\n",
      "Batch 1266 size: 64\n",
      "Batch 1267 size: 64\n",
      "Batch 1268 size: 64\n",
      "Batch 1269 size: 64\n",
      "Batch 1270 size: 64\n",
      "Batch 1271 size: 64\n",
      "Batch 1272 size: 64\n",
      "Batch 1273 size: 64\n",
      "Batch 1274 size: 64\n",
      "Batch 1275 size: 64\n",
      "Batch 1276 size: 64\n",
      "Batch 1277 size: 64\n",
      "Batch 1278 size: 64\n",
      "Batch 1279 size: 64\n",
      "Batch 1280 size: 64\n",
      "Batch 1281 size: 64\n",
      "Batch 1282 size: 64\n",
      "Batch 1283 size: 64\n",
      "Batch 1284 size: 64\n",
      "Batch 1285 size: 64\n",
      "Batch 1286 size: 64\n",
      "Batch 1287 size: 64\n",
      "Batch 1288 size: 64\n",
      "Batch 1289 size: 64\n",
      "Batch 1290 size: 64\n",
      "Batch 1291 size: 64\n",
      "Batch 1292 size: 64\n",
      "Batch 1293 size: 64\n",
      "Batch 1294 size: 64\n",
      "Batch 1295 size: 64\n",
      "Batch 1296 size: 64\n",
      "Batch 1297 size: 64\n",
      "Batch 1298 size: 64\n",
      "Batch 1299 size: 64\n",
      "Batch 1300 size: 64\n",
      "Batch 1301 size: 64\n",
      "Batch 1302 size: 64\n",
      "Batch 1303 size: 64\n",
      "Batch 1304 size: 64\n",
      "Batch 1305 size: 64\n",
      "Batch 1306 size: 64\n",
      "Batch 1307 size: 64\n",
      "Batch 1308 size: 64\n",
      "Batch 1309 size: 64\n",
      "Batch 1310 size: 64\n",
      "Batch 1311 size: 64\n",
      "Batch 1312 size: 64\n",
      "Batch 1313 size: 64\n",
      "Batch 1314 size: 64\n",
      "Batch 1315 size: 64\n",
      "Batch 1316 size: 64\n",
      "Batch 1317 size: 64\n",
      "Batch 1318 size: 64\n",
      "Batch 1319 size: 64\n",
      "Batch 1320 size: 64\n",
      "Batch 1321 size: 64\n",
      "Batch 1322 size: 64\n",
      "Batch 1323 size: 64\n",
      "Batch 1324 size: 64\n",
      "Batch 1325 size: 64\n",
      "Batch 1326 size: 64\n",
      "Batch 1327 size: 64\n",
      "Batch 1328 size: 64\n",
      "Batch 1329 size: 64\n",
      "Batch 1330 size: 64\n",
      "Batch 1331 size: 64\n",
      "Batch 1332 size: 64\n",
      "Batch 1333 size: 64\n",
      "Batch 1334 size: 64\n",
      "Batch 1335 size: 64\n",
      "Batch 1336 size: 64\n",
      "Batch 1337 size: 64\n",
      "Batch 1338 size: 64\n",
      "Batch 1339 size: 64\n",
      "Batch 1340 size: 64\n",
      "Batch 1341 size: 64\n",
      "Batch 1342 size: 64\n",
      "Batch 1343 size: 64\n",
      "Batch 1344 size: 64\n",
      "Batch 1345 size: 64\n",
      "Batch 1346 size: 64\n",
      "Batch 1347 size: 64\n",
      "Batch 1348 size: 64\n",
      "Batch 1349 size: 64\n",
      "Batch 1350 size: 64\n",
      "Batch 1351 size: 64\n",
      "Batch 1352 size: 64\n",
      "Batch 1353 size: 64\n",
      "Batch 1354 size: 64\n",
      "Batch 1355 size: 64\n",
      "Batch 1356 size: 64\n",
      "Batch 1357 size: 64\n",
      "Batch 1358 size: 64\n",
      "Batch 1359 size: 64\n",
      "Batch 1360 size: 64\n",
      "Batch 1361 size: 64\n",
      "Batch 1362 size: 64\n",
      "Batch 1363 size: 64\n",
      "Batch 1364 size: 64\n",
      "Batch 1365 size: 64\n",
      "Batch 1366 size: 64\n",
      "Batch 1367 size: 64\n",
      "Batch 1368 size: 64\n",
      "Batch 1369 size: 64\n",
      "Batch 1370 size: 64\n",
      "Batch 1371 size: 64\n",
      "Batch 1372 size: 64\n",
      "Batch 1373 size: 64\n",
      "Batch 1374 size: 64\n",
      "Batch 1375 size: 64\n",
      "Batch 1376 size: 64\n",
      "Batch 1377 size: 64\n",
      "Batch 1378 size: 64\n",
      "Batch 1379 size: 64\n",
      "Batch 1380 size: 64\n",
      "Batch 1381 size: 64\n",
      "Batch 1382 size: 64\n",
      "Batch 1383 size: 64\n",
      "Batch 1384 size: 64\n",
      "Batch 1385 size: 64\n",
      "Batch 1386 size: 64\n",
      "Batch 1387 size: 64\n",
      "Batch 1388 size: 64\n",
      "Batch 1389 size: 64\n",
      "Batch 1390 size: 64\n",
      "Batch 1391 size: 64\n",
      "Batch 1392 size: 64\n",
      "Batch 1393 size: 64\n",
      "Batch 1394 size: 64\n",
      "Batch 1395 size: 64\n",
      "Batch 1396 size: 64\n",
      "Batch 1397 size: 64\n",
      "Batch 1398 size: 64\n",
      "Batch 1399 size: 64\n",
      "Batch 1400 size: 64\n",
      "Batch 1401 size: 64\n",
      "Batch 1402 size: 64\n",
      "Batch 1403 size: 64\n",
      "Batch 1404 size: 64\n",
      "Batch 1405 size: 64\n",
      "Batch 1406 size: 64\n",
      "Batch 1407 size: 64\n",
      "Batch 1408 size: 64\n",
      "Batch 1409 size: 64\n",
      "Batch 1410 size: 64\n",
      "Batch 1411 size: 64\n",
      "Batch 1412 size: 64\n",
      "Batch 1413 size: 64\n",
      "Batch 1414 size: 64\n",
      "Batch 1415 size: 64\n",
      "Batch 1416 size: 64\n",
      "Batch 1417 size: 64\n",
      "Batch 1418 size: 64\n",
      "Batch 1419 size: 64\n",
      "Batch 1420 size: 64\n",
      "Batch 1421 size: 64\n",
      "Batch 1422 size: 64\n",
      "Batch 1423 size: 64\n",
      "Batch 1424 size: 64\n",
      "Batch 1425 size: 64\n",
      "Batch 1426 size: 64\n",
      "Batch 1427 size: 64\n",
      "Batch 1428 size: 64\n",
      "Batch 1429 size: 64\n",
      "Batch 1430 size: 64\n",
      "Batch 1431 size: 64\n",
      "Batch 1432 size: 64\n",
      "Batch 1433 size: 64\n",
      "Batch 1434 size: 64\n",
      "Batch 1435 size: 64\n",
      "Batch 1436 size: 64\n",
      "Batch 1437 size: 64\n",
      "Batch 1438 size: 64\n",
      "Batch 1439 size: 64\n",
      "Batch 1440 size: 64\n",
      "Batch 1441 size: 64\n",
      "Batch 1442 size: 64\n",
      "Batch 1443 size: 64\n",
      "Batch 1444 size: 64\n",
      "Batch 1445 size: 64\n",
      "Batch 1446 size: 64\n",
      "Batch 1447 size: 64\n",
      "Batch 1448 size: 64\n",
      "Batch 1449 size: 64\n",
      "Batch 1450 size: 64\n",
      "Batch 1451 size: 64\n",
      "Batch 1452 size: 64\n",
      "Batch 1453 size: 64\n",
      "Batch 1454 size: 64\n",
      "Batch 1455 size: 64\n",
      "Batch 1456 size: 64\n",
      "Batch 1457 size: 64\n",
      "Batch 1458 size: 64\n",
      "Batch 1459 size: 64\n",
      "Batch 1460 size: 64\n",
      "Batch 1461 size: 64\n",
      "Batch 1462 size: 64\n",
      "Batch 1463 size: 64\n",
      "Batch 1464 size: 64\n",
      "Batch 1465 size: 64\n",
      "Batch 1466 size: 64\n",
      "Batch 1467 size: 64\n",
      "Batch 1468 size: 64\n",
      "Batch 1469 size: 64\n",
      "Batch 1470 size: 64\n",
      "Batch 1471 size: 64\n",
      "Batch 1472 size: 64\n",
      "Batch 1473 size: 64\n",
      "Batch 1474 size: 64\n",
      "Batch 1475 size: 64\n",
      "Batch 1476 size: 64\n",
      "Batch 1477 size: 64\n",
      "Batch 1478 size: 64\n",
      "Batch 1479 size: 64\n",
      "Batch 1480 size: 64\n",
      "Batch 1481 size: 64\n",
      "Batch 1482 size: 64\n",
      "Batch 1483 size: 64\n",
      "Batch 1484 size: 64\n",
      "Batch 1485 size: 64\n",
      "Batch 1486 size: 64\n",
      "Batch 1487 size: 64\n",
      "Batch 1488 size: 64\n",
      "Batch 1489 size: 64\n",
      "Batch 1490 size: 64\n",
      "Batch 1491 size: 64\n",
      "Batch 1492 size: 64\n",
      "Batch 1493 size: 64\n",
      "Batch 1494 size: 64\n",
      "Batch 1495 size: 64\n",
      "Batch 1496 size: 64\n",
      "Batch 1497 size: 64\n",
      "Batch 1498 size: 64\n",
      "Batch 1499 size: 64\n",
      "Batch 1500 size: 64\n",
      "Batch 1501 size: 64\n",
      "Batch 1502 size: 64\n",
      "Batch 1503 size: 64\n",
      "Batch 1504 size: 64\n",
      "Batch 1505 size: 64\n",
      "Batch 1506 size: 64\n",
      "Batch 1507 size: 64\n",
      "Batch 1508 size: 64\n",
      "Batch 1509 size: 64\n",
      "Batch 1510 size: 64\n",
      "Batch 1511 size: 64\n",
      "Batch 1512 size: 64\n",
      "Batch 1513 size: 64\n",
      "Batch 1514 size: 64\n",
      "Batch 1515 size: 64\n",
      "Batch 1516 size: 64\n",
      "Batch 1517 size: 64\n",
      "Batch 1518 size: 64\n",
      "Batch 1519 size: 64\n",
      "Batch 1520 size: 64\n",
      "Batch 1521 size: 64\n",
      "Batch 1522 size: 64\n",
      "Batch 1523 size: 64\n",
      "Batch 1524 size: 64\n",
      "Batch 1525 size: 64\n",
      "Batch 1526 size: 64\n",
      "Batch 1527 size: 64\n",
      "Batch 1528 size: 64\n",
      "Batch 1529 size: 64\n",
      "Batch 1530 size: 64\n",
      "Batch 1531 size: 64\n",
      "Batch 1532 size: 64\n",
      "Batch 1533 size: 64\n",
      "Batch 1534 size: 64\n",
      "Batch 1535 size: 64\n",
      "Batch 1536 size: 64\n",
      "Batch 1537 size: 64\n",
      "Batch 1538 size: 64\n",
      "Batch 1539 size: 64\n",
      "Batch 1540 size: 64\n",
      "Batch 1541 size: 64\n",
      "Batch 1542 size: 64\n",
      "Batch 1543 size: 64\n",
      "Batch 1544 size: 64\n",
      "Batch 1545 size: 64\n",
      "Batch 1546 size: 64\n",
      "Batch 1547 size: 64\n",
      "Batch 1548 size: 64\n",
      "Batch 1549 size: 64\n",
      "Batch 1550 size: 64\n",
      "Batch 1551 size: 64\n",
      "Batch 1552 size: 64\n",
      "Batch 1553 size: 64\n",
      "Batch 1554 size: 64\n",
      "Batch 1555 size: 64\n",
      "Batch 1556 size: 64\n",
      "Batch 1557 size: 64\n",
      "Batch 1558 size: 64\n",
      "Batch 1559 size: 64\n",
      "Batch 1560 size: 64\n",
      "Batch 1561 size: 64\n",
      "Batch 1562 size: 64\n",
      "Batch 1563 size: 64\n",
      "Batch 1564 size: 64\n",
      "Batch 1565 size: 64\n",
      "Batch 1566 size: 64\n",
      "Batch 1567 size: 64\n",
      "Batch 1568 size: 64\n",
      "Batch 1569 size: 64\n",
      "Batch 1570 size: 64\n",
      "Batch 1571 size: 64\n",
      "Batch 1572 size: 64\n",
      "Batch 1573 size: 64\n",
      "Batch 1574 size: 64\n",
      "Batch 1575 size: 64\n",
      "Batch 1576 size: 64\n",
      "Batch 1577 size: 64\n",
      "Batch 1578 size: 64\n",
      "Batch 1579 size: 64\n",
      "Batch 1580 size: 64\n",
      "Batch 1581 size: 64\n",
      "Batch 1582 size: 64\n",
      "Batch 1583 size: 64\n",
      "Batch 1584 size: 64\n",
      "Batch 1585 size: 64\n",
      "Batch 1586 size: 64\n",
      "Batch 1587 size: 64\n",
      "Batch 1588 size: 64\n",
      "Batch 1589 size: 64\n",
      "Batch 1590 size: 64\n",
      "Batch 1591 size: 64\n",
      "Batch 1592 size: 64\n",
      "Batch 1593 size: 64\n",
      "Batch 1594 size: 64\n",
      "Batch 1595 size: 64\n",
      "Batch 1596 size: 64\n",
      "Batch 1597 size: 64\n",
      "Batch 1598 size: 64\n",
      "Batch 1599 size: 64\n",
      "Batch 1600 size: 64\n",
      "Batch 1601 size: 64\n",
      "Batch 1602 size: 64\n",
      "Batch 1603 size: 64\n",
      "Batch 1604 size: 64\n",
      "Batch 1605 size: 64\n",
      "Batch 1606 size: 64\n",
      "Batch 1607 size: 64\n",
      "Batch 1608 size: 64\n",
      "Batch 1609 size: 64\n",
      "Batch 1610 size: 64\n",
      "Batch 1611 size: 64\n",
      "Batch 1612 size: 64\n",
      "Batch 1613 size: 64\n",
      "Batch 1614 size: 64\n",
      "Batch 1615 size: 64\n",
      "Batch 1616 size: 64\n",
      "Batch 1617 size: 64\n",
      "Batch 1618 size: 64\n",
      "Batch 1619 size: 64\n",
      "Batch 1620 size: 64\n",
      "Batch 1621 size: 64\n",
      "Batch 1622 size: 64\n",
      "Batch 1623 size: 64\n",
      "Batch 1624 size: 64\n",
      "Batch 1625 size: 64\n",
      "Batch 1626 size: 64\n",
      "Batch 1627 size: 64\n",
      "Batch 1628 size: 64\n",
      "Batch 1629 size: 64\n",
      "Batch 1630 size: 64\n",
      "Batch 1631 size: 64\n",
      "Batch 1632 size: 64\n",
      "Batch 1633 size: 64\n",
      "Batch 1634 size: 64\n",
      "Batch 1635 size: 64\n",
      "Batch 1636 size: 64\n",
      "Batch 1637 size: 64\n",
      "Batch 1638 size: 64\n",
      "Batch 1639 size: 64\n",
      "Batch 1640 size: 64\n",
      "Batch 1641 size: 64\n",
      "Batch 1642 size: 64\n",
      "Batch 1643 size: 64\n",
      "Batch 1644 size: 64\n",
      "Batch 1645 size: 64\n",
      "Batch 1646 size: 64\n",
      "Batch 1647 size: 64\n",
      "Batch 1648 size: 64\n",
      "Batch 1649 size: 64\n",
      "Batch 1650 size: 64\n",
      "Batch 1651 size: 64\n",
      "Batch 1652 size: 64\n",
      "Batch 1653 size: 64\n",
      "Batch 1654 size: 64\n",
      "Batch 1655 size: 64\n",
      "Batch 1656 size: 64\n",
      "Batch 1657 size: 64\n",
      "Batch 1658 size: 64\n",
      "Batch 1659 size: 64\n",
      "Batch 1660 size: 64\n",
      "Batch 1661 size: 64\n",
      "Batch 1662 size: 64\n",
      "Batch 1663 size: 64\n",
      "Batch 1664 size: 64\n",
      "Batch 1665 size: 64\n",
      "Batch 1666 size: 64\n",
      "Batch 1667 size: 64\n",
      "Batch 1668 size: 64\n",
      "Batch 1669 size: 64\n",
      "Batch 1670 size: 64\n",
      "Batch 1671 size: 64\n",
      "Batch 1672 size: 64\n",
      "Batch 1673 size: 64\n",
      "Batch 1674 size: 64\n",
      "Batch 1675 size: 64\n",
      "Batch 1676 size: 64\n",
      "Batch 1677 size: 64\n",
      "Batch 1678 size: 64\n",
      "Batch 1679 size: 64\n",
      "Batch 1680 size: 64\n",
      "Batch 1681 size: 64\n",
      "Batch 1682 size: 64\n",
      "Batch 1683 size: 64\n",
      "Batch 1684 size: 64\n",
      "Batch 1685 size: 64\n",
      "Batch 1686 size: 64\n",
      "Batch 1687 size: 64\n",
      "Batch 1688 size: 64\n",
      "Batch 1689 size: 64\n",
      "Batch 1690 size: 64\n",
      "Batch 1691 size: 64\n",
      "Batch 1692 size: 64\n",
      "Batch 1693 size: 64\n",
      "Batch 1694 size: 64\n",
      "Batch 1695 size: 64\n",
      "Batch 1696 size: 64\n",
      "Batch 1697 size: 64\n",
      "Batch 1698 size: 64\n",
      "Batch 1699 size: 64\n",
      "Batch 1700 size: 64\n",
      "Batch 1701 size: 64\n",
      "Batch 1702 size: 64\n",
      "Batch 1703 size: 64\n",
      "Batch 1704 size: 64\n",
      "Batch 1705 size: 64\n",
      "Batch 1706 size: 64\n",
      "Batch 1707 size: 64\n",
      "Batch 1708 size: 64\n",
      "Batch 1709 size: 64\n",
      "Batch 1710 size: 64\n",
      "Batch 1711 size: 64\n",
      "Batch 1712 size: 64\n",
      "Batch 1713 size: 64\n",
      "Batch 1714 size: 64\n",
      "Batch 1715 size: 64\n",
      "Batch 1716 size: 64\n",
      "Batch 1717 size: 64\n",
      "Batch 1718 size: 64\n",
      "Batch 1719 size: 64\n",
      "Batch 1720 size: 64\n",
      "Batch 1721 size: 64\n",
      "Batch 1722 size: 64\n",
      "Batch 1723 size: 64\n",
      "Batch 1724 size: 64\n",
      "Batch 1725 size: 64\n",
      "Batch 1726 size: 64\n",
      "Batch 1727 size: 64\n",
      "Batch 1728 size: 64\n",
      "Batch 1729 size: 64\n",
      "Batch 1730 size: 64\n",
      "Batch 1731 size: 64\n",
      "Batch 1732 size: 64\n",
      "Batch 1733 size: 64\n",
      "Batch 1734 size: 64\n",
      "Batch 1735 size: 64\n",
      "Batch 1736 size: 64\n",
      "Batch 1737 size: 64\n",
      "Batch 1738 size: 64\n",
      "Batch 1739 size: 64\n",
      "Batch 1740 size: 64\n",
      "Batch 1741 size: 64\n",
      "Batch 1742 size: 64\n",
      "Batch 1743 size: 64\n",
      "Batch 1744 size: 64\n",
      "Batch 1745 size: 64\n",
      "Batch 1746 size: 64\n",
      "Batch 1747 size: 64\n",
      "Batch 1748 size: 64\n",
      "Batch 1749 size: 64\n",
      "Batch 1750 size: 64\n",
      "Batch 1751 size: 64\n",
      "Batch 1752 size: 64\n",
      "Batch 1753 size: 64\n",
      "Batch 1754 size: 64\n",
      "Batch 1755 size: 64\n",
      "Batch 1756 size: 64\n",
      "Batch 1757 size: 64\n",
      "Batch 1758 size: 64\n",
      "Batch 1759 size: 64\n",
      "Batch 1760 size: 64\n",
      "Batch 1761 size: 64\n",
      "Batch 1762 size: 64\n",
      "Batch 1763 size: 64\n",
      "Batch 1764 size: 64\n",
      "Batch 1765 size: 64\n",
      "Batch 1766 size: 64\n",
      "Batch 1767 size: 64\n",
      "Batch 1768 size: 64\n",
      "Batch 1769 size: 64\n",
      "Batch 1770 size: 64\n",
      "Batch 1771 size: 64\n",
      "Batch 1772 size: 64\n",
      "Batch 1773 size: 64\n",
      "Batch 1774 size: 64\n",
      "Batch 1775 size: 64\n",
      "Batch 1776 size: 64\n",
      "Batch 1777 size: 64\n",
      "Batch 1778 size: 64\n",
      "Batch 1779 size: 64\n",
      "Batch 1780 size: 64\n",
      "Batch 1781 size: 64\n",
      "Batch 1782 size: 64\n",
      "Batch 1783 size: 64\n",
      "Batch 1784 size: 64\n",
      "Batch 1785 size: 64\n",
      "Batch 1786 size: 64\n",
      "Batch 1787 size: 64\n",
      "Batch 1788 size: 64\n",
      "Batch 1789 size: 64\n",
      "Batch 1790 size: 64\n",
      "Batch 1791 size: 64\n",
      "Batch 1792 size: 64\n",
      "Batch 1793 size: 64\n",
      "Batch 1794 size: 64\n",
      "Batch 1795 size: 64\n",
      "Batch 1796 size: 64\n",
      "Batch 1797 size: 64\n",
      "Batch 1798 size: 64\n",
      "Batch 1799 size: 64\n",
      "Batch 1800 size: 64\n",
      "Batch 1801 size: 64\n",
      "Batch 1802 size: 64\n",
      "Batch 1803 size: 64\n",
      "Batch 1804 size: 64\n",
      "Batch 1805 size: 64\n",
      "Batch 1806 size: 64\n",
      "Batch 1807 size: 64\n",
      "Batch 1808 size: 64\n",
      "Batch 1809 size: 64\n",
      "Batch 1810 size: 64\n",
      "Batch 1811 size: 64\n",
      "Batch 1812 size: 64\n",
      "Batch 1813 size: 64\n",
      "Batch 1814 size: 64\n",
      "Batch 1815 size: 64\n",
      "Batch 1816 size: 64\n",
      "Batch 1817 size: 64\n",
      "Batch 1818 size: 64\n",
      "Batch 1819 size: 64\n",
      "Batch 1820 size: 64\n",
      "Batch 1821 size: 64\n",
      "Batch 1822 size: 64\n",
      "Batch 1823 size: 64\n",
      "Batch 1824 size: 64\n",
      "Batch 1825 size: 64\n",
      "Batch 1826 size: 64\n",
      "Batch 1827 size: 64\n",
      "Batch 1828 size: 64\n",
      "Batch 1829 size: 64\n",
      "Batch 1830 size: 64\n",
      "Batch 1831 size: 64\n",
      "Batch 1832 size: 64\n",
      "Batch 1833 size: 64\n",
      "Batch 1834 size: 64\n",
      "Batch 1835 size: 64\n",
      "Batch 1836 size: 64\n",
      "Batch 1837 size: 64\n",
      "Batch 1838 size: 64\n",
      "Batch 1839 size: 64\n",
      "Batch 1840 size: 64\n",
      "Batch 1841 size: 64\n",
      "Batch 1842 size: 64\n",
      "Batch 1843 size: 64\n",
      "Batch 1844 size: 64\n",
      "Batch 1845 size: 64\n",
      "Batch 1846 size: 64\n",
      "Batch 1847 size: 64\n",
      "Batch 1848 size: 64\n",
      "Batch 1849 size: 64\n",
      "Batch 1850 size: 64\n",
      "Batch 1851 size: 64\n",
      "Batch 1852 size: 64\n",
      "Batch 1853 size: 64\n",
      "Batch 1854 size: 64\n",
      "Batch 1855 size: 64\n",
      "Batch 1856 size: 64\n",
      "Batch 1857 size: 64\n",
      "Batch 1858 size: 64\n",
      "Batch 1859 size: 64\n",
      "Batch 1860 size: 64\n",
      "Batch 1861 size: 64\n",
      "Batch 1862 size: 64\n",
      "Batch 1863 size: 64\n",
      "Batch 1864 size: 64\n",
      "Batch 1865 size: 64\n",
      "Batch 1866 size: 64\n",
      "Batch 1867 size: 64\n",
      "Batch 1868 size: 64\n",
      "Batch 1869 size: 64\n",
      "Batch 1870 size: 64\n",
      "Batch 1871 size: 64\n",
      "Batch 1872 size: 64\n",
      "Batch 1873 size: 64\n",
      "Batch 1874 size: 64\n",
      "Batch 1875 size: 64\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(Dataloaders['Test']):\n",
    "    batch_size = len(batch[0])  # Assuming the first element of the batch is the data\n",
    "    print(f\"Batch {i+1} size: {batch_size}\")\n",
    "    if batch_size != 64:\n",
    "        print(f\"Batch {i+1} does not contain 64 records.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mZH7kgxF9p00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1670395868510,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "mZH7kgxF9p00",
    "outputId": "3b5f2da8-cb90-43b1-ea26-b5a9fad4dd31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 size: 64\n",
      "Batch 2 size: 64\n",
      "Batch 3 size: 64\n",
      "Batch 4 size: 64\n",
      "Batch 5 size: 64\n",
      "Batch 6 size: 64\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(Dataloaders[5][0]):\n",
    "    batch_size = len(batch[0])  # Assuming the first element of the batch is the data\n",
    "    print(f\"Batch {i+1} size: {batch_size}\")\n",
    "    if batch_size != 64:\n",
    "        print(f\"Batch {i+1} does not contain 64 records.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4c70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21.6796875]\n",
      "[48.6328125]\n",
      "[75.0]\n",
      "[32.2265625]\n",
      "[51.7578125]\n",
      "[41.796875]\n",
      "[36.328125]\n",
      "[68.03385416666667]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "for CLUSTER in range (1, 9):\n",
    "    DEVICE_PERCENTAGE = []\n",
    "    for DEVICE__ in range(0,3):\n",
    "        for i, batch in enumerate(Dataloaders[CLUSTER][DEVICE__]):\n",
    "            _, labels = batch\n",
    "            class_counts = Counter(labels.numpy())\n",
    "            total_records = sum(class_counts.values())\n",
    "            class_0_count = class_counts.get(0, 0)\n",
    "            percentage_class_0 = (class_0_count / total_records) * 100\n",
    "            DEVICE_PERCENTAGE.append(percentage_class_0)\n",
    "            # print(f\"Batch {i+1}: {dict(class_counts)}\")\n",
    "            # print(f\"Percentage of class 0: {percentage_class_0:.2f}%\\n\")\n",
    "    # print(DEVICE_PERCENTAGE)        \n",
    "    chunk_size = 6\n",
    "    averages = [sum(DEVICE_PERCENTAGE[i:i + chunk_size]) / chunk_size for i in range(0, len(DEVICE_PERCENTAGE), chunk_size)]\n",
    "    # print(\"Averages of every device:\")\n",
    "    # print(averages)\n",
    "    chunk_size_4 = 4\n",
    "    averages = [sum(averages[i:i + chunk_size_4]) / chunk_size_4 for i in range(0, len(averages), chunk_size_4)]\n",
    "    # print(\"Averages of every 4 devices:\")\n",
    "    print(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3a112-2124-4571-802b-61850c0be001",
   "metadata": {},
   "outputs": [],
   "source": [
    "del TrafficData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f464a9f3-f71b-4aab-adc9-90f8c14e7313",
   "metadata": {
    "editable": true,
    "id": "gjVnC-rj9gC4",
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<font color='Red'>***Neural Network***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fLD8dUBg9pyY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1670813208940,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "fLD8dUBg9pyY",
    "outputId": "6df101c4-37c9-4660-f5c7-3a42be8b8951"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim=98,\n",
    "                 output_dim=15,\n",
    "                 h1=64,\n",
    "                 h2=32,\n",
    "                 dropout=0.0,\n",
    "                 activation=\"relu\",\n",
    "                 batch_norm=False):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Define layers\n",
    "        self.layer_1 = nn.Linear(input_dim, h1)\n",
    "        self.layer_2 = nn.Linear(h1, h2)\n",
    "        self.layer_out = nn.Linear(h2, output_dim)\n",
    "\n",
    "        # Optional batch normalization\n",
    "        self.bn1 = nn.BatchNorm1d(h1) if batch_norm else nn.Identity()\n",
    "        self.bn2 = nn.BatchNorm1d(h2) if batch_norm else nn.Identity()\n",
    "\n",
    "        # Activation mapping\n",
    "        act_map = {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"leakyrelu\": nn.LeakyReLU(0.1)\n",
    "        }\n",
    "        self.act = act_map.get(activation, nn.ReLU())\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer_2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer_out(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc624e0-c16a-413d-bb70-c7fc7743def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # Initialize and save a random starting model (Round 0)\n",
    "# # ============================================================\n",
    "# print(\"🔧 Initializing base model using current TRAIN_CFG...\")\n",
    "\n",
    "# Random = Net(\n",
    "#     input_dim=98,\n",
    "#     output_dim=15,\n",
    "#     h1=TRAIN_CFG.get(\"hidden_size_1\", 64),\n",
    "#     h2=TRAIN_CFG.get(\"hidden_size_2\", 32),\n",
    "#     dropout=TRAIN_CFG.get(\"dropout\", 0.0),\n",
    "#     activation=TRAIN_CFG.get(\"activation\", \"relu\"),\n",
    "#     batch_norm=TRAIN_CFG.get(\"batch_norm\", False),\n",
    "# )\n",
    "\n",
    "# # Show layer shapes for sanity check\n",
    "# for param_tensor in Random.state_dict():\n",
    "#     print(param_tensor, \"\\t\", Random.state_dict()[param_tensor].size())\n",
    "\n",
    "# # Ensure the output directory exists\n",
    "# os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "# # Save initial random model weights\n",
    "# init_model_path = os.path.join(PATH, \"0_Input_Random_model_Net.pth\")\n",
    "# torch.save(Random.state_dict(), init_model_path)\n",
    "\n",
    "# print(f\"✅ Saved initial model to {init_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SKRdGrET9pvn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1670813234860,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "SKRdGrET9pvn",
    "outputId": "fe9c8747-dad6-4606-f067-021d127d259c"
   },
   "outputs": [],
   "source": [
    "def train(net, trainloader, epochs: int, lr: float, weight_decay: float = 0.0, verbose=True):\n",
    "    \"\"\"Train the network for one FL round using the current TRAIN_CFG parameters.\"\"\"\n",
    "\n",
    "    # --- Loss function (with optional label smoothing)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=TRAIN_CFG.get(\"label_smoothing\", 0.0))\n",
    "\n",
    "    # --- Optimizer selection\n",
    "    opt_name = TRAIN_CFG.get(\"optimizer\", \"adamw\").lower()\n",
    "    wd = weight_decay\n",
    "    momentum = TRAIN_CFG.get(\"momentum\", 0.0)\n",
    "\n",
    "    if opt_name == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=wd)\n",
    "    elif opt_name == \"rmsprop\":\n",
    "        optimizer = torch.optim.RMSprop(net.parameters(), lr=lr, momentum=momentum, weight_decay=wd)\n",
    "    elif opt_name == \"adam\":\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "    else:  # default: AdamW\n",
    "        optimizer = torch.optim.AdamW(net.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    # --- Scheduler (optional)\n",
    "    sched_type = TRAIN_CFG.get(\"scheduler\", \"none\").lower()\n",
    "    scheduler = None\n",
    "    if sched_type == \"step\":\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=int(TRAIN_CFG.get(\"scheduler_step\", 2)),\n",
    "            gamma=float(TRAIN_CFG.get(\"scheduler_gamma\", 0.9))\n",
    "        )\n",
    "    elif sched_type == \"cosine\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=epochs\n",
    "        )\n",
    "    elif sched_type == \"plateau\":\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, factor=float(TRAIN_CFG.get(\"scheduler_gamma\", 0.9)), patience=2\n",
    "        )\n",
    "\n",
    "    # --- Training loop\n",
    "    net.train()\n",
    "    prediction_matrix, actual_matrix, acc_matrix, loss_matrix = [], [], [], []\n",
    "    grad_clip = float(TRAIN_CFG.get(\"gradient_clip_norm\", 0.0))\n",
    "    early_stop_patience = int(TRAIN_CFG.get(\"early_stopping_patience\", 0))\n",
    "    best_loss = float(\"inf\")\n",
    "    patience_ctr = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # optional gradient clipping\n",
    "            if grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "            prediction_matrix.append(torch.max(outputs.data, 1)[1].tolist())\n",
    "            actual_matrix.append(labels.tolist())\n",
    "\n",
    "        # --- epoch end\n",
    "        epoch_loss /= max(1, len(trainloader))\n",
    "        epoch_acc = correct / max(1, total)\n",
    "        loss_matrix.append(epoch_loss)\n",
    "        acc_matrix.append(epoch_acc)\n",
    "\n",
    "        # Scheduler step\n",
    "        if scheduler:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(epoch_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        # Early stopping check\n",
    "        if early_stop_patience > 0:\n",
    "            if epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                patience_ctr = 0\n",
    "            else:\n",
    "                patience_ctr += 1\n",
    "                if patience_ctr >= early_stop_patience:\n",
    "                    if verbose:\n",
    "                        print(f\"⏹ Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "    return prediction_matrix, actual_matrix, acc_matrix, loss_matrix\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate model performance on validation/test data.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=TRAIN_CFG.get(\"label_smoothing\", 0.0))\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    prediction_matrix, actual_matrix, acc_matrix, loss_matrix = [], [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            prediction_matrix.append(predicted.tolist())\n",
    "            actual_matrix.append(labels.tolist())\n",
    "\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / max(1, total)\n",
    "    loss_matrix.append(loss)\n",
    "    acc_matrix.append(accuracy)\n",
    "\n",
    "    print(f\"Evaluation: eval loss {loss:.6f}, eval accuracy {accuracy:.4f}\")\n",
    "    return loss, accuracy, prediction_matrix, actual_matrix, acc_matrix, loss_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otE9jhmS-IXU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1670813239232,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "otE9jhmS-IXU",
    "outputId": "ec01b70c-2185-4293-87ec-b3eaa8b6d92a"
   },
   "outputs": [],
   "source": [
    "prediction_dict= {}\n",
    "actual_dict= {}\n",
    "accuracy_dict= {}\n",
    "loss_dict= {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7eeeda-eb5e-499f-8b0f-a3bf9c7558c9",
   "metadata": {
    "id": "gjVnC-rj9gC4",
    "tags": []
   },
   "source": [
    "<font color='green'>***LLM***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32692c51-4319-4742-8577-8868f5d6bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LLM Reviewer/Optimizer (Gemma-first, synchronized JSON + DONE sentinel)\n",
    "# ============================================================\n",
    "import os, re, json, math, glob, time, torch\n",
    "from functools import lru_cache\n",
    "from statistics import mean\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# ---- Toggle offline/online\n",
    "LLM_OFFLINE = False  # set True to force offline/cached mode\n",
    "\n",
    "if LLM_OFFLINE:\n",
    "    os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "    os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "os.environ.setdefault(\"TRANSFORMERS_CACHE\", \"/tmp/hf_cache\")\n",
    "os.makedirs(\"/tmp/gemma_offload\", exist_ok=True)\n",
    "\n",
    "# ---- Model candidates (Gemma 3 preferred, then open fallbacks)\n",
    "_GEMMA_CANDIDATES = [\n",
    "    \"google/gemma-3-4b-it\",\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "]\n",
    "\n",
    "def _load_first_available_model(candidates):\n",
    "    last_err = None\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "    for mid in candidates:\n",
    "        try:\n",
    "            print(f\"Trying LLM: {mid}\")\n",
    "            tok = AutoTokenizer.from_pretrained(mid, use_fast=True, token=hf_token)\n",
    "            if tok.pad_token is None:\n",
    "                tok.pad_token = tok.eos_token\n",
    "            device_llm = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            dtype_llm = torch.float16 if device_llm == \"cuda\" else torch.float32\n",
    "            llm = AutoModelForCausalLM.from_pretrained(\n",
    "                mid,\n",
    "                dtype=dtype_llm,\n",
    "                device_map=\"auto\",\n",
    "                low_cpu_mem_usage=True,\n",
    "                offload_folder=\"/tmp/gemma_offload\",\n",
    "                token=hf_token,\n",
    "            )\n",
    "            pipe = pipeline(\"text-generation\", model=llm, tokenizer=tok, dtype=dtype_llm)\n",
    "            print(f\"✅ Loaded {mid}\")\n",
    "            return mid, pipe\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed {mid}: {e}\")\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"No candidate LLM could be loaded. Last error: {last_err}\")\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_llm_pipe():\n",
    "    return _load_first_available_model(tuple(_GEMMA_CANDIDATES))\n",
    "\n",
    "# ---- Default configuration\n",
    "DEFAULT_CONFIG = {\n",
    "    \"round\": 0,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"optimizer\": \"adamw\",\n",
    "    \"momentum\": 0.0,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"dropout\": 0.0,\n",
    "    \"gradient_clip_norm\": 0.0,\n",
    "    \"label_smoothing\": 0.0,\n",
    "    \"scheduler\": \"none\",\n",
    "    \"scheduler_step\": 2,\n",
    "    \"scheduler_gamma\": 0.9,\n",
    "    \"early_stopping_patience\": 0,\n",
    "    \"fraction_fit\": 1.0,\n",
    "    \"aggregation\": \"FedAvg\",\n",
    "    \"hidden_size_1\": 64,\n",
    "    \"hidden_size_2\": 32,\n",
    "    \"activation\": \"relu\",\n",
    "    \"batch_norm\": False,\n",
    "    \"notes\": \"\",\n",
    "}\n",
    "\n",
    "def save_config(cfg: dict, path: str):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(cfg, f, indent=2)\n",
    "\n",
    "def load_or_create_config(path: str) -> dict:\n",
    "    if os.path.exists(path):\n",
    "        with open(path) as f:\n",
    "            return json.load(f)\n",
    "    save_config(DEFAULT_CONFIG, path)\n",
    "    return dict(DEFAULT_CONFIG)\n",
    "\n",
    "# ---- Merge and clamp LLM suggestions\n",
    "def apply_llm_suggestions(current_cfg: dict, llm_json: dict) -> dict:\n",
    "    import copy\n",
    "    updated = copy.deepcopy(current_cfg)\n",
    "\n",
    "    allowed_keys = {\n",
    "        \"learning_rate\": float,\n",
    "        \"epochs\": int,\n",
    "        \"weight_decay\": float,\n",
    "        \"dropout\": float,\n",
    "        \"batch_size\": int,\n",
    "        \"optimizer\": str,\n",
    "        \"momentum\": float,\n",
    "        \"gradient_clip_norm\": float,\n",
    "        \"label_smoothing\": float,\n",
    "        \"scheduler\": str,\n",
    "        \"scheduler_step\": int,\n",
    "        \"scheduler_gamma\": float,\n",
    "        \"early_stopping_patience\": int,\n",
    "        \"fraction_fit\": float,\n",
    "        \"aggregation\": str,\n",
    "        \"hidden_size_1\": int,\n",
    "        \"hidden_size_2\": int,\n",
    "        \"activation\": str,\n",
    "        \"batch_norm\": lambda x: str(x).lower() == \"true\",\n",
    "        \"notes\": str,\n",
    "    }\n",
    "\n",
    "    for k, caster in allowed_keys.items():\n",
    "        if k in llm_json:\n",
    "            try:\n",
    "                updated[k] = caster(llm_json[k])\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Attach entire LLM output to notes when available, otherwise stringify llm_json\n",
    "    raw_output = None\n",
    "    for k in (\"_raw_output\", \"raw_output\", \"full_output\", \"llm_output\", \"output\"):\n",
    "        if k in llm_json and llm_json[k]:\n",
    "            raw_output = llm_json[k]\n",
    "            break\n",
    "    if raw_output is None:\n",
    "        try:\n",
    "            raw_output = json.dumps(llm_json, ensure_ascii=False)\n",
    "        except Exception:\n",
    "            raw_output = str(llm_json)\n",
    "    updated[\"notes\"] = str(raw_output)\n",
    "    return updated\n",
    "\n",
    "# ---- Text generation and JSON parsing helpers\n",
    "_GEN_ARGS = dict(max_new_tokens=512, do_sample=False)\n",
    "\n",
    "def _extract_json(text: str) -> dict:\n",
    "    \"\"\"Extract last valid JSON block even if truncated.\"\"\"\n",
    "    import json\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    if start == -1 or end == -1 or end <= start:\n",
    "        return {}\n",
    "    snippet = text[start:end + 1]\n",
    "    try:\n",
    "        return json.loads(snippet)\n",
    "    except json.JSONDecodeError:\n",
    "        # Try trimming stepwise until it parses\n",
    "        for cut in range(len(snippet) - 1, 0, -1):\n",
    "            try:\n",
    "                return json.loads(snippet[:cut] + \"}\")\n",
    "            except Exception:\n",
    "                continue\n",
    "        return {}\n",
    "\n",
    "def generate_until_done(pipe, prompt, max_wait=120, max_retries=5):\n",
    "    \"\"\"Generate text from LLM and wait until '### DONE' appears.\"\"\"\n",
    "    output = \"\"\n",
    "    for attempt in range(max_retries):\n",
    "        chunk = pipe(prompt, **_GEN_ARGS)[0][\"generated_text\"]\n",
    "        output += chunk\n",
    "        if \"### DONE\" in output:\n",
    "            print(\"✅ LLM finished generating (### DONE detected).\")\n",
    "            output = output.split(\"### DONE\")[0]\n",
    "            return output.strip()\n",
    "        print(f\"⚠️ LLM output incomplete (attempt {attempt+1}/{max_retries}), retrying...\")\n",
    "        time.sleep(2)\n",
    "    print(\"⚠️ Giving up waiting for DONE sentinel — using best-effort output.\")\n",
    "    return output.strip()\n",
    "\n",
    "# ---- Historical config loading\n",
    "def _detect_current_round_from_jsons(base_path: str) -> int:\n",
    "    files = glob.glob(os.path.join(base_path, \"train_config_round_*.json\"))\n",
    "    rounds = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            r = int(os.path.splitext(os.path.basename(f))[0].split(\"_\")[-1])\n",
    "            rounds.append(r)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return max(rounds) if rounds else 1\n",
    "\n",
    "def _load_recent_configs(base_path: str, current_round: int, window: int = 5):\n",
    "    cfgs = []\n",
    "    for r in range(max(1, current_round - window), current_round):\n",
    "        p = os.path.join(base_path, f\"train_config_round_{r}.json\")\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                with open(p, \"r\") as f:\n",
    "                    data = json.load(f)\n",
    "                    data[\"round\"] = r\n",
    "                    cfgs.append(data)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return cfgs\n",
    "\n",
    "# ---- Main LLM review + suggestion step\n",
    "def llm_review_and_suggest_llm3(current_cfg: dict, metrics: dict, info: dict, *,\n",
    "                                base_path: str = None, round_idx: int | None = None) -> dict:\n",
    "    \"\"\"\n",
    "    Review training results and suggest improved hyperparameters.\n",
    "    Waits until LLM finishes ('### DONE') before continuing.\n",
    "    \"\"\"\n",
    "    model_id, pipe = get_llm_pipe()\n",
    "\n",
    "    base_path = base_path or (globals().get(\"PATH\") or \".\")\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    if round_idx is None:\n",
    "        round_idx = _detect_current_round_from_jsons(base_path)\n",
    "\n",
    "    prev_cfgs = _load_recent_configs(base_path, round_idx, window=5)\n",
    "    hist_summary = {\n",
    "        \"n_previous\": len(prev_cfgs),\n",
    "        \"avg_lr\": mean([c.get(\"learning_rate\", 0.0) for c in prev_cfgs]) if prev_cfgs else None,\n",
    "        \"avg_epochs\": mean([c.get(\"epochs\", 0) for c in prev_cfgs]) if prev_cfgs else None,\n",
    "        \"avg_wd\": mean([c.get(\"weight_decay\", 0.0) for c in prev_cfgs]) if prev_cfgs else None,\n",
    "        \"avg_dropout\": mean([c.get(\"dropout\", 0.0) for c in prev_cfgs]) if prev_cfgs else None,\n",
    "    }\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert federated learning optimization assistant integrated into an automated ML pipeline.\n",
    "Your purpose is to improve generalization, validation precision, and accuracy across rounds of federated training.\n",
    "You will be provided with:\n",
    "  1️⃣ CURRENT_CONFIG — the current round’s hyperparameters,\n",
    "  2️⃣ ROUND_METRICS — validation metrics for this round,\n",
    "  3️⃣ DATA_INFO — dataset and hardware context,\n",
    "  4️⃣ LAST_5_CONFIGS — up to five prior configurations for trend analysis.\n",
    "\n",
    "# Instructions\n",
    "- Analyze performance and trends.\n",
    "- Suggest small, data-driven hyperparameter updates.\n",
    "- Avoid overfitting; stability is more important than drastic improvements.\n",
    "- Prefer conservative changes in learning rate and epochs.\n",
    "- If val_loss ↑ while accuracy ↑ → add mild regularization (dropout or weight_decay).\n",
    "- If val_loss ↓ but accuracy stagnates → slightly increase epochs or learning_rate.\n",
    "- If both degrade → reduce learning_rate and add regularization.\n",
    "- If metrics plateau for 3+ rounds → only minor exploration.\n",
    "\n",
    "# STRICT OUTPUT FORMAT\n",
    "Return **only** a single JSON object.\n",
    "Never include markdown code fences (no ```json or ```).\n",
    "Do not include explanations, text, comments, or reasoning outside the JSON object.\n",
    "\n",
    "The JSON must follow this schema exactly:\n",
    "{{\n",
    "  \"learning_rate\": <float>,\n",
    "  \"epochs\": <int>,\n",
    "  \"weight_decay\": <float>,\n",
    "  \"dropout\": <float>,\n",
    "  \"batch_size\": <int>,\n",
    "  \"optimizer\": <\"adam\" | \"adamw\" | \"sgd\">,\n",
    "  \"momentum\": <float>,\n",
    "  \"gradient_clip_norm\": <float>,\n",
    "  \"label_smoothing\": <float>,\n",
    "  \"scheduler\": <\"none\" | \"StepLR\" | \"ReduceLROnPlateau\" | \"CosineAnnealing\">,\n",
    "  \"scheduler_step\": <int>,\n",
    "  \"scheduler_gamma\": <float>,\n",
    "  \"early_stopping_patience\": <int>,\n",
    "  \"fraction_fit\": <float>,\n",
    "  \"aggregation\": <\"FedAvg\" | \"FedProx\" | \"mean\">,\n",
    "  \"hidden_size_1\": <int>,\n",
    "  \"hidden_size_2\": <int>,\n",
    "  \"activation\": <\"relu\" | \"gelu\" | \"tanh\" | \"elu\">,\n",
    "  \"batch_norm\": <bool>,\n",
    "  \"notes\": \"<short rationale under 20 words>\"\n",
    "}}\n",
    "\n",
    "CURRENT_CONFIG:\n",
    "{json.dumps(current_cfg, indent=2)}\n",
    "\n",
    "LAST_ROUND_METRICS:\n",
    "{json.dumps(metrics, indent=2)}\n",
    "\n",
    "DATA_INFO:\n",
    "{json.dumps(info, indent=2)}\n",
    "\n",
    "LAST_5_CONFIGS:\n",
    "{json.dumps(prev_cfgs, indent=2)}\n",
    "\n",
    "HISTORICAL_SUMMARY:\n",
    "{json.dumps(hist_summary, indent=2)}\n",
    "\n",
    "Return *only* the JSON object matching the above schema, nothing else.\n",
    "# Your task:\n",
    "Analyze all inputs and output ONLY the JSON object above — nothing else.\n",
    "After the JSON, append a new line that says exactly without quotation marks: \"### DONE\"\n",
    "\"\"\"\n",
    "\n",
    "    # --- Generate output, wait until sentinel ---\n",
    "    out = generate_until_done(pipe, prompt)\n",
    "\n",
    "    # --- Show and save full output ---\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"🧠 Full LLM Output (Round {round_idx})\")\n",
    "    print(\"==============================\")\n",
    "    print(out)\n",
    "    print(\"==============================\\n\")\n",
    "\n",
    "    with open(os.path.join(base_path, f\"llm_raw_round_{round_idx}.txt\"), \"w\") as f:\n",
    "        f.write(out)\n",
    "\n",
    "    # --- Parse JSON safely ---\n",
    "    suggestions = _extract_json(out) or {}\n",
    "    if not suggestions:\n",
    "        print(\"⚠️ No valid JSON detected — keeping previous configuration.\")\n",
    "    else:\n",
    "        print(f\"✅ Parsed LLM suggestions ({len(suggestions)} keys):\")\n",
    "        print(json.dumps(suggestions, indent=2))\n",
    "\n",
    "    return suggestions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56e6068-6cb0-4cd3-844f-e771c5435b2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Live training config (LLM-overridable between rounds)\n",
    "# ============================================================\n",
    "TRAIN_CFG = {\n",
    "    # Core\n",
    "    \"learning_rate\": float(LEARNING_RATE),\n",
    "    \"epochs\": int(EPOCHS),\n",
    "    \"batch_size\": int(BATCH_SIZE),\n",
    "\n",
    "    # Regularization\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"dropout\": 0.0,\n",
    "    \"label_smoothing\": 0.0,\n",
    "    \"gradient_clip_norm\": 0.0,   # 0 = disabled\n",
    "\n",
    "    # Optimizer / Scheduler\n",
    "    \"optimizer\": \"adamw\",        # adam|adamw|sgd|rmsprop\n",
    "    \"momentum\": 0.0,             # used only for SGD-like opts\n",
    "    \"scheduler\": \"none\",         # none|cosine|step|plateau\n",
    "    \"scheduler_step\": 2,         # for 'step'\n",
    "    \"scheduler_gamma\": 0.9,      # for 'step'/'plateau'\n",
    "    \"early_stopping_patience\": 0,\n",
    "\n",
    "    # Federated knobs (used when wiring strategy)\n",
    "    \"fraction_fit\": 1.0,         # 0.1..1.0\n",
    "\n",
    "    # (Model structure keys are persisted but applied when instantiating Net)\n",
    "    \"hidden_size_1\": 64,\n",
    "    \"hidden_size_2\": 32,\n",
    "    \"activation\": \"relu\",        # relu|gelu|leakyrelu\n",
    "    \"batch_norm\": False,\n",
    "}\n",
    "\n",
    "def use_cfg_in_globals(cfg: dict):\n",
    "    \"\"\"\n",
    "    Apply *safe mid-round* overrides to globals.\n",
    "    Keep batch_size static mid-run (rebuild loaders if you want it live).\n",
    "    \"\"\"\n",
    "    global LEARNING_RATE, EPOCHS\n",
    "    LEARNING_RATE = float(cfg.get(\"learning_rate\", LEARNING_RATE))\n",
    "    EPOCHS       = int(cfg.get(\"epochs\", EPOCHS))\n",
    "\n",
    "    # Persisted in TRAIN_CFG for the rest of the pipeline to consume:\n",
    "    # - weight_decay/dropout: used when building optimizer / Net()\n",
    "    # - optimizer/scheduler fields: read inside your train() to build opt/sched\n",
    "    # - gradient_clip_norm/label_smoothing: applied inside train() if > 0\n",
    "    # - fraction_fit: wire into SaveModelStrategy(fraction_fit=TRAIN_CFG[\"fraction_fit\"])\n",
    "    # - hidden sizes/activation/batch_norm: apply when you instantiate Net(...)\n",
    "    #   e.g., Net(h1=TRAIN_CFG[\"hidden_size_1\"], h2=..., activation=..., batch_norm=..., dropout=...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40e05e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Metrics + LLM review step (call this after each round)\n",
    "# ============================================================\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def evaluate_full(model, testloader):\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    all_preds, all_labels = [], []\n",
    "    loss_sum, n_batches = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in testloader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            out = model(x)\n",
    "            loss_sum += crit(out, y).item()\n",
    "            n_batches += 1\n",
    "            preds = torch.max(out, 1)[1]\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "            all_labels.extend(y.cpu().numpy().tolist())\n",
    "\n",
    "    loss = loss_sum / max(1, n_batches)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=\"macro\", zero_division=0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"val_loss\": float(loss),\n",
    "        \"val_accuracy\": float(acc),\n",
    "        \"precision_macro\": float(precision),\n",
    "        \"recall_macro\": float(recall),\n",
    "        \"f1_macro\": float(f1),\n",
    "    }\n",
    "\n",
    "\n",
    "def llm_step_after_round(round_idx: int, model_for_eval: nn.Module):\n",
    "    \"\"\"\n",
    "    Evaluate the round’s model, run LLM review, and save next-round JSON.\n",
    "    Blocks execution until the LLM provides a valid JSON response or times out.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"🧮 Evaluating global model (Round {round_idx}) ...\")\n",
    "    metrics = evaluate_full(model_for_eval, Dataloaders['Test'])\n",
    "\n",
    "    print(\"📊 Validation Metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Load or initialize round config\n",
    "    # ------------------------------------------------------\n",
    "    current_cfg_path = os.path.join(PATH, f\"train_config_round_{round_idx}.json\")\n",
    "    current_cfg = load_or_create_config(current_cfg_path)\n",
    "\n",
    "    current_cfg.update({\n",
    "        \"round\": round_idx,\n",
    "        \"learning_rate\": float(TRAIN_CFG.get(\"learning_rate\", LEARNING_RATE)),\n",
    "        \"epochs\": int(TRAIN_CFG.get(\"epochs\", EPOCHS)),\n",
    "        \"weight_decay\": float(TRAIN_CFG.get(\"weight_decay\", 0.0)),\n",
    "        \"dropout\": float(TRAIN_CFG.get(\"dropout\", 0.0)),\n",
    "        \"batch_size\": int(TRAIN_CFG.get(\"batch_size\", BATCH_SIZE)),\n",
    "        \"notes\": f\"Auto-updated after round {round_idx}\",\n",
    "    })\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Build LLM context\n",
    "    # ------------------------------------------------------\n",
    "    data_info = {\n",
    "        \"num_clients\": int(NUM_CLIENTS),\n",
    "        \"round_size\": int(SIZE_ROUND),\n",
    "        \"batch_size\": int(BATCH_SIZE),\n",
    "        \"n_features\": 98,\n",
    "        \"n_classes\": 15,\n",
    "        \"device\": str(DEVICE),\n",
    "    }\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Call LLM and wait for DONE sentinel\n",
    "    # ------------------------------------------------------\n",
    "    print(\"\\n🤖 Requesting LLM optimization suggestions...\")\n",
    "    suggestions = {}\n",
    "    for attempt in range(2):  # retry once if JSON fails\n",
    "        suggestions = llm_review_and_suggest_llm3(\n",
    "            current_cfg, metrics, data_info,\n",
    "            base_path=PATH, round_idx=round_idx\n",
    "        )\n",
    "        if suggestions:\n",
    "            break\n",
    "        print(f\"⚠️ Attempt {attempt+1} failed — LLM did not return valid JSON. Retrying...\\n\")\n",
    "\n",
    "    if not suggestions:\n",
    "        print(\"⚠️ LLM did not return valid JSON after retries — keeping current hyperparameters.\")\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Merge + save next configuration\n",
    "    # ------------------------------------------------------\n",
    "    updated_cfg = apply_llm_suggestions(current_cfg, suggestions)\n",
    "    next_round = round_idx + 1\n",
    "    next_cfg_path = os.path.join(PATH, f\"train_config_round_{next_round}.json\")\n",
    "    save_config(updated_cfg, next_cfg_path)\n",
    "\n",
    "    print(f\"\\n✅ Wrote {next_cfg_path}\")\n",
    "    print(\"LLM suggestions (applied):\")\n",
    "    print(f\"  learning_rate = {updated_cfg.get('learning_rate')}\")\n",
    "    print(f\"  epochs        = {updated_cfg.get('epochs')}\")\n",
    "    print(f\"  weight_decay  = {updated_cfg.get('weight_decay')}\")\n",
    "    print(f\"  dropout       = {updated_cfg.get('dropout')}\")\n",
    "    print(\"------------------------------------------------------\")\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Update live training parameters for next round\n",
    "    # ------------------------------------------------------\n",
    "    TRAIN_CFG.update({\n",
    "        \"learning_rate\": float(updated_cfg.get(\"learning_rate\", LEARNING_RATE)),\n",
    "        \"epochs\": int(updated_cfg.get(\"epochs\", EPOCHS)),\n",
    "        \"weight_decay\": float(updated_cfg.get(\"weight_decay\", 0.0)),\n",
    "        \"dropout\": float(updated_cfg.get(\"dropout\", 0.0)),\n",
    "    })\n",
    "    use_cfg_in_globals(TRAIN_CFG)\n",
    "    print(\"🔄 Global TRAIN_CFG updated for next round.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2GrOePXWWEzi",
   "metadata": {
    "id": "2GrOePXWWEzi",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='Brown'>***Federated Learning Classes***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fk8W4F6K8Sy1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1670813254332,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "fk8W4F6K8Sy1",
    "outputId": "a8c6b6fe-b416-4a5f-f41f-68a76993e47a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    \"\"\"Extract model parameters as a list of NumPy arrays (CPU-safe).\"\"\"\n",
    "    return [val.detach().cpu().numpy().astype(np.float32) for _, val in net.state_dict().items()]\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    \"\"\"Load parameters from NumPy arrays into the given network safely.\"\"\"\n",
    "    # Ensure matching key order\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    # Convert arrays to tensors with the same dtype/device\n",
    "    state_dict = OrderedDict({k: torch.tensor(v, dtype=torch.float32, device=next(net.parameters()).device)\n",
    "                              for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WMjtRo9r8Sv7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 155,
     "status": "ok",
     "timestamp": 1670813544785,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "WMjtRo9r8Sv7",
    "outputId": "056c1096-a023-47b7-9668-409abbf3b50f"
   },
   "outputs": [],
   "source": [
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, FL_Update):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.FL_Update = FL_Update\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        # --- Read config (LLM-driven) ---\n",
    "        local_epochs = int(config.get(\"local_epochs\", 1))\n",
    "        lr = float(config.get(\"learning_rate\", LEARNING_RATE))\n",
    "        wd = float(config.get(\"weight_decay\", 0.0))\n",
    "        clip_norm = float(config.get(\"gradient_clip_norm\", 0.0))\n",
    "\n",
    "        print(f\"[Client {self.cid}, round {self.FL_Update}] fit, \"\n",
    "              f\"epochs={local_epochs}, lr={lr}, wd={wd}, clip_norm={clip_norm}\")\n",
    "\n",
    "        # --- Load global weights ---\n",
    "        set_parameters(self.net, parameters)\n",
    "\n",
    "        # --- Train locally ---\n",
    "        _1, _2, _3, _4 = train(\n",
    "            self.net,\n",
    "            self.trainloader,\n",
    "            epochs=local_epochs,\n",
    "            lr=lr,\n",
    "            weight_decay=wd,\n",
    "        )\n",
    "\n",
    "        # --- Optional gradient clipping if requested ---\n",
    "        if clip_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(self.net.parameters(), clip_norm)\n",
    "\n",
    "        # --- Log local performance for analysis ---\n",
    "        prediction_dict[f'C{self.cid}R{self.FL_Update}'] = _1\n",
    "        actual_dict[f'C{self.cid}R{self.FL_Update}'] = _2\n",
    "        accuracy_dict[f'C{self.cid}R{self.FL_Update}'] = _3\n",
    "        loss_dict[f'C{self.cid}R{self.FL_Update}'] = _4\n",
    "\n",
    "        # --- Return updated model parameters ---\n",
    "        return get_parameters(self.net), len(self.trainloader.dataset), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] evaluate skipped (centralized eval only)\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0abdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_sort_client_updates(global_model, round_number, client_ids, base_path=PATH):\n",
    "    \"\"\"Compare and rank client contributions for a given round based on update magnitude.\"\"\"\n",
    "    client_updates = {}\n",
    "    for cid in client_ids:\n",
    "        update_filename = os.path.join(base_path, f\"C{cid}R{round_number}_update.pkl\")\n",
    "        if not os.path.exists(update_filename):\n",
    "            print(f\"⚠️ Missing update file for client {cid}, round {round_number}\")\n",
    "            continue\n",
    "        with open(update_filename, 'rb') as f:\n",
    "            client_updates[cid] = pickle.load(f)\n",
    "\n",
    "    if not client_updates:\n",
    "        print(\"⚠️ No client updates found.\")\n",
    "        return [], []\n",
    "\n",
    "    client_contributions = {\n",
    "        cid: calculate_weight_magnitude(global_model, update)\n",
    "        for cid, update in client_updates.items()\n",
    "    }\n",
    "\n",
    "    sorted_clients = sorted(client_contributions.items(), key=lambda x: x[1], reverse=True)\n",
    "    least_contributing_clients = sorted_clients[-3:] if len(sorted_clients) >= 3 else sorted_clients\n",
    "    return sorted_clients, least_contributing_clients\n",
    "\n",
    "\n",
    "def calculate_weight_magnitude(global_model, client_update):\n",
    "    \"\"\"Compute the L2 norm of parameter differences between global and client weights.\"\"\"\n",
    "    weight_diff = 0.0\n",
    "    global_params = [p.detach().cpu().numpy() for p in global_model.parameters()]\n",
    "\n",
    "    for gp, cu in zip(global_params, client_update):\n",
    "        try:\n",
    "            diff = np.linalg.norm(gp - cu)\n",
    "            if not np.isnan(diff):\n",
    "                weight_diff += diff\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return float(weight_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edfb4fa-2a88-48f9-a070-e42770855b74",
   "metadata": {
    "id": "2GrOePXWWEzi",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='Brown'>***Clients Functions***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe8ff6-416d-4e64-9fbf-441497bd4fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def General_Client():\n",
    "    def client_fn(cid: int, Round: int) -> FlowerClient:\n",
    "        clients_ids_list = TrainingListPerRound[Round]\n",
    "        if int(cid) in clients_ids_list:\n",
    "            # Instantiate client model using current LLM-tuned architecture\n",
    "            net = Net(\n",
    "                input_dim=98,\n",
    "                output_dim=15,\n",
    "                h1=int(TRAIN_CFG.get(\"hidden_size_1\", 64)),\n",
    "                h2=int(TRAIN_CFG.get(\"hidden_size_2\", 32)),\n",
    "                dropout=float(TRAIN_CFG.get(\"dropout\", 0.0)),\n",
    "                activation=str(TRAIN_CFG.get(\"activation\", \"relu\")),\n",
    "                batch_norm=bool(TRAIN_CFG.get(\"batch_norm\", False)),\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            trainloader = Dataloaders[Round][int(cid)]\n",
    "            return FlowerClient(cid, net, trainloader, Round)\n",
    "        else:\n",
    "            raise ValueError(f\"Client ID {cid} not found in the list for round {Round}\")\n",
    "    return client_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c63cf7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<font color='Brown'>***FL Strategy***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590c9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Global_Models = {}\n",
    "\n",
    "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
    "    def __init__(self, additional_argument, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.additional_argument = additional_argument\n",
    "\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        rnd: int,\n",
    "        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],\n",
    "        failures: List[BaseException],\n",
    "    ) -> Optional[fl.common.NDArrays]:\n",
    "        \"\"\"Aggregate client updates, rebuild the global model with current TRAIN_CFG, and save checkpoint.\"\"\"\n",
    "        aggregated_parameters_tuple = super().aggregate_fit(rnd, results, failures)\n",
    "        if aggregated_parameters_tuple is None:\n",
    "            return None\n",
    "\n",
    "        aggregated_parameters, _ = aggregated_parameters_tuple\n",
    "        if aggregated_parameters is not None:\n",
    "            # Convert FL parameters to ndarray list\n",
    "            aggregated_weights: List[np.ndarray] = fl.common.parameters_to_ndarrays(aggregated_parameters)\n",
    "\n",
    "            # --- Rebuild model using the LLM-controlled architecture ---\n",
    "            model_cfg = {\n",
    "                \"input_dim\": 98,\n",
    "                \"output_dim\": 15,\n",
    "                \"h1\": int(TRAIN_CFG.get(\"hidden_size_1\", 64)),\n",
    "                \"h2\": int(TRAIN_CFG.get(\"hidden_size_2\", 32)),\n",
    "                \"dropout\": float(TRAIN_CFG.get(\"dropout\", 0.0)),\n",
    "                \"activation\": str(TRAIN_CFG.get(\"activation\", \"relu\")),\n",
    "                \"batch_norm\": bool(TRAIN_CFG.get(\"batch_norm\", False)),\n",
    "            }\n",
    "            Global_Models[self.additional_argument] = Net(**model_cfg)\n",
    "\n",
    "            # Map FL weights into model’s state dict\n",
    "            params_dict = zip(Global_Models[self.additional_argument].state_dict().keys(), aggregated_weights)\n",
    "            state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "            Global_Models[self.additional_argument].load_state_dict(state_dict, strict=True)\n",
    "\n",
    "            # Ensure path exists\n",
    "            os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "            # Save checkpoint for current round\n",
    "            save_path = f\"{PATH}/GlobalModel_{self.additional_argument}.pth\"\n",
    "            torch.save(Global_Models[self.additional_argument].state_dict(), save_path)\n",
    "            print(f\"💾 Saved aggregated global model: {save_path}\")\n",
    "\n",
    "        return aggregated_parameters_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_config(server_round: int):\n",
    "    \"\"\"Return the current LLM-tuned configuration for this federated round.\"\"\"\n",
    "    return {\n",
    "        # Round tracking\n",
    "        \"current_round\": int(server_round),\n",
    "\n",
    "        # Core training hyperparameters\n",
    "        \"local_epochs\": int(TRAIN_CFG.get(\"epochs\", EPOCHS)),\n",
    "        \"learning_rate\": float(TRAIN_CFG.get(\"learning_rate\", LEARNING_RATE)),\n",
    "        \"weight_decay\": float(TRAIN_CFG.get(\"weight_decay\", 0.0)),\n",
    "\n",
    "        # Regularization\n",
    "        \"dropout\": float(TRAIN_CFG.get(\"dropout\", 0.0)),\n",
    "        \"label_smoothing\": float(TRAIN_CFG.get(\"label_smoothing\", 0.0)),\n",
    "        \"gradient_clip_norm\": float(TRAIN_CFG.get(\"gradient_clip_norm\", 0.0)),\n",
    "\n",
    "        # Architecture awareness (for reference / logging)\n",
    "        \"hidden_size_1\": int(TRAIN_CFG.get(\"hidden_size_1\", 64)),\n",
    "        \"hidden_size_2\": int(TRAIN_CFG.get(\"hidden_size_2\", 32)),\n",
    "        \"activation\": str(TRAIN_CFG.get(\"activation\", \"relu\")),\n",
    "        \"batch_norm\": bool(TRAIN_CFG.get(\"batch_norm\", False)),\n",
    "\n",
    "        # Optimizer & scheduler (kept for LLM-driven experimentation)\n",
    "        \"optimizer\": str(TRAIN_CFG.get(\"optimizer\", \"adam\")),\n",
    "        \"scheduler\": str(TRAIN_CFG.get(\"scheduler\", \"none\")),\n",
    "\n",
    "        # Meta info\n",
    "        \"batch_size\": int(TRAIN_CFG.get(\"batch_size\", BATCH_SIZE)),\n",
    "        \"early_stopping_patience\": int(TRAIN_CFG.get(\"early_stopping_patience\", 0)),\n",
    "        \"notes\": TRAIN_CFG.get(\"notes\", f\"Auto-updated from round {server_round}\"),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d181247",
   "metadata": {
    "tags": []
   },
   "source": [
    "***Running the Generalized FL Round***\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a886e135-e368-4660-bcba-475038f48597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Initializing base model using current TRAIN_CFG...\n",
      "layer_1.weight \t torch.Size([64, 98])\n",
      "layer_1.bias \t torch.Size([64])\n",
      "layer_2.weight \t torch.Size([32, 64])\n",
      "layer_2.bias \t torch.Size([32])\n",
      "layer_out.weight \t torch.Size([15, 32])\n",
      "layer_out.bias \t torch.Size([15])\n",
      "✅ Saved initial model to llm-test-2/0_Input_Random_model_Net.pth\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Initialize and save a random starting model (Round 0)\n",
    "# ============================================================\n",
    "print(\"🔧 Initializing base model using current TRAIN_CFG...\")\n",
    "\n",
    "Random = Net(\n",
    "    input_dim=98,\n",
    "    output_dim=15,\n",
    "    h1=TRAIN_CFG.get(\"hidden_size_1\", 64),\n",
    "    h2=TRAIN_CFG.get(\"hidden_size_2\", 32),\n",
    "    dropout=TRAIN_CFG.get(\"dropout\", 0.0),\n",
    "    activation=TRAIN_CFG.get(\"activation\", \"relu\"),\n",
    "    batch_norm=TRAIN_CFG.get(\"batch_norm\", False),\n",
    ")\n",
    "\n",
    "# Show layer shapes for sanity check\n",
    "for param_tensor in Random.state_dict():\n",
    "    print(param_tensor, \"\\t\", Random.state_dict()[param_tensor].size())\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "# Save initial random model weights\n",
    "init_model_path = os.path.join(PATH, \"0_Input_Random_model_Net.pth\")\n",
    "torch.save(Random.state_dict(), init_model_path)\n",
    "\n",
    "print(f\"✅ Saved initial model to {init_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834689ad-179a-4cf7-860e-2ffb1008c6fa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Initial Global Model\n",
      "✅ Pretrained model found — loading weights.\n",
      "\n",
      "======================================================================\n",
      "🚀 Starting Federated Round 1\n",
      "======================================================================\n",
      "[Round 1] Using TRAIN_CFG:\n",
      "  round: 0\n",
      "  learning_rate: 0.0025\n",
      "  epochs: 10\n",
      "  batch_size: 64\n",
      "  optimizer: adamw\n",
      "  momentum: 0.0\n",
      "  weight_decay: 0.0\n",
      "  dropout: 0.0\n",
      "  gradient_clip_norm: 0.0\n",
      "  label_smoothing: 0.0\n",
      "  scheduler: none\n",
      "  scheduler_step: 2\n",
      "  scheduler_gamma: 0.9\n",
      "  early_stopping_patience: 0\n",
      "  fraction_fit: 1.0\n",
      "  aggregation: FedAvg\n",
      "  hidden_size_1: 64\n",
      "  hidden_size_2: 32\n",
      "  activation: relu\n",
      "  batch_norm: False\n",
      "  notes: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 18:55:27,463\tINFO worker.py:2012 -- Started a local Ray instance.\n",
      "[2025-11-05 18:55:27,474 I 51726 51726] logging.cc:303: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DefaultActor pid=55903)\u001b[0m [Client 6, round 1] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DefaultActor pid=55903)\u001b[0m /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "\u001b[36m(DefaultActor pid=55903)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DefaultActor pid=55903)\u001b[0m [Client 2, round 1] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n",
      "\u001b[36m(DefaultActor pid=55903)\u001b[0m [Client 7, round 1] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n",
      "\u001b[36m(DefaultActor pid=55903)\u001b[0m [Client 5, round 1] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n",
      "\u001b[36m(DefaultActor pid=55903)\u001b[0m [Client 3, round 1] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n",
      "\u001b[36m(DefaultActor pid=55903)\u001b[0m [Client 8, round 1] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n",
      "\u001b[36m(DefaultActor pid=55903)\u001b[0m [Client 4, round 1] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n",
      "\u001b[36m(DefaultActor pid=55903)\u001b[0m [Client 9, round 1] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n",
      "\u001b[36m(DefaultActor pid=55903)\u001b[0m [Client 1, round 1] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n",
      "\u001b[36m(DefaultActor pid=55903)\u001b[0m [Client 0, round 1] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n",
      "💾 Saved aggregated global model: llm-test-2/GlobalModel_1.pth\n",
      "\n",
      "🔁 Loading Global Model for Round 1 ...\n",
      "✅ Model loaded from llm-test-2/GlobalModel_1.pth\n",
      "\n",
      "📊 Evaluating and optimizing Round 1 with LLM ...\n",
      "🧮 Evaluating global model (Round 1) ...\n",
      "📊 Validation Metrics:\n",
      "  val_loss: 1.8326\n",
      "  val_accuracy: 0.5000\n",
      "  precision_macro: 0.0833\n",
      "  recall_macro: 0.1667\n",
      "  f1_macro: 0.1111\n",
      "\n",
      "🤖 Requesting LLM optimization suggestions...\n",
      "Trying LLM: google/gemma-3-4b-it\n",
      "❌ Failed google/gemma-3-4b-it: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.\n",
      "401 Client Error. (Request ID: Root=1-690b9daf-743f2a853e76ad8a0611e587;e2090354-b868-49a1-982d-2c264d03eda6)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
      "Trying LLM: microsoft/Phi-3-mini-4k-instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e08b8d03244398a9af4d1f70f7bcba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-11-05 18:55:53,441 E 55763 55763] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded microsoft/Phi-3-mini-4k-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2025-11-05 18:55:56,483 E 55846 55846] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(DefaultActor pid=55903)\u001b[0m [2025-11-05 18:56:02,130 E 55903 56344] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2025-11-05 18:56:02,738 E 51726 55902] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM finished generating (### DONE detected).\n",
      "\n",
      "==============================\n",
      "🧠 Full LLM Output (Round 1)\n",
      "==============================\n",
      "You are an expert federated learning optimization assistant integrated into an automated ML pipeline.\n",
      "Your purpose is to improve generalization, validation precision, and accuracy across rounds of federated training.\n",
      "You will be provided with:\n",
      "  1️⃣ CURRENT_CONFIG — the current round’s hyperparameters,\n",
      "  2️⃣ ROUND_METRICS — validation metrics for this round,\n",
      "  3️⃣ DATA_INFO — dataset and hardware context,\n",
      "  4️⃣ LAST_5_CONFIGS — up to five prior configurations for trend analysis.\n",
      "\n",
      "# Instructions\n",
      "- Analyze performance and trends.\n",
      "- Suggest small, data-driven hyperparameter updates.\n",
      "- Avoid overfitting; stability is more important than drastic improvements.\n",
      "- Prefer conservative changes in learning rate and epochs.\n",
      "- If val_loss ↑ while accuracy ↑ → add mild regularization (dropout or weight_decay).\n",
      "- If val_loss ↓ but accuracy stagnates → slightly increase epochs or learning_rate.\n",
      "- If both degrade → reduce learning_rate and add regularization.\n",
      "- If metrics plateau for 3+ rounds → only minor exploration.\n",
      "\n",
      "# STRICT OUTPUT FORMAT\n",
      "Return **only** a single JSON object.\n",
      "Never include markdown code fences (no ```json or ```).\n",
      "Do not include explanations, text, comments, or reasoning outside the JSON object.\n",
      "\n",
      "The JSON must follow this schema exactly:\n",
      "{\n",
      "  \"learning_rate\": <float>,\n",
      "  \"epochs\": <int>,\n",
      "  \"weight_decay\": <float>,\n",
      "  \"dropout\": <float>,\n",
      "  \"batch_size\": <int>,\n",
      "  \"optimizer\": <\"adam\" | \"adamw\" | \"sgd\">,\n",
      "  \"momentum\": <float>,\n",
      "  \"gradient_clip_norm\": <float>,\n",
      "  \"label_smoothing\": <float>,\n",
      "  \"scheduler\": <\"none\" | \"StepLR\" | \"ReduceLROnPlateau\" | \"CosineAnnealing\">,\n",
      "  \"scheduler_step\": <int>,\n",
      "  \"scheduler_gamma\": <float>,\n",
      "  \"early_stopping_patience\": <int>,\n",
      "  \"fraction_fit\": <float>,\n",
      "  \"aggregation\": <\"FedAvg\" | \"FedProx\" | \"mean\">,\n",
      "  \"hidden_size_1\": <int>,\n",
      "  \"hidden_size_2\": <int>,\n",
      "  \"activation\": <\"relu\" | \"gelu\" | \"tanh\" | \"elu\">,\n",
      "  \"batch_norm\": <bool>,\n",
      "  \"notes\": \"<short rationale under 20 words>\"\n",
      "}\n",
      "\n",
      "CURRENT_CONFIG:\n",
      "{\n",
      "  \"round\": 1,\n",
      "  \"learning_rate\": 0.0025,\n",
      "  \"epochs\": 10,\n",
      "  \"batch_size\": 64,\n",
      "  \"optimizer\": \"adamw\",\n",
      "  \"momentum\": 0.0,\n",
      "  \"weight_decay\": 0.0,\n",
      "  \"dropout\": 0.0,\n",
      "  \"gradient_clip_norm\": 0.0,\n",
      "  \"label_smoothing\": 0.0,\n",
      "  \"scheduler\": \"none\",\n",
      "  \"scheduler_step\": 2,\n",
      "  \"scheduler_gamma\": 0.9,\n",
      "  \"early_stopping_patience\": 0,\n",
      "  \"fraction_fit\": 1.0,\n",
      "  \"aggregation\": \"FedAvg\",\n",
      "  \"hidden_size_1\": 64,\n",
      "  \"hidden_size_2\": 32,\n",
      "  \"activation\": \"relu\",\n",
      "  \"batch_norm\": false,\n",
      "  \"notes\": \"Auto-updated after round 1\"\n",
      "}\n",
      "\n",
      "LAST_ROUND_METRICS:\n",
      "{\n",
      "  \"val_loss\": 1.832581741587321,\n",
      "  \"val_accuracy\": 0.5,\n",
      "  \"precision_macro\": 0.08333333333333333,\n",
      "  \"recall_macro\": 0.16666666666666666,\n",
      "  \"f1_macro\": 0.1111111111111111\n",
      "}\n",
      "\n",
      "DATA_INFO:\n",
      "{\n",
      "  \"num_clients\": 10,\n",
      "  \"round_size\": 3840,\n",
      "  \"batch_size\": 64,\n",
      "  \"n_features\": 98,\n",
      "  \"n_classes\": 15,\n",
      "  \"device\": \"cuda:0\"\n",
      "}\n",
      "\n",
      "LAST_5_CONFIGS:\n",
      "[]\n",
      "\n",
      "HISTORICAL_SUMMARY:\n",
      "{\n",
      "  \"n_previous\": 0,\n",
      "  \"avg_lr\": null,\n",
      "  \"avg_epochs\": null,\n",
      "  \"avg_wd\": null,\n",
      "  \"avg_dropout\": null\n",
      "}\n",
      "\n",
      "Return *only* the JSON object matching the above schema, nothing else.\n",
      "# Your task:\n",
      "Analyze all inputs and output ONLY the JSON object above — nothing else.\n",
      "After the JSON, append a new line that says exactly without quotation marks: \"\n",
      "==============================\n",
      "\n",
      "⚠️ No valid JSON detected — keeping previous configuration.\n",
      "⚠️ Attempt 1 failed — LLM did not return valid JSON. Retrying...\n",
      "\n",
      "✅ LLM finished generating (### DONE detected).\n",
      "\n",
      "==============================\n",
      "🧠 Full LLM Output (Round 1)\n",
      "==============================\n",
      "You are an expert federated learning optimization assistant integrated into an automated ML pipeline.\n",
      "Your purpose is to improve generalization, validation precision, and accuracy across rounds of federated training.\n",
      "You will be provided with:\n",
      "  1️⃣ CURRENT_CONFIG — the current round’s hyperparameters,\n",
      "  2️⃣ ROUND_METRICS — validation metrics for this round,\n",
      "  3️⃣ DATA_INFO — dataset and hardware context,\n",
      "  4️⃣ LAST_5_CONFIGS — up to five prior configurations for trend analysis.\n",
      "\n",
      "# Instructions\n",
      "- Analyze performance and trends.\n",
      "- Suggest small, data-driven hyperparameter updates.\n",
      "- Avoid overfitting; stability is more important than drastic improvements.\n",
      "- Prefer conservative changes in learning rate and epochs.\n",
      "- If val_loss ↑ while accuracy ↑ → add mild regularization (dropout or weight_decay).\n",
      "- If val_loss ↓ but accuracy stagnates → slightly increase epochs or learning_rate.\n",
      "- If both degrade → reduce learning_rate and add regularization.\n",
      "- If metrics plateau for 3+ rounds → only minor exploration.\n",
      "\n",
      "# STRICT OUTPUT FORMAT\n",
      "Return **only** a single JSON object.\n",
      "Never include markdown code fences (no ```json or ```).\n",
      "Do not include explanations, text, comments, or reasoning outside the JSON object.\n",
      "\n",
      "The JSON must follow this schema exactly:\n",
      "{\n",
      "  \"learning_rate\": <float>,\n",
      "  \"epochs\": <int>,\n",
      "  \"weight_decay\": <float>,\n",
      "  \"dropout\": <float>,\n",
      "  \"batch_size\": <int>,\n",
      "  \"optimizer\": <\"adam\" | \"adamw\" | \"sgd\">,\n",
      "  \"momentum\": <float>,\n",
      "  \"gradient_clip_norm\": <float>,\n",
      "  \"label_smoothing\": <float>,\n",
      "  \"scheduler\": <\"none\" | \"StepLR\" | \"ReduceLROnPlateau\" | \"CosineAnnealing\">,\n",
      "  \"scheduler_step\": <int>,\n",
      "  \"scheduler_gamma\": <float>,\n",
      "  \"early_stopping_patience\": <int>,\n",
      "  \"fraction_fit\": <float>,\n",
      "  \"aggregation\": <\"FedAvg\" | \"FedProx\" | \"mean\">,\n",
      "  \"hidden_size_1\": <int>,\n",
      "  \"hidden_size_2\": <int>,\n",
      "  \"activation\": <\"relu\" | \"gelu\" | \"tanh\" | \"elu\">,\n",
      "  \"batch_norm\": <bool>,\n",
      "  \"notes\": \"<short rationale under 20 words>\"\n",
      "}\n",
      "\n",
      "CURRENT_CONFIG:\n",
      "{\n",
      "  \"round\": 1,\n",
      "  \"learning_rate\": 0.0025,\n",
      "  \"epochs\": 10,\n",
      "  \"batch_size\": 64,\n",
      "  \"optimizer\": \"adamw\",\n",
      "  \"momentum\": 0.0,\n",
      "  \"weight_decay\": 0.0,\n",
      "  \"dropout\": 0.0,\n",
      "  \"gradient_clip_norm\": 0.0,\n",
      "  \"label_smoothing\": 0.0,\n",
      "  \"scheduler\": \"none\",\n",
      "  \"scheduler_step\": 2,\n",
      "  \"scheduler_gamma\": 0.9,\n",
      "  \"early_stopping_patience\": 0,\n",
      "  \"fraction_fit\": 1.0,\n",
      "  \"aggregation\": \"FedAvg\",\n",
      "  \"hidden_size_1\": 64,\n",
      "  \"hidden_size_2\": 32,\n",
      "  \"activation\": \"relu\",\n",
      "  \"batch_norm\": false,\n",
      "  \"notes\": \"Auto-updated after round 1\"\n",
      "}\n",
      "\n",
      "LAST_ROUND_METRICS:\n",
      "{\n",
      "  \"val_loss\": 1.832581741587321,\n",
      "  \"val_accuracy\": 0.5,\n",
      "  \"precision_macro\": 0.08333333333333333,\n",
      "  \"recall_macro\": 0.16666666666666666,\n",
      "  \"f1_macro\": 0.1111111111111111\n",
      "}\n",
      "\n",
      "DATA_INFO:\n",
      "{\n",
      "  \"num_clients\": 10,\n",
      "  \"round_size\": 3840,\n",
      "  \"batch_size\": 64,\n",
      "  \"n_features\": 98,\n",
      "  \"n_classes\": 15,\n",
      "  \"device\": \"cuda:0\"\n",
      "}\n",
      "\n",
      "LAST_5_CONFIGS:\n",
      "[]\n",
      "\n",
      "HISTORICAL_SUMMARY:\n",
      "{\n",
      "  \"n_previous\": 0,\n",
      "  \"avg_lr\": null,\n",
      "  \"avg_epochs\": null,\n",
      "  \"avg_wd\": null,\n",
      "  \"avg_dropout\": null\n",
      "}\n",
      "\n",
      "Return *only* the JSON object matching the above schema, nothing else.\n",
      "# Your task:\n",
      "Analyze all inputs and output ONLY the JSON object above — nothing else.\n",
      "After the JSON, append a new line that says exactly without quotation marks: \"\n",
      "==============================\n",
      "\n",
      "⚠️ No valid JSON detected — keeping previous configuration.\n",
      "⚠️ Attempt 2 failed — LLM did not return valid JSON. Retrying...\n",
      "\n",
      "⚠️ LLM did not return valid JSON after retries — keeping current hyperparameters.\n",
      "\n",
      "✅ Wrote llm-test-2/train_config_round_2.json\n",
      "LLM suggestions (applied):\n",
      "  learning_rate = 0.0025\n",
      "  epochs        = 10\n",
      "  weight_decay  = 0.0\n",
      "  dropout       = 0.0\n",
      "------------------------------------------------------\n",
      "🔄 Global TRAIN_CFG updated for next round.\n",
      "\n",
      "\n",
      "======================================================================\n",
      "🚀 Starting Federated Round 2\n",
      "======================================================================\n",
      "[Round 2] Using TRAIN_CFG:\n",
      "  round: 1\n",
      "  learning_rate: 0.0025\n",
      "  epochs: 10\n",
      "  batch_size: 64\n",
      "  optimizer: adamw\n",
      "  momentum: 0.0\n",
      "  weight_decay: 0.0\n",
      "  dropout: 0.0\n",
      "  gradient_clip_norm: 0.0\n",
      "  label_smoothing: 0.0\n",
      "  scheduler: none\n",
      "  scheduler_step: 2\n",
      "  scheduler_gamma: 0.9\n",
      "  early_stopping_patience: 0\n",
      "  fraction_fit: 1.0\n",
      "  aggregation: FedAvg\n",
      "  hidden_size_1: 64\n",
      "  hidden_size_2: 32\n",
      "  activation: relu\n",
      "  batch_norm: False\n",
      "  notes: LLM-updated after round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 18:56:46,290\tINFO worker.py:2012 -- Started a local Ray instance.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-11-05 18:56:52,748 I 61289 61289] logging.cc:303: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DefaultActor pid=61288)\u001b[0m [Client 9, round 2] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DefaultActor pid=61288)\u001b[0m /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "\u001b[36m(DefaultActor pid=61288)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DefaultActor pid=61288)\u001b[0m [Client 1, round 2] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n",
      "\u001b[36m(DefaultActor pid=61288)\u001b[0m [Client 4, round 2] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n",
      "\u001b[36m(DefaultActor pid=61288)\u001b[0m [Client 5, round 2] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n",
      "\u001b[36m(DefaultActor pid=61288)\u001b[0m [Client 0, round 2] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n",
      "\u001b[36m(DefaultActor pid=61288)\u001b[0m [Client 7, round 2] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n",
      "\u001b[36m(DefaultActor pid=61288)\u001b[0m [Client 6, round 2] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n",
      "\u001b[36m(DefaultActor pid=61288)\u001b[0m [Client 8, round 2] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n",
      "\u001b[36m(DefaultActor pid=61288)\u001b[0m [Client 2, round 2] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n",
      "\u001b[36m(DefaultActor pid=61288)\u001b[0m [Client 3, round 2] fit, epochs=10, lr=0.0025, wd=0.0, clip_norm=0.0\n",
      "💾 Saved aggregated global model: llm-test-2/GlobalModel_2.pth\n",
      "\n",
      "🔁 Loading Global Model for Round 2 ...\n",
      "✅ Model loaded from llm-test-2/GlobalModel_2.pth\n",
      "\n",
      "📊 Evaluating and optimizing Round 2 with LLM ...\n",
      "🧮 Evaluating global model (Round 2) ...\n",
      "📊 Validation Metrics:\n",
      "  val_loss: 0.4352\n",
      "  val_accuracy: 0.9484\n",
      "  precision_macro: 0.9216\n",
      "  recall_macro: 0.9339\n",
      "  f1_macro: 0.9245\n",
      "\n",
      "🤖 Requesting LLM optimization suggestions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-11-05 18:57:13,085 E 60812 60812] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-11-05 18:57:16,273 E 61236 61236] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(DefaultActor pid=61288)\u001b[0m [2025-11-05 18:57:22,487 E 61288 61676] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2025-11-05 18:57:22,792 E 51726 61285] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM finished generating (### DONE detected).\n",
      "\n",
      "==============================\n",
      "🧠 Full LLM Output (Round 2)\n",
      "==============================\n",
      "You are an expert federated learning optimization assistant integrated into an automated ML pipeline.\n",
      "Your purpose is to improve generalization, validation precision, and accuracy across rounds of federated training.\n",
      "You will be provided with:\n",
      "  1️⃣ CURRENT_CONFIG — the current round’s hyperparameters,\n",
      "  2️⃣ ROUND_METRICS — validation metrics for this round,\n",
      "  3️⃣ DATA_INFO — dataset and hardware context,\n",
      "  4️⃣ LAST_5_CONFIGS — up to five prior configurations for trend analysis.\n",
      "\n",
      "# Instructions\n",
      "- Analyze performance and trends.\n",
      "- Suggest small, data-driven hyperparameter updates.\n",
      "- Avoid overfitting; stability is more important than drastic improvements.\n",
      "- Prefer conservative changes in learning rate and epochs.\n",
      "- If val_loss ↑ while accuracy ↑ → add mild regularization (dropout or weight_decay).\n",
      "- If val_loss ↓ but accuracy stagnates → slightly increase epochs or learning_rate.\n",
      "- If both degrade → reduce learning_rate and add regularization.\n",
      "- If metrics plateau for 3+ rounds → only minor exploration.\n",
      "\n",
      "# STRICT OUTPUT FORMAT\n",
      "Return **only** a single JSON object.\n",
      "Never include markdown code fences (no ```json or ```).\n",
      "Do not include explanations, text, comments, or reasoning outside the JSON object.\n",
      "\n",
      "The JSON must follow this schema exactly:\n",
      "{\n",
      "  \"learning_rate\": <float>,\n",
      "  \"epochs\": <int>,\n",
      "  \"weight_decay\": <float>,\n",
      "  \"dropout\": <float>,\n",
      "  \"batch_size\": <int>,\n",
      "  \"optimizer\": <\"adam\" | \"adamw\" | \"sgd\">,\n",
      "  \"momentum\": <float>,\n",
      "  \"gradient_clip_norm\": <float>,\n",
      "  \"label_smoothing\": <float>,\n",
      "  \"scheduler\": <\"none\" | \"StepLR\" | \"ReduceLROnPlateau\" | \"CosineAnnealing\">,\n",
      "  \"scheduler_step\": <int>,\n",
      "  \"scheduler_gamma\": <float>,\n",
      "  \"early_stopping_patience\": <int>,\n",
      "  \"fraction_fit\": <float>,\n",
      "  \"aggregation\": <\"FedAvg\" | \"FedProx\" | \"mean\">,\n",
      "  \"hidden_size_1\": <int>,\n",
      "  \"hidden_size_2\": <int>,\n",
      "  \"activation\": <\"relu\" | \"gelu\" | \"tanh\" | \"elu\">,\n",
      "  \"batch_norm\": <bool>,\n",
      "  \"notes\": \"<short rationale under 20 words>\"\n",
      "}\n",
      "\n",
      "CURRENT_CONFIG:\n",
      "{\n",
      "  \"round\": 2,\n",
      "  \"learning_rate\": 0.0025,\n",
      "  \"epochs\": 10,\n",
      "  \"batch_size\": 64,\n",
      "  \"optimizer\": \"adamw\",\n",
      "  \"momentum\": 0.0,\n",
      "  \"weight_decay\": 0.0,\n",
      "  \"dropout\": 0.0,\n",
      "  \"gradient_clip_norm\": 0.0,\n",
      "  \"label_smoothing\": 0.0,\n",
      "  \"scheduler\": \"none\",\n",
      "  \"scheduler_step\": 2,\n",
      "  \"scheduler_gamma\": 0.9,\n",
      "  \"early_stopping_patience\": 0,\n",
      "  \"fraction_fit\": 1.0,\n",
      "  \"aggregation\": \"FedAvg\",\n",
      "  \"hidden_size_1\": 64,\n",
      "  \"hidden_size_2\": 32,\n",
      "  \"activation\": \"relu\",\n",
      "  \"batch_norm\": false,\n",
      "  \"notes\": \"Auto-updated after round 2\"\n",
      "}\n",
      "\n",
      "LAST_ROUND_METRICS:\n",
      "{\n",
      "  \"val_loss\": 0.435245224348704,\n",
      "  \"val_accuracy\": 0.9483583333333333,\n",
      "  \"precision_macro\": 0.9215513281914172,\n",
      "  \"recall_macro\": 0.9339416666666666,\n",
      "  \"f1_macro\": 0.9244794726041916\n",
      "}\n",
      "\n",
      "DATA_INFO:\n",
      "{\n",
      "  \"num_clients\": 10,\n",
      "  \"round_size\": 3840,\n",
      "  \"batch_size\": 64,\n",
      "  \"n_features\": 98,\n",
      "  \"n_classes\": 15,\n",
      "  \"device\": \"cuda:0\"\n",
      "}\n",
      "\n",
      "LAST_5_CONFIGS:\n",
      "[\n",
      "  {\n",
      "    \"round\": 1,\n",
      "    \"learning_rate\": 0.0025,\n",
      "    \"epochs\": 10,\n",
      "    \"batch_size\": 64,\n",
      "    \"optimizer\": \"adamw\",\n",
      "    \"momentum\": 0.0,\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"dropout\": 0.0,\n",
      "    \"gradient_clip_norm\": 0.0,\n",
      "    \"label_smoothing\": 0.0,\n",
      "    \"scheduler\": \"none\",\n",
      "    \"scheduler_step\": 2,\n",
      "    \"scheduler_gamma\": 0.9,\n",
      "    \"early_stopping_patience\": 0,\n",
      "    \"fraction_fit\": 1.0,\n",
      "    \"aggregation\": \"FedAvg\",\n",
      "    \"hidden_size_1\": 64,\n",
      "    \"hidden_size_2\": 32,\n",
      "    \"activation\": \"relu\",\n",
      "    \"batch_norm\": false,\n",
      "    \"notes\": \"\"\n",
      "  }\n",
      "]\n",
      "\n",
      "HISTORICAL_SUMMARY:\n",
      "{\n",
      "  \"n_previous\": 1,\n",
      "  \"avg_lr\": 0.0025,\n",
      "  \"avg_epochs\": 10,\n",
      "  \"avg_wd\": 0.0,\n",
      "  \"avg_dropout\": 0.0\n",
      "}\n",
      "\n",
      "Return *only* the JSON object matching the above schema, nothing else.\n",
      "# Your task:\n",
      "Analyze all inputs and output ONLY the JSON object above — nothing else.\n",
      "After the JSON, append a new line that says exactly without quotation marks: \"\n",
      "==============================\n",
      "\n",
      "⚠️ No valid JSON detected — keeping previous configuration.\n",
      "⚠️ Attempt 1 failed — LLM did not return valid JSON. Retrying...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 106\u001b[0m\n\u001b[1;32m    104\u001b[0m     Global_Models[Round]\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📊 Evaluating and optimizing Round \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRound\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with LLM ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mllm_step_after_round\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGlobal_Models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRound\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== 🎯 Training + LLM Optimization Complete =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll models and JSON configs saved under:\u001b[39m\u001b[38;5;124m\"\u001b[39m, PATH)\n",
      "Cell \u001b[0;32mIn[25], line 84\u001b[0m, in \u001b[0;36mllm_step_after_round\u001b[0;34m(round_idx, model_for_eval)\u001b[0m\n\u001b[1;32m     82\u001b[0m suggestions \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):  \u001b[38;5;66;03m# retry once if JSON fails\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     suggestions \u001b[38;5;241m=\u001b[39m \u001b[43mllm_review_and_suggest_llm3\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mround_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mround_idx\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m suggestions:\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 285\u001b[0m, in \u001b[0;36mllm_review_and_suggest_llm3\u001b[0;34m(current_cfg, metrics, info, base_path, round_idx)\u001b[0m\n\u001b[1;32m    215\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124mYou are an expert federated learning optimization assistant integrated into an automated ML pipeline.\u001b[39m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124mYour purpose is to improve generalization, validation precision, and accuracy across rounds of federated training.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124mAfter the JSON, append a new line that says exactly without quotation marks: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### DONE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# --- Generate output, wait until sentinel ---\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_until_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# --- Show and save full output ---\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m==============================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 155\u001b[0m, in \u001b[0;36mgenerate_until_done\u001b[0;34m(pipe, prompt, max_wait, max_retries)\u001b[0m\n\u001b[1;32m    153\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_retries):\n\u001b[0;32m--> 155\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_GEN_ARGS\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    156\u001b[0m     output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### DONE\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:332\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/pipelines/base.py:1467\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1460\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1461\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1464\u001b[0m         )\n\u001b[1;32m   1465\u001b[0m     )\n\u001b[1;32m   1466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/pipelines/base.py:1474\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1473\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1474\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1475\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1476\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/pipelines/base.py:1374\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1373\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1374\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:432\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    430\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 432\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    435\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/generation/utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[0;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[1;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2579\u001b[0m ):\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/generation/utils.py:2787\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2787\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   2790\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   2791\u001b[0m     outputs,\n\u001b[1;32m   2792\u001b[0m     model_kwargs,\n\u001b[1;32m   2793\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2794\u001b[0m )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/phi3/modeling_phi3.py:465\u001b[0m, in \u001b[0;36mPhi3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    447\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[1;32m    448\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/utils/generic.py:1064\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1064\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/phi3/modeling_phi3.py:401\u001b[0m, in \u001b[0;36mPhi3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[0;32m--> 401\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[1;32m    413\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    414\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    415\u001b[0m )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/phi3/modeling_phi3.py:263\u001b[0m, in \u001b[0;36mPhi3DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    261\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m--> 263\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_attn_dropout(hidden_states)  \u001b[38;5;66;03m# main diff with Llama\u001b[39;00m\n\u001b[1;32m    275\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/phi3/modeling_phi3.py:211\u001b[0m, in \u001b[0;36mPhi3Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m attention_interface(\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    200\u001b[0m     query_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    208\u001b[0m )\n\u001b[1;32m    210\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 211\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Loading Initial Global Model\")\n",
    "\n",
    "# torch.set_float32_matmul_precision(\"medium\")  # or \"high\" for A100s (ENABLE FOR LONG RUNS)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Initialize base model using the latest TRAIN_CFG\n",
    "# (ensures architecture matches LLM-tuned parameters)\n",
    "# ------------------------------------------------------\n",
    "Global_Models[0] = Net(\n",
    "    h1=int(TRAIN_CFG.get(\"hidden_size_1\", 64)),\n",
    "    h2=int(TRAIN_CFG.get(\"hidden_size_2\", 32)),\n",
    "    dropout=float(TRAIN_CFG.get(\"dropout\", 0.0)),\n",
    "    activation=str(TRAIN_CFG.get(\"activation\", \"relu\")),\n",
    "    batch_norm=bool(TRAIN_CFG.get(\"batch_norm\", False)),\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Load existing checkpoint if available\n",
    "# ------------------------------------------------------\n",
    "model_path = f\"{PATH}/0_Input_Random_model_Net.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"✅ Pretrained model found — loading weights.\")\n",
    "    Global_Models[0].load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    print(\"⚠️ No pretrained model found — starting fresh.\")\n",
    "Global_Models[0].train()\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Prepare client lists for all rounds\n",
    "# ------------------------------------------------------\n",
    "TrainingListPerRound = {rnd: [int(c) for c in range(NUM_CLIENTS)] for rnd in range(1, ROUNDS + 1)}\n",
    "\n",
    "# ======================================================\n",
    "# Federated Learning Rounds with LLM Review\n",
    "# ======================================================\n",
    "for Round in range(1, ROUNDS + 1):\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"🚀 Starting Federated Round {Round}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # 1️⃣ Load (or create) LLM-tuned training configuration\n",
    "    # ------------------------------------------------------\n",
    "    cfg_path = os.path.join(PATH, f\"train_config_round_{Round}.json\")\n",
    "    TRAIN_CFG = load_or_create_config(cfg_path)\n",
    "    use_cfg_in_globals(TRAIN_CFG)\n",
    "\n",
    "    print(f\"[Round {Round}] Using TRAIN_CFG:\")\n",
    "    for k, v in TRAIN_CFG.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # 2️⃣ Initialize Federated Strategy\n",
    "    # ------------------------------------------------------\n",
    "    strategy = SaveModelStrategy(\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=0.0,\n",
    "        min_fit_clients=2,\n",
    "        min_evaluate_clients=0,\n",
    "        min_available_clients=2,\n",
    "        on_fit_config_fn=fit_config,\n",
    "        initial_parameters=fl.common.ndarrays_to_parameters(get_parameters(Global_Models[Round - 1])),\n",
    "        additional_argument=Round,\n",
    "    )\n",
    "\n",
    "    client_fn = General_Client()\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # 3️⃣ Run Federated Simulation for 1 Round\n",
    "    # ------------------------------------------------------\n",
    "    fl.simulation.start_simulation(\n",
    "        client_fn=lambda cid: client_fn(cid, int(Round)),\n",
    "        num_clients=int(NUM_CLIENTS),\n",
    "        config=fl.server.ServerConfig(num_rounds=1),\n",
    "        client_resources={\"num_cpus\": 16, \"num_gpus\": 1},\n",
    "        ray_init_args={\"num_cpus\": 16, \"num_gpus\": 1},\n",
    "        strategy=strategy,\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # 4️⃣ Load Global Model for Post-Round Evaluation\n",
    "    # ------------------------------------------------------\n",
    "    print(f\"\\n🔁 Loading Global Model for Round {Round} ...\")\n",
    "    os.makedirs(PATH, exist_ok=True)\n",
    "    model_path = f\"{PATH}/GlobalModel_{Round}.pth\"\n",
    "\n",
    "    Global_Models[Round] = Net(\n",
    "        h1=int(TRAIN_CFG.get(\"hidden_size_1\", 64)),\n",
    "        h2=int(TRAIN_CFG.get(\"hidden_size_2\", 32)),\n",
    "        dropout=float(TRAIN_CFG.get(\"dropout\", 0.0)),\n",
    "        activation=str(TRAIN_CFG.get(\"activation\", \"relu\")),\n",
    "        batch_norm=bool(TRAIN_CFG.get(\"batch_norm\", False)),\n",
    "    )\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        Global_Models[Round].load_state_dict(torch.load(model_path))\n",
    "        print(f\"✅ Model loaded from {model_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No model found at {model_path}, continuing with initialized weights.\")\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # 5️⃣ Evaluate and Invoke LLM Optimization\n",
    "    # ------------------------------------------------------\n",
    "    Global_Models[Round].eval()\n",
    "    print(f\"\\n📊 Evaluating and optimizing Round {Round} with LLM ...\")\n",
    "    llm_step_after_round(Round, Global_Models[Round])\n",
    "\n",
    "print(\"\\n===== 🎯 Training + LLM Optimization Complete =====\")\n",
    "print(\"All models and JSON configs saved under:\", PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555eb63-e297-4c7b-8e1b-5985d2cde0e9",
   "metadata": {
    "id": "l0PgqJTEwwWk",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='Grey'>***Performance Testing***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f39255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the directory and file pattern\n",
    "directory = PATH + '/'\n",
    "pattern = \"GlobalModel_*.pth\"\n",
    "\n",
    "# Find all matching files\n",
    "files = glob.glob(os.path.join(directory, pattern))\n",
    "\n",
    "# Extract numbers from file names\n",
    "numbers = []\n",
    "for file in files:\n",
    "    base_name = os.path.basename(file)\n",
    "    num_str = base_name.replace(\"GlobalModel_\", \"\").replace(\".pth\", \"\")\n",
    "    try:\n",
    "        numbers.append(int(num_str))\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "# Determine the maximum number\n",
    "max_num = max(numbers) if numbers else 0\n",
    "print(max_num)\n",
    "\n",
    "# Use the max_num in a loop\n",
    "for num in range(1, max_num + 1):\n",
    "    file_path = f\"{PATH}/GlobalModel_{num}.pth\"\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the file or perform any operation you need\n",
    "        print(f\"Loading {file_path}\")\n",
    "    else:\n",
    "        print(f\"File {file_path} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ec610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_pickle(path, G=None, suffix=None):\n",
    "    print(G)\n",
    "    if G is not None:\n",
    "        rounds = [G]\n",
    "    else:\n",
    "        # Auto-detect how many Global_X files exist\n",
    "        rounds = sorted({\n",
    "            int(f.split(\"_\")[1])\n",
    "            for f in os.listdir(path)\n",
    "            if f.startswith(\"Global_\") and f.split(\"_\")[1].isdigit()\n",
    "        })\n",
    "\n",
    "    suffixes = [suffix] if suffix else [\"pred\", \"actual\", \"accurracy\", \"loss\"]\n",
    "\n",
    "    for g in rounds:\n",
    "        for s in suffixes:\n",
    "            filename = os.path.join(path, f\"Global_{g}_{s}\")\n",
    "            if not os.path.exists(filename):\n",
    "                print(f\"Missing file: {filename}\")\n",
    "                continue\n",
    "            try:\n",
    "                with open(filename, \"rb\") as f:\n",
    "                    data = pickle.load(f)\n",
    "                print(f\"File: {filename}\")\n",
    "                if isinstance(data, list) and len(data) > 2:\n",
    "                    pprint(data[:2])\n",
    "                    print(f\"... ({len(data)} total items)\")\n",
    "                else:\n",
    "                    pprint(data)\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error reading {filename}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q7O6Uj-at0L7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 364,
     "status": "error",
     "timestamp": 1670839612569,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "Q7O6Uj-at0L7",
    "outputId": "e4d532e2-d5a1-4a0c-b486-f5975af6cd0e",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "pred_test = {}\n",
    "actual_test = {}\n",
    "accuracy_test = {}\n",
    "loss_test = {}\n",
    "G = 0\n",
    "txt_path = f\"{PATH}/Global_{G}.txt\"\n",
    "for num in range(1, max_num+1):\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load(f\"{PATH}/GlobalModel_{num}.pth\"))\n",
    "    model.eval()\n",
    "    \n",
    "    prediction_matrix = []\n",
    "    actual_matrix= []\n",
    "    acc_matrix = []\n",
    "    loss_matrix=[]\n",
    "    G = G + 1\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in Dataloaders['Test']:\n",
    "            outputs = model(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            prediction_matrix.append(predicted.tolist())\n",
    "            actual_matrix.append(labels.tolist())\n",
    "    loss /= len(Dataloaders['Test'].dataset)\n",
    "    accuracy = correct / total\n",
    "    loss_matrix.append(loss)\n",
    "    acc_matrix.append(accuracy) \n",
    "\n",
    "    pred_test[f'Global_{G}'] = prediction_matrix\n",
    "    actual_test[f'Global_{G}'] = actual_matrix\n",
    "    accuracy_test[f'Global_{G}'] = acc_matrix\n",
    "    loss_test[f'Global_{G}'] = loss_matrix \n",
    "\n",
    "    filename = f'{PATH}/Global_{G}_pred'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(pred_test[f'Global_{G}'],outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    filename = f'{PATH}/Global_{G}_actual'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(actual_test[f'Global_{G}'],outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    filename = f'{PATH}/Global_{G}_accurracy'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(accuracy_test[f'Global_{G}'],outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    filename = f'{PATH}/Global_{G}_loss'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(loss_test[f'Global_{G}'],outfile)\n",
    "    outfile.close()\n",
    "    \n",
    "view_pickle(PATH)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "bcoA2oGB6Kyi",
    "-hiMkZhLVT63",
    "Fg6LBRPK-E6_",
    "gjVnC-rj9gC4",
    "KNYdybJu5uhn",
    "LwpshLHduBDL",
    "Df92buFDvAl8",
    "D8dfI-OyvOBt",
    "s02Uob4Bvebm",
    "3H-0K0GXvuQo",
    "Y3LLG0dvwAFl",
    "thyLEKcswNst",
    "scNSl39rwhZu"
   ],
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

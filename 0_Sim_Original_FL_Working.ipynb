{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcoA2oGB6Kyi",
   "metadata": {
    "id": "bcoA2oGB6Kyi",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='green'>***Installation and Libraries Import***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b0b1a7-d1b9-403f-8c98-84e601860c8f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# %pip install flwr\n",
    "# %pip install ray \n",
    "# %pip install --upgrade pip\n",
    "# %pip install torch torchvision matplotlib\n",
    "# %pip install async-timeout\n",
    "# %pip install async-numpy\n",
    "# %pip install pandas\n",
    "# %pip install datasets\n",
    "# %pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8316c4a3-e1c6-44ac-b41b-0da26527d7cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import flwr as fl\n",
    "import torch, ray, pandas, sklearn\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# print(\"All modules loaded successfully!\")\n",
    "# print(\"FLWR version:\", fl.__version__)\n",
    "# print(\"Ray version:\", ray.__version__)\n",
    "# print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "# libraries_to_uninstall = [\n",
    "#     \"tb-nightly==2.18.0a20240701\",\n",
    "#     \"tensorboard==2.16.2\",\n",
    "#     \"tensorboard-data-server==0.7.2\",\n",
    "#     \"tensorboard-plugin-wit==1.8.1\",\n",
    "#     \"tensorflow==2.16.2\",\n",
    "#     \"tensorflow-io-gcs-filesystem==0.37.0\",\n",
    "#     \"termcolor==2.4.0\",\n",
    "#     \"terminado==0.18.1\",\n",
    "#     \"tf-estimator-nightly==2.8.0.dev2021122109\",\n",
    "#     \"tf_keras-nightly==2.18.0.dev2024070109\",\n",
    "#     \"tf-nightly==2.18.0.dev20240626\"\n",
    "# ]\n",
    "# for library in libraries_to_uninstall:\n",
    "#     os.system(f\"pip uninstall -y {library}\")\n",
    "# print(\"All modules loaded successfully!\")\n",
    "# print(\"FLWR version:\", fl.__version__)\n",
    "# print(\"Ray version:\", ray.__version__)\n",
    "# print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e677784-3ccb-49de-9aa0-339fdfa926ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, WeightedRandomSampler\n",
    "import flwr as fl\n",
    "from flwr.common import Metrics\n",
    "# from flwr_datasets import FederatedDataset\n",
    "from sklearn.preprocessing import MinMaxScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import csv\n",
    "import copy\n",
    "print(fl.__version__)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fg6LBRPK-E6_",
   "metadata": {
    "id": "Fg6LBRPK-E6_",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='Brown'>***Constants***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZFQQhlvc-c4P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 181,
     "status": "ok",
     "timestamp": 1674161131349,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "ZFQQhlvc-c4P",
    "outputId": "b14bdc45-179f-4a11-fb12-49e6e350ee3f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DEVICE\n",
    "NUM_CLIENTS = 1 #48\n",
    "ROUNDS = 40 #40\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0018\n",
    "EPOCHS = 3\n",
    "DATA_GROUPS = 120\n",
    "BATCH_ROUND = 6\n",
    "SIZE_ROUND = int(BATCH_ROUND * BATCH_SIZE * NUM_CLIENTS)\n",
    "NUM_ATCKS = 5\n",
    "PATH = f'DNNCent{NUM_ATCKS}atk_{ROUNDS}_rounds_{NUM_CLIENTS}_clients_{EPOCHS}_epochs_{BATCH_SIZE}_batch_{LEARNING_RATE}_lr_{DATA_GROUPS}_data_groups/'\n",
    "G=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gjVnC-rj9gC4",
   "metadata": {
    "id": "gjVnC-rj9gC4",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='Light Blue'>***Dataset Preparations***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1aa6e75b-01b6-4bc4-a0e3-af41e9e15d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 (184320, 99)\n",
      "100 (184320, 99)\n",
      "70 (184320, 99)\n",
      "50 (184320, 99)\n",
      "120 (120000, 99)\n"
     ]
    }
   ],
   "source": [
    "TrafficData = {}\n",
    "TrafficData['Dataset']={}\n",
    "sets_names = ['30','100','70','50','120']\n",
    "for  DATA_NUM in sets_names:\n",
    "    TrafficData['Dataset'][DATA_NUM]=pd.read_csv(f'data/2_Dataset_{NUM_ATCKS}_Attack_{DATA_NUM}_normal.csv', low_memory=False, quoting=csv.QUOTE_NONE, on_bad_lines='skip')\n",
    "    print(DATA_NUM, TrafficData['Dataset'][DATA_NUM].shape)\n",
    "for DATA_NUM in TrafficData['Dataset']:\n",
    "    TrafficData['Dataset'][DATA_NUM]=TrafficData['Dataset'][DATA_NUM].sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb2857fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(737280, 99)\n"
     ]
    }
   ],
   "source": [
    "TrafficData['Split'] = {}\n",
    "sets_training =  ['30','100','70','50']\n",
    "for DATA_NUM in sets_training:\n",
    "    TrafficData['Split'][DATA_NUM] = np.array_split(TrafficData['Dataset'][DATA_NUM],DATA_GROUPS)\n",
    "\n",
    "TrafficData['Combined'] = pd.concat([TrafficData['Split']['30'][0], TrafficData['Split']['100'][0], TrafficData['Split']['70'][0], TrafficData['Split']['50'][0]]).reset_index(drop=True)\n",
    "for GROUP in range(1, DATA_GROUPS):\n",
    "    TrafficData['Combined'] = pd.concat([TrafficData['Combined'], TrafficData['Split']['30'][GROUP], TrafficData['Split']['100'][GROUP], TrafficData['Split']['70'][GROUP], TrafficData['Split']['50'][GROUP]]).reset_index(drop=True)\n",
    "print(TrafficData['Combined'].shape)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a8d74ea7-0942-4de4-aa49-cb0c9032e148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(737280, 98)\n",
      "(737280,)\n",
      "(120000, 98)\n",
      "(120000,)\n"
     ]
    }
   ],
   "source": [
    "TrafficData['Train'] = {}\n",
    "TrafficData['Train']['X'] = TrafficData['Combined'].iloc[:, 0:-1]\n",
    "TrafficData['Train']['y'] = TrafficData['Combined'].iloc[:, -1]\n",
    "print(TrafficData['Train']['X'].shape)\n",
    "print(TrafficData['Train']['y'].shape)\n",
    "\n",
    "TrafficData['Test'] = {}\n",
    "TrafficData['Test']['X']=TrafficData['Dataset']['120'].iloc[:, 0:-1]\n",
    "TrafficData['Test']['y']=TrafficData['Dataset']['120'].iloc[:, -1]\n",
    "print(TrafficData['Test']['X'].shape)\n",
    "print(TrafficData['Test']['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f406c44d-9a2b-4831-a713-ce5a95134832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(737280, 98) (737280,)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(120000, 98) (120000,)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "model = scaler.fit(TrafficData['Train']['X'])\n",
    "TrafficData['Train']['X'] = model.transform(TrafficData['Train']['X'])\n",
    "TrafficData['Test']['X'] = model.transform(TrafficData['Test']['X'])\n",
    "\n",
    "TrafficData['Train']['X'], TrafficData['Train']['y']= np.array(TrafficData['Train']['X']), np.array(TrafficData['Train']['y'])\n",
    "print(type(TrafficData['Train']['X']),type(TrafficData['Train']['y']))\n",
    "print(TrafficData['Train']['X'].shape,TrafficData['Train']['y'].shape)\n",
    "TrafficData['Test']['X'], TrafficData['Test']['y']= np.array(TrafficData['Test']['X']), np.array(TrafficData['Test']['y'])\n",
    "print(type(TrafficData['Test']['X']),type(TrafficData['Test']['y']))\n",
    "print(TrafficData['Test']['X'].shape,TrafficData['Test']['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "751b2901-2954-4d8c-9185-8e6ac28ad965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 768\n",
      "768 1152\n",
      "1152 1536\n",
      "1536 1920\n",
      "1920 2304\n",
      "2304 2688\n",
      "2688 3072\n",
      "3072 3456\n",
      "3456 3840\n",
      "3840 4224\n",
      "4224 4608\n",
      "4608 4992\n",
      "4992 5376\n",
      "5376 5760\n",
      "5760 6144\n",
      "6144 6528\n",
      "6528 6912\n",
      "6912 7296\n",
      "7296 7680\n",
      "7680 8064\n",
      "8064 8448\n",
      "8448 8832\n",
      "8832 9216\n",
      "9216 9600\n",
      "9600 9984\n",
      "9984 10368\n",
      "10368 10752\n",
      "10752 11136\n",
      "11136 11520\n",
      "11520 11904\n",
      "11904 12288\n",
      "12288 12672\n",
      "12672 13056\n",
      "13056 13440\n",
      "13440 13824\n",
      "13824 14208\n",
      "14208 14592\n",
      "14592 14976\n",
      "14976 15360\n",
      "ROUND:  1 (384, 98) (384,)\n",
      "ROUND:  2 (384, 98) (384,)\n",
      "ROUND:  3 (384, 98) (384,)\n",
      "ROUND:  4 (384, 98) (384,)\n",
      "ROUND:  5 (384, 98) (384,)\n",
      "ROUND:  6 (384, 98) (384,)\n",
      "ROUND:  7 (384, 98) (384,)\n",
      "ROUND:  8 (384, 98) (384,)\n",
      "ROUND:  9 (384, 98) (384,)\n",
      "ROUND:  10 (384, 98) (384,)\n",
      "ROUND:  11 (384, 98) (384,)\n",
      "ROUND:  12 (384, 98) (384,)\n",
      "ROUND:  13 (384, 98) (384,)\n",
      "ROUND:  14 (384, 98) (384,)\n",
      "ROUND:  15 (384, 98) (384,)\n",
      "ROUND:  16 (384, 98) (384,)\n",
      "ROUND:  17 (384, 98) (384,)\n",
      "ROUND:  18 (384, 98) (384,)\n",
      "ROUND:  19 (384, 98) (384,)\n",
      "ROUND:  20 (384, 98) (384,)\n",
      "ROUND:  21 (384, 98) (384,)\n",
      "ROUND:  22 (384, 98) (384,)\n",
      "ROUND:  23 (384, 98) (384,)\n",
      "ROUND:  24 (384, 98) (384,)\n",
      "ROUND:  25 (384, 98) (384,)\n",
      "ROUND:  26 (384, 98) (384,)\n",
      "ROUND:  27 (384, 98) (384,)\n",
      "ROUND:  28 (384, 98) (384,)\n",
      "ROUND:  29 (384, 98) (384,)\n",
      "ROUND:  30 (384, 98) (384,)\n",
      "ROUND:  31 (384, 98) (384,)\n",
      "ROUND:  32 (384, 98) (384,)\n",
      "ROUND:  33 (384, 98) (384,)\n",
      "ROUND:  34 (384, 98) (384,)\n",
      "ROUND:  35 (384, 98) (384,)\n",
      "ROUND:  36 (384, 98) (384,)\n",
      "ROUND:  37 (384, 98) (384,)\n",
      "ROUND:  38 (384, 98) (384,)\n",
      "ROUND:  39 (384, 98) (384,)\n",
      "ROUND:  40 (384, 98) (384,)\n",
      "15744 384\n"
     ]
    }
   ],
   "source": [
    "TrafficData['ROUNDS']={}\n",
    "for ROUND in range(1, ROUNDS+1):\n",
    "    TrafficData['ROUNDS'][ROUND]={}\n",
    "\n",
    "SIZE_Demo = SIZE_ROUND\n",
    "for ROUND in range(1,ROUNDS+1):\n",
    "    if ROUND == 1:\n",
    "        TrafficData['ROUNDS'][ROUND]['X']= TrafficData['Train']['X'][:SIZE_Demo]\n",
    "        TrafficData['ROUNDS'][ROUND]['y']= TrafficData['Train']['y'][:SIZE_Demo]\n",
    "    else:\n",
    "        print((SIZE_Demo - SIZE_ROUND),SIZE_Demo)\n",
    "        TrafficData['ROUNDS'][ROUND]['X']= TrafficData['Train']['X'][(SIZE_Demo - SIZE_ROUND):SIZE_Demo]\n",
    "        TrafficData['ROUNDS'][ROUND]['y']= TrafficData['Train']['y'][(SIZE_Demo - SIZE_ROUND):SIZE_Demo]\n",
    "    SIZE_Demo = SIZE_Demo + SIZE_ROUND\n",
    "for ROUND in TrafficData['ROUNDS']:\n",
    "    print(\"ROUND: \", ROUND, TrafficData['ROUNDS'][ROUND]['X'].shape, TrafficData['ROUNDS'][ROUND]['y'].shape)\n",
    "print(SIZE_Demo, SIZE_ROUND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "128f73dc-662e-441e-9361-32fbca07af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "69ade83a-b7fd-450d-82cb-0649af87fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrafficData['trainsets']={}\n",
    "for ROUND in range(1, ROUNDS+1):\n",
    "    TrafficData['trainsets'][ROUND]= ClassifierDataset(torch.from_numpy(TrafficData['ROUNDS'][ROUND]['X']).float(), torch.from_numpy(TrafficData['ROUNDS'][ROUND]['y']).long())\n",
    "TrafficData['testset'] = ClassifierDataset(torch.from_numpy(TrafficData['Test']['X']).float(), torch.from_numpy(TrafficData['Test']['y']).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e40d6a1f-95bc-4173-a8b4-8961e3d2875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train(numberofclients, ROUND):    \n",
    "    portion_size = int(BATCH_ROUND*BATCH_SIZE)\n",
    "    num_portions = int(NUM_CLIENTS)\n",
    "    portion_indices = []\n",
    "    for i in range(num_portions):\n",
    "        start_idx = i * portion_size\n",
    "        end_idx = (i + 1) * portion_size\n",
    "        portion_indices.append(list(range(start_idx, min(end_idx, SIZE_ROUND))))\n",
    "    portion_datasets = [Subset(TrafficData['trainsets'][ROUND], indices) for indices in portion_indices]\n",
    "    portion_loaders = [DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False) for dataset in portion_datasets]             \n",
    "    return portion_loaders\n",
    "def load_test(numberofclients):    \n",
    "    testloader = DataLoader(TrafficData['testset'], batch_size=BATCH_SIZE, shuffle=False)\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "00e2e5d8-60da-4260-80bc-4850460279c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataloaders = {}\n",
    "for ROUND in range(1, ROUNDS+1):\n",
    "    Dataloaders[ROUND] = load_train(NUM_CLIENTS, ROUND)\n",
    "Dataloaders['Test'] = load_test(NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c6455bfc-e8b5-4d01-84d6-3f484a2c8ad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 size: 64\n",
      "Batch 2 size: 64\n",
      "Batch 3 size: 64\n",
      "Batch 4 size: 64\n",
      "Batch 5 size: 64\n",
      "Batch 6 size: 64\n",
      "Batch 7 size: 64\n",
      "Batch 8 size: 64\n",
      "Batch 9 size: 64\n",
      "Batch 10 size: 64\n",
      "Batch 11 size: 64\n",
      "Batch 12 size: 64\n",
      "Batch 13 size: 64\n",
      "Batch 14 size: 64\n",
      "Batch 15 size: 64\n",
      "Batch 16 size: 64\n",
      "Batch 17 size: 64\n",
      "Batch 18 size: 64\n",
      "Batch 19 size: 64\n",
      "Batch 20 size: 64\n",
      "Batch 21 size: 64\n",
      "Batch 22 size: 64\n",
      "Batch 23 size: 64\n",
      "Batch 24 size: 64\n",
      "Batch 25 size: 64\n",
      "Batch 26 size: 64\n",
      "Batch 27 size: 64\n",
      "Batch 28 size: 64\n",
      "Batch 29 size: 64\n",
      "Batch 30 size: 64\n",
      "Batch 31 size: 64\n",
      "Batch 32 size: 64\n",
      "Batch 33 size: 64\n",
      "Batch 34 size: 64\n",
      "Batch 35 size: 64\n",
      "Batch 36 size: 64\n",
      "Batch 37 size: 64\n",
      "Batch 38 size: 64\n",
      "Batch 39 size: 64\n",
      "Batch 40 size: 64\n",
      "Batch 41 size: 64\n",
      "Batch 42 size: 64\n",
      "Batch 43 size: 64\n",
      "Batch 44 size: 64\n",
      "Batch 45 size: 64\n",
      "Batch 46 size: 64\n",
      "Batch 47 size: 64\n",
      "Batch 48 size: 64\n",
      "Batch 49 size: 64\n",
      "Batch 50 size: 64\n",
      "Batch 51 size: 64\n",
      "Batch 52 size: 64\n",
      "Batch 53 size: 64\n",
      "Batch 54 size: 64\n",
      "Batch 55 size: 64\n",
      "Batch 56 size: 64\n",
      "Batch 57 size: 64\n",
      "Batch 58 size: 64\n",
      "Batch 59 size: 64\n",
      "Batch 60 size: 64\n",
      "Batch 61 size: 64\n",
      "Batch 62 size: 64\n",
      "Batch 63 size: 64\n",
      "Batch 64 size: 64\n",
      "Batch 65 size: 64\n",
      "Batch 66 size: 64\n",
      "Batch 67 size: 64\n",
      "Batch 68 size: 64\n",
      "Batch 69 size: 64\n",
      "Batch 70 size: 64\n",
      "Batch 71 size: 64\n",
      "Batch 72 size: 64\n",
      "Batch 73 size: 64\n",
      "Batch 74 size: 64\n",
      "Batch 75 size: 64\n",
      "Batch 76 size: 64\n",
      "Batch 77 size: 64\n",
      "Batch 78 size: 64\n",
      "Batch 79 size: 64\n",
      "Batch 80 size: 64\n",
      "Batch 81 size: 64\n",
      "Batch 82 size: 64\n",
      "Batch 83 size: 64\n",
      "Batch 84 size: 64\n",
      "Batch 85 size: 64\n",
      "Batch 86 size: 64\n",
      "Batch 87 size: 64\n",
      "Batch 88 size: 64\n",
      "Batch 89 size: 64\n",
      "Batch 90 size: 64\n",
      "Batch 91 size: 64\n",
      "Batch 92 size: 64\n",
      "Batch 93 size: 64\n",
      "Batch 94 size: 64\n",
      "Batch 95 size: 64\n",
      "Batch 96 size: 64\n",
      "Batch 97 size: 64\n",
      "Batch 98 size: 64\n",
      "Batch 99 size: 64\n",
      "Batch 100 size: 64\n",
      "Batch 101 size: 64\n",
      "Batch 102 size: 64\n",
      "Batch 103 size: 64\n",
      "Batch 104 size: 64\n",
      "Batch 105 size: 64\n",
      "Batch 106 size: 64\n",
      "Batch 107 size: 64\n",
      "Batch 108 size: 64\n",
      "Batch 109 size: 64\n",
      "Batch 110 size: 64\n",
      "Batch 111 size: 64\n",
      "Batch 112 size: 64\n",
      "Batch 113 size: 64\n",
      "Batch 114 size: 64\n",
      "Batch 115 size: 64\n",
      "Batch 116 size: 64\n",
      "Batch 117 size: 64\n",
      "Batch 118 size: 64\n",
      "Batch 119 size: 64\n",
      "Batch 120 size: 64\n",
      "Batch 121 size: 64\n",
      "Batch 122 size: 64\n",
      "Batch 123 size: 64\n",
      "Batch 124 size: 64\n",
      "Batch 125 size: 64\n",
      "Batch 126 size: 64\n",
      "Batch 127 size: 64\n",
      "Batch 128 size: 64\n",
      "Batch 129 size: 64\n",
      "Batch 130 size: 64\n",
      "Batch 131 size: 64\n",
      "Batch 132 size: 64\n",
      "Batch 133 size: 64\n",
      "Batch 134 size: 64\n",
      "Batch 135 size: 64\n",
      "Batch 136 size: 64\n",
      "Batch 137 size: 64\n",
      "Batch 138 size: 64\n",
      "Batch 139 size: 64\n",
      "Batch 140 size: 64\n",
      "Batch 141 size: 64\n",
      "Batch 142 size: 64\n",
      "Batch 143 size: 64\n",
      "Batch 144 size: 64\n",
      "Batch 145 size: 64\n",
      "Batch 146 size: 64\n",
      "Batch 147 size: 64\n",
      "Batch 148 size: 64\n",
      "Batch 149 size: 64\n",
      "Batch 150 size: 64\n",
      "Batch 151 size: 64\n",
      "Batch 152 size: 64\n",
      "Batch 153 size: 64\n",
      "Batch 154 size: 64\n",
      "Batch 155 size: 64\n",
      "Batch 156 size: 64\n",
      "Batch 157 size: 64\n",
      "Batch 158 size: 64\n",
      "Batch 159 size: 64\n",
      "Batch 160 size: 64\n",
      "Batch 161 size: 64\n",
      "Batch 162 size: 64\n",
      "Batch 163 size: 64\n",
      "Batch 164 size: 64\n",
      "Batch 165 size: 64\n",
      "Batch 166 size: 64\n",
      "Batch 167 size: 64\n",
      "Batch 168 size: 64\n",
      "Batch 169 size: 64\n",
      "Batch 170 size: 64\n",
      "Batch 171 size: 64\n",
      "Batch 172 size: 64\n",
      "Batch 173 size: 64\n",
      "Batch 174 size: 64\n",
      "Batch 175 size: 64\n",
      "Batch 176 size: 64\n",
      "Batch 177 size: 64\n",
      "Batch 178 size: 64\n",
      "Batch 179 size: 64\n",
      "Batch 180 size: 64\n",
      "Batch 181 size: 64\n",
      "Batch 182 size: 64\n",
      "Batch 183 size: 64\n",
      "Batch 184 size: 64\n",
      "Batch 185 size: 64\n",
      "Batch 186 size: 64\n",
      "Batch 187 size: 64\n",
      "Batch 188 size: 64\n",
      "Batch 189 size: 64\n",
      "Batch 190 size: 64\n",
      "Batch 191 size: 64\n",
      "Batch 192 size: 64\n",
      "Batch 193 size: 64\n",
      "Batch 194 size: 64\n",
      "Batch 195 size: 64\n",
      "Batch 196 size: 64\n",
      "Batch 197 size: 64\n",
      "Batch 198 size: 64\n",
      "Batch 199 size: 64\n",
      "Batch 200 size: 64\n",
      "Batch 201 size: 64\n",
      "Batch 202 size: 64\n",
      "Batch 203 size: 64\n",
      "Batch 204 size: 64\n",
      "Batch 205 size: 64\n",
      "Batch 206 size: 64\n",
      "Batch 207 size: 64\n",
      "Batch 208 size: 64\n",
      "Batch 209 size: 64\n",
      "Batch 210 size: 64\n",
      "Batch 211 size: 64\n",
      "Batch 212 size: 64\n",
      "Batch 213 size: 64\n",
      "Batch 214 size: 64\n",
      "Batch 215 size: 64\n",
      "Batch 216 size: 64\n",
      "Batch 217 size: 64\n",
      "Batch 218 size: 64\n",
      "Batch 219 size: 64\n",
      "Batch 220 size: 64\n",
      "Batch 221 size: 64\n",
      "Batch 222 size: 64\n",
      "Batch 223 size: 64\n",
      "Batch 224 size: 64\n",
      "Batch 225 size: 64\n",
      "Batch 226 size: 64\n",
      "Batch 227 size: 64\n",
      "Batch 228 size: 64\n",
      "Batch 229 size: 64\n",
      "Batch 230 size: 64\n",
      "Batch 231 size: 64\n",
      "Batch 232 size: 64\n",
      "Batch 233 size: 64\n",
      "Batch 234 size: 64\n",
      "Batch 235 size: 64\n",
      "Batch 236 size: 64\n",
      "Batch 237 size: 64\n",
      "Batch 238 size: 64\n",
      "Batch 239 size: 64\n",
      "Batch 240 size: 64\n",
      "Batch 241 size: 64\n",
      "Batch 242 size: 64\n",
      "Batch 243 size: 64\n",
      "Batch 244 size: 64\n",
      "Batch 245 size: 64\n",
      "Batch 246 size: 64\n",
      "Batch 247 size: 64\n",
      "Batch 248 size: 64\n",
      "Batch 249 size: 64\n",
      "Batch 250 size: 64\n",
      "Batch 251 size: 64\n",
      "Batch 252 size: 64\n",
      "Batch 253 size: 64\n",
      "Batch 254 size: 64\n",
      "Batch 255 size: 64\n",
      "Batch 256 size: 64\n",
      "Batch 257 size: 64\n",
      "Batch 258 size: 64\n",
      "Batch 259 size: 64\n",
      "Batch 260 size: 64\n",
      "Batch 261 size: 64\n",
      "Batch 262 size: 64\n",
      "Batch 263 size: 64\n",
      "Batch 264 size: 64\n",
      "Batch 265 size: 64\n",
      "Batch 266 size: 64\n",
      "Batch 267 size: 64\n",
      "Batch 268 size: 64\n",
      "Batch 269 size: 64\n",
      "Batch 270 size: 64\n",
      "Batch 271 size: 64\n",
      "Batch 272 size: 64\n",
      "Batch 273 size: 64\n",
      "Batch 274 size: 64\n",
      "Batch 275 size: 64\n",
      "Batch 276 size: 64\n",
      "Batch 277 size: 64\n",
      "Batch 278 size: 64\n",
      "Batch 279 size: 64\n",
      "Batch 280 size: 64\n",
      "Batch 281 size: 64\n",
      "Batch 282 size: 64\n",
      "Batch 283 size: 64\n",
      "Batch 284 size: 64\n",
      "Batch 285 size: 64\n",
      "Batch 286 size: 64\n",
      "Batch 287 size: 64\n",
      "Batch 288 size: 64\n",
      "Batch 289 size: 64\n",
      "Batch 290 size: 64\n",
      "Batch 291 size: 64\n",
      "Batch 292 size: 64\n",
      "Batch 293 size: 64\n",
      "Batch 294 size: 64\n",
      "Batch 295 size: 64\n",
      "Batch 296 size: 64\n",
      "Batch 297 size: 64\n",
      "Batch 298 size: 64\n",
      "Batch 299 size: 64\n",
      "Batch 300 size: 64\n",
      "Batch 301 size: 64\n",
      "Batch 302 size: 64\n",
      "Batch 303 size: 64\n",
      "Batch 304 size: 64\n",
      "Batch 305 size: 64\n",
      "Batch 306 size: 64\n",
      "Batch 307 size: 64\n",
      "Batch 308 size: 64\n",
      "Batch 309 size: 64\n",
      "Batch 310 size: 64\n",
      "Batch 311 size: 64\n",
      "Batch 312 size: 64\n",
      "Batch 313 size: 64\n",
      "Batch 314 size: 64\n",
      "Batch 315 size: 64\n",
      "Batch 316 size: 64\n",
      "Batch 317 size: 64\n",
      "Batch 318 size: 64\n",
      "Batch 319 size: 64\n",
      "Batch 320 size: 64\n",
      "Batch 321 size: 64\n",
      "Batch 322 size: 64\n",
      "Batch 323 size: 64\n",
      "Batch 324 size: 64\n",
      "Batch 325 size: 64\n",
      "Batch 326 size: 64\n",
      "Batch 327 size: 64\n",
      "Batch 328 size: 64\n",
      "Batch 329 size: 64\n",
      "Batch 330 size: 64\n",
      "Batch 331 size: 64\n",
      "Batch 332 size: 64\n",
      "Batch 333 size: 64\n",
      "Batch 334 size: 64\n",
      "Batch 335 size: 64\n",
      "Batch 336 size: 64\n",
      "Batch 337 size: 64\n",
      "Batch 338 size: 64\n",
      "Batch 339 size: 64\n",
      "Batch 340 size: 64\n",
      "Batch 341 size: 64\n",
      "Batch 342 size: 64\n",
      "Batch 343 size: 64\n",
      "Batch 344 size: 64\n",
      "Batch 345 size: 64\n",
      "Batch 346 size: 64\n",
      "Batch 347 size: 64\n",
      "Batch 348 size: 64\n",
      "Batch 349 size: 64\n",
      "Batch 350 size: 64\n",
      "Batch 351 size: 64\n",
      "Batch 352 size: 64\n",
      "Batch 353 size: 64\n",
      "Batch 354 size: 64\n",
      "Batch 355 size: 64\n",
      "Batch 356 size: 64\n",
      "Batch 357 size: 64\n",
      "Batch 358 size: 64\n",
      "Batch 359 size: 64\n",
      "Batch 360 size: 64\n",
      "Batch 361 size: 64\n",
      "Batch 362 size: 64\n",
      "Batch 363 size: 64\n",
      "Batch 364 size: 64\n",
      "Batch 365 size: 64\n",
      "Batch 366 size: 64\n",
      "Batch 367 size: 64\n",
      "Batch 368 size: 64\n",
      "Batch 369 size: 64\n",
      "Batch 370 size: 64\n",
      "Batch 371 size: 64\n",
      "Batch 372 size: 64\n",
      "Batch 373 size: 64\n",
      "Batch 374 size: 64\n",
      "Batch 375 size: 64\n",
      "Batch 376 size: 64\n",
      "Batch 377 size: 64\n",
      "Batch 378 size: 64\n",
      "Batch 379 size: 64\n",
      "Batch 380 size: 64\n",
      "Batch 381 size: 64\n",
      "Batch 382 size: 64\n",
      "Batch 383 size: 64\n",
      "Batch 384 size: 64\n",
      "Batch 385 size: 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 386 size: 64\n",
      "Batch 387 size: 64\n",
      "Batch 388 size: 64\n",
      "Batch 389 size: 64\n",
      "Batch 390 size: 64\n",
      "Batch 391 size: 64\n",
      "Batch 392 size: 64\n",
      "Batch 393 size: 64\n",
      "Batch 394 size: 64\n",
      "Batch 395 size: 64\n",
      "Batch 396 size: 64\n",
      "Batch 397 size: 64\n",
      "Batch 398 size: 64\n",
      "Batch 399 size: 64\n",
      "Batch 400 size: 64\n",
      "Batch 401 size: 64\n",
      "Batch 402 size: 64\n",
      "Batch 403 size: 64\n",
      "Batch 404 size: 64\n",
      "Batch 405 size: 64\n",
      "Batch 406 size: 64\n",
      "Batch 407 size: 64\n",
      "Batch 408 size: 64\n",
      "Batch 409 size: 64\n",
      "Batch 410 size: 64\n",
      "Batch 411 size: 64\n",
      "Batch 412 size: 64\n",
      "Batch 413 size: 64\n",
      "Batch 414 size: 64\n",
      "Batch 415 size: 64\n",
      "Batch 416 size: 64\n",
      "Batch 417 size: 64\n",
      "Batch 418 size: 64\n",
      "Batch 419 size: 64\n",
      "Batch 420 size: 64\n",
      "Batch 421 size: 64\n",
      "Batch 422 size: 64\n",
      "Batch 423 size: 64\n",
      "Batch 424 size: 64\n",
      "Batch 425 size: 64\n",
      "Batch 426 size: 64\n",
      "Batch 427 size: 64\n",
      "Batch 428 size: 64\n",
      "Batch 429 size: 64\n",
      "Batch 430 size: 64\n",
      "Batch 431 size: 64\n",
      "Batch 432 size: 64\n",
      "Batch 433 size: 64\n",
      "Batch 434 size: 64\n",
      "Batch 435 size: 64\n",
      "Batch 436 size: 64\n",
      "Batch 437 size: 64\n",
      "Batch 438 size: 64\n",
      "Batch 439 size: 64\n",
      "Batch 440 size: 64\n",
      "Batch 441 size: 64\n",
      "Batch 442 size: 64\n",
      "Batch 443 size: 64\n",
      "Batch 444 size: 64\n",
      "Batch 445 size: 64\n",
      "Batch 446 size: 64\n",
      "Batch 447 size: 64\n",
      "Batch 448 size: 64\n",
      "Batch 449 size: 64\n",
      "Batch 450 size: 64\n",
      "Batch 451 size: 64\n",
      "Batch 452 size: 64\n",
      "Batch 453 size: 64\n",
      "Batch 454 size: 64\n",
      "Batch 455 size: 64\n",
      "Batch 456 size: 64\n",
      "Batch 457 size: 64\n",
      "Batch 458 size: 64\n",
      "Batch 459 size: 64\n",
      "Batch 460 size: 64\n",
      "Batch 461 size: 64\n",
      "Batch 462 size: 64\n",
      "Batch 463 size: 64\n",
      "Batch 464 size: 64\n",
      "Batch 465 size: 64\n",
      "Batch 466 size: 64\n",
      "Batch 467 size: 64\n",
      "Batch 468 size: 64\n",
      "Batch 469 size: 64\n",
      "Batch 470 size: 64\n",
      "Batch 471 size: 64\n",
      "Batch 472 size: 64\n",
      "Batch 473 size: 64\n",
      "Batch 474 size: 64\n",
      "Batch 475 size: 64\n",
      "Batch 476 size: 64\n",
      "Batch 477 size: 64\n",
      "Batch 478 size: 64\n",
      "Batch 479 size: 64\n",
      "Batch 480 size: 64\n",
      "Batch 481 size: 64\n",
      "Batch 482 size: 64\n",
      "Batch 483 size: 64\n",
      "Batch 484 size: 64\n",
      "Batch 485 size: 64\n",
      "Batch 486 size: 64\n",
      "Batch 487 size: 64\n",
      "Batch 488 size: 64\n",
      "Batch 489 size: 64\n",
      "Batch 490 size: 64\n",
      "Batch 491 size: 64\n",
      "Batch 492 size: 64\n",
      "Batch 493 size: 64\n",
      "Batch 494 size: 64\n",
      "Batch 495 size: 64\n",
      "Batch 496 size: 64\n",
      "Batch 497 size: 64\n",
      "Batch 498 size: 64\n",
      "Batch 499 size: 64\n",
      "Batch 500 size: 64\n",
      "Batch 501 size: 64\n",
      "Batch 502 size: 64\n",
      "Batch 503 size: 64\n",
      "Batch 504 size: 64\n",
      "Batch 505 size: 64\n",
      "Batch 506 size: 64\n",
      "Batch 507 size: 64\n",
      "Batch 508 size: 64\n",
      "Batch 509 size: 64\n",
      "Batch 510 size: 64\n",
      "Batch 511 size: 64\n",
      "Batch 512 size: 64\n",
      "Batch 513 size: 64\n",
      "Batch 514 size: 64\n",
      "Batch 515 size: 64\n",
      "Batch 516 size: 64\n",
      "Batch 517 size: 64\n",
      "Batch 518 size: 64\n",
      "Batch 519 size: 64\n",
      "Batch 520 size: 64\n",
      "Batch 521 size: 64\n",
      "Batch 522 size: 64\n",
      "Batch 523 size: 64\n",
      "Batch 524 size: 64\n",
      "Batch 525 size: 64\n",
      "Batch 526 size: 64\n",
      "Batch 527 size: 64\n",
      "Batch 528 size: 64\n",
      "Batch 529 size: 64\n",
      "Batch 530 size: 64\n",
      "Batch 531 size: 64\n",
      "Batch 532 size: 64\n",
      "Batch 533 size: 64\n",
      "Batch 534 size: 64\n",
      "Batch 535 size: 64\n",
      "Batch 536 size: 64\n",
      "Batch 537 size: 64\n",
      "Batch 538 size: 64\n",
      "Batch 539 size: 64\n",
      "Batch 540 size: 64\n",
      "Batch 541 size: 64\n",
      "Batch 542 size: 64\n",
      "Batch 543 size: 64\n",
      "Batch 544 size: 64\n",
      "Batch 545 size: 64\n",
      "Batch 546 size: 64\n",
      "Batch 547 size: 64\n",
      "Batch 548 size: 64\n",
      "Batch 549 size: 64\n",
      "Batch 550 size: 64\n",
      "Batch 551 size: 64\n",
      "Batch 552 size: 64\n",
      "Batch 553 size: 64\n",
      "Batch 554 size: 64\n",
      "Batch 555 size: 64\n",
      "Batch 556 size: 64\n",
      "Batch 557 size: 64\n",
      "Batch 558 size: 64\n",
      "Batch 559 size: 64\n",
      "Batch 560 size: 64\n",
      "Batch 561 size: 64\n",
      "Batch 562 size: 64\n",
      "Batch 563 size: 64\n",
      "Batch 564 size: 64\n",
      "Batch 565 size: 64\n",
      "Batch 566 size: 64\n",
      "Batch 567 size: 64\n",
      "Batch 568 size: 64\n",
      "Batch 569 size: 64\n",
      "Batch 570 size: 64\n",
      "Batch 571 size: 64\n",
      "Batch 572 size: 64\n",
      "Batch 573 size: 64\n",
      "Batch 574 size: 64\n",
      "Batch 575 size: 64\n",
      "Batch 576 size: 64\n",
      "Batch 577 size: 64\n",
      "Batch 578 size: 64\n",
      "Batch 579 size: 64\n",
      "Batch 580 size: 64\n",
      "Batch 581 size: 64\n",
      "Batch 582 size: 64\n",
      "Batch 583 size: 64\n",
      "Batch 584 size: 64\n",
      "Batch 585 size: 64\n",
      "Batch 586 size: 64\n",
      "Batch 587 size: 64\n",
      "Batch 588 size: 64\n",
      "Batch 589 size: 64\n",
      "Batch 590 size: 64\n",
      "Batch 591 size: 64\n",
      "Batch 592 size: 64\n",
      "Batch 593 size: 64\n",
      "Batch 594 size: 64\n",
      "Batch 595 size: 64\n",
      "Batch 596 size: 64\n",
      "Batch 597 size: 64\n",
      "Batch 598 size: 64\n",
      "Batch 599 size: 64\n",
      "Batch 600 size: 64\n",
      "Batch 601 size: 64\n",
      "Batch 602 size: 64\n",
      "Batch 603 size: 64\n",
      "Batch 604 size: 64\n",
      "Batch 605 size: 64\n",
      "Batch 606 size: 64\n",
      "Batch 607 size: 64\n",
      "Batch 608 size: 64\n",
      "Batch 609 size: 64\n",
      "Batch 610 size: 64\n",
      "Batch 611 size: 64\n",
      "Batch 612 size: 64\n",
      "Batch 613 size: 64\n",
      "Batch 614 size: 64\n",
      "Batch 615 size: 64\n",
      "Batch 616 size: 64\n",
      "Batch 617 size: 64\n",
      "Batch 618 size: 64\n",
      "Batch 619 size: 64\n",
      "Batch 620 size: 64\n",
      "Batch 621 size: 64\n",
      "Batch 622 size: 64\n",
      "Batch 623 size: 64\n",
      "Batch 624 size: 64\n",
      "Batch 625 size: 64\n",
      "Batch 626 size: 64\n",
      "Batch 627 size: 64\n",
      "Batch 628 size: 64\n",
      "Batch 629 size: 64\n",
      "Batch 630 size: 64\n",
      "Batch 631 size: 64\n",
      "Batch 632 size: 64\n",
      "Batch 633 size: 64\n",
      "Batch 634 size: 64\n",
      "Batch 635 size: 64\n",
      "Batch 636 size: 64\n",
      "Batch 637 size: 64\n",
      "Batch 638 size: 64\n",
      "Batch 639 size: 64\n",
      "Batch 640 size: 64\n",
      "Batch 641 size: 64\n",
      "Batch 642 size: 64\n",
      "Batch 643 size: 64\n",
      "Batch 644 size: 64\n",
      "Batch 645 size: 64\n",
      "Batch 646 size: 64\n",
      "Batch 647 size: 64\n",
      "Batch 648 size: 64\n",
      "Batch 649 size: 64\n",
      "Batch 650 size: 64\n",
      "Batch 651 size: 64\n",
      "Batch 652 size: 64\n",
      "Batch 653 size: 64\n",
      "Batch 654 size: 64\n",
      "Batch 655 size: 64\n",
      "Batch 656 size: 64\n",
      "Batch 657 size: 64\n",
      "Batch 658 size: 64\n",
      "Batch 659 size: 64\n",
      "Batch 660 size: 64\n",
      "Batch 661 size: 64\n",
      "Batch 662 size: 64\n",
      "Batch 663 size: 64\n",
      "Batch 664 size: 64\n",
      "Batch 665 size: 64\n",
      "Batch 666 size: 64\n",
      "Batch 667 size: 64\n",
      "Batch 668 size: 64\n",
      "Batch 669 size: 64\n",
      "Batch 670 size: 64\n",
      "Batch 671 size: 64\n",
      "Batch 672 size: 64\n",
      "Batch 673 size: 64\n",
      "Batch 674 size: 64\n",
      "Batch 675 size: 64\n",
      "Batch 676 size: 64\n",
      "Batch 677 size: 64\n",
      "Batch 678 size: 64\n",
      "Batch 679 size: 64\n",
      "Batch 680 size: 64\n",
      "Batch 681 size: 64\n",
      "Batch 682 size: 64\n",
      "Batch 683 size: 64\n",
      "Batch 684 size: 64\n",
      "Batch 685 size: 64\n",
      "Batch 686 size: 64\n",
      "Batch 687 size: 64\n",
      "Batch 688 size: 64\n",
      "Batch 689 size: 64\n",
      "Batch 690 size: 64\n",
      "Batch 691 size: 64\n",
      "Batch 692 size: 64\n",
      "Batch 693 size: 64\n",
      "Batch 694 size: 64\n",
      "Batch 695 size: 64\n",
      "Batch 696 size: 64\n",
      "Batch 697 size: 64\n",
      "Batch 698 size: 64\n",
      "Batch 699 size: 64\n",
      "Batch 700 size: 64\n",
      "Batch 701 size: 64\n",
      "Batch 702 size: 64\n",
      "Batch 703 size: 64\n",
      "Batch 704 size: 64\n",
      "Batch 705 size: 64\n",
      "Batch 706 size: 64\n",
      "Batch 707 size: 64\n",
      "Batch 708 size: 64\n",
      "Batch 709 size: 64\n",
      "Batch 710 size: 64\n",
      "Batch 711 size: 64\n",
      "Batch 712 size: 64\n",
      "Batch 713 size: 64\n",
      "Batch 714 size: 64\n",
      "Batch 715 size: 64\n",
      "Batch 716 size: 64\n",
      "Batch 717 size: 64\n",
      "Batch 718 size: 64\n",
      "Batch 719 size: 64\n",
      "Batch 720 size: 64\n",
      "Batch 721 size: 64\n",
      "Batch 722 size: 64\n",
      "Batch 723 size: 64\n",
      "Batch 724 size: 64\n",
      "Batch 725 size: 64\n",
      "Batch 726 size: 64\n",
      "Batch 727 size: 64\n",
      "Batch 728 size: 64\n",
      "Batch 729 size: 64\n",
      "Batch 730 size: 64\n",
      "Batch 731 size: 64\n",
      "Batch 732 size: 64\n",
      "Batch 733 size: 64\n",
      "Batch 734 size: 64\n",
      "Batch 735 size: 64\n",
      "Batch 736 size: 64\n",
      "Batch 737 size: 64\n",
      "Batch 738 size: 64\n",
      "Batch 739 size: 64\n",
      "Batch 740 size: 64\n",
      "Batch 741 size: 64\n",
      "Batch 742 size: 64\n",
      "Batch 743 size: 64\n",
      "Batch 744 size: 64\n",
      "Batch 745 size: 64\n",
      "Batch 746 size: 64\n",
      "Batch 747 size: 64\n",
      "Batch 748 size: 64\n",
      "Batch 749 size: 64\n",
      "Batch 750 size: 64\n",
      "Batch 751 size: 64\n",
      "Batch 752 size: 64\n",
      "Batch 753 size: 64\n",
      "Batch 754 size: 64\n",
      "Batch 755 size: 64\n",
      "Batch 756 size: 64\n",
      "Batch 757 size: 64\n",
      "Batch 758 size: 64\n",
      "Batch 759 size: 64\n",
      "Batch 760 size: 64\n",
      "Batch 761 size: 64\n",
      "Batch 762 size: 64\n",
      "Batch 763 size: 64\n",
      "Batch 764 size: 64\n",
      "Batch 765 size: 64\n",
      "Batch 766 size: 64\n",
      "Batch 767 size: 64\n",
      "Batch 768 size: 64\n",
      "Batch 769 size: 64\n",
      "Batch 770 size: 64\n",
      "Batch 771 size: 64\n",
      "Batch 772 size: 64\n",
      "Batch 773 size: 64\n",
      "Batch 774 size: 64\n",
      "Batch 775 size: 64\n",
      "Batch 776 size: 64\n",
      "Batch 777 size: 64\n",
      "Batch 778 size: 64\n",
      "Batch 779 size: 64\n",
      "Batch 780 size: 64\n",
      "Batch 781 size: 64\n",
      "Batch 782 size: 64\n",
      "Batch 783 size: 64\n",
      "Batch 784 size: 64\n",
      "Batch 785 size: 64\n",
      "Batch 786 size: 64\n",
      "Batch 787 size: 64\n",
      "Batch 788 size: 64\n",
      "Batch 789 size: 64\n",
      "Batch 790 size: 64\n",
      "Batch 791 size: 64\n",
      "Batch 792 size: 64\n",
      "Batch 793 size: 64\n",
      "Batch 794 size: 64\n",
      "Batch 795 size: 64\n",
      "Batch 796 size: 64\n",
      "Batch 797 size: 64\n",
      "Batch 798 size: 64\n",
      "Batch 799 size: 64\n",
      "Batch 800 size: 64\n",
      "Batch 801 size: 64\n",
      "Batch 802 size: 64\n",
      "Batch 803 size: 64\n",
      "Batch 804 size: 64\n",
      "Batch 805 size: 64\n",
      "Batch 806 size: 64\n",
      "Batch 807 size: 64\n",
      "Batch 808 size: 64\n",
      "Batch 809 size: 64\n",
      "Batch 810 size: 64\n",
      "Batch 811 size: 64\n",
      "Batch 812 size: 64\n",
      "Batch 813 size: 64\n",
      "Batch 814 size: 64\n",
      "Batch 815 size: 64\n",
      "Batch 816 size: 64\n",
      "Batch 817 size: 64\n",
      "Batch 818 size: 64\n",
      "Batch 819 size: 64\n",
      "Batch 820 size: 64\n",
      "Batch 821 size: 64\n",
      "Batch 822 size: 64\n",
      "Batch 823 size: 64\n",
      "Batch 824 size: 64\n",
      "Batch 825 size: 64\n",
      "Batch 826 size: 64\n",
      "Batch 827 size: 64\n",
      "Batch 828 size: 64\n",
      "Batch 829 size: 64\n",
      "Batch 830 size: 64\n",
      "Batch 831 size: 64\n",
      "Batch 832 size: 64\n",
      "Batch 833 size: 64\n",
      "Batch 834 size: 64\n",
      "Batch 835 size: 64\n",
      "Batch 836 size: 64\n",
      "Batch 837 size: 64\n",
      "Batch 838 size: 64\n",
      "Batch 839 size: 64\n",
      "Batch 840 size: 64\n",
      "Batch 841 size: 64\n",
      "Batch 842 size: 64\n",
      "Batch 843 size: 64\n",
      "Batch 844 size: 64\n",
      "Batch 845 size: 64\n",
      "Batch 846 size: 64\n",
      "Batch 847 size: 64\n",
      "Batch 848 size: 64\n",
      "Batch 849 size: 64\n",
      "Batch 850 size: 64\n",
      "Batch 851 size: 64\n",
      "Batch 852 size: 64\n",
      "Batch 853 size: 64\n",
      "Batch 854 size: 64\n",
      "Batch 855 size: 64\n",
      "Batch 856 size: 64\n",
      "Batch 857 size: 64\n",
      "Batch 858 size: 64\n",
      "Batch 859 size: 64\n",
      "Batch 860 size: 64\n",
      "Batch 861 size: 64\n",
      "Batch 862 size: 64\n",
      "Batch 863 size: 64\n",
      "Batch 864 size: 64\n",
      "Batch 865 size: 64\n",
      "Batch 866 size: 64\n",
      "Batch 867 size: 64\n",
      "Batch 868 size: 64\n",
      "Batch 869 size: 64\n",
      "Batch 870 size: 64\n",
      "Batch 871 size: 64\n",
      "Batch 872 size: 64\n",
      "Batch 873 size: 64\n",
      "Batch 874 size: 64\n",
      "Batch 875 size: 64\n",
      "Batch 876 size: 64\n",
      "Batch 877 size: 64\n",
      "Batch 878 size: 64\n",
      "Batch 879 size: 64\n",
      "Batch 880 size: 64\n",
      "Batch 881 size: 64\n",
      "Batch 882 size: 64\n",
      "Batch 883 size: 64\n",
      "Batch 884 size: 64\n",
      "Batch 885 size: 64\n",
      "Batch 886 size: 64\n",
      "Batch 887 size: 64\n",
      "Batch 888 size: 64\n",
      "Batch 889 size: 64\n",
      "Batch 890 size: 64\n",
      "Batch 891 size: 64\n",
      "Batch 892 size: 64\n",
      "Batch 893 size: 64\n",
      "Batch 894 size: 64\n",
      "Batch 895 size: 64\n",
      "Batch 896 size: 64\n",
      "Batch 897 size: 64\n",
      "Batch 898 size: 64\n",
      "Batch 899 size: 64\n",
      "Batch 900 size: 64\n",
      "Batch 901 size: 64\n",
      "Batch 902 size: 64\n",
      "Batch 903 size: 64\n",
      "Batch 904 size: 64\n",
      "Batch 905 size: 64\n",
      "Batch 906 size: 64\n",
      "Batch 907 size: 64\n",
      "Batch 908 size: 64\n",
      "Batch 909 size: 64\n",
      "Batch 910 size: 64\n",
      "Batch 911 size: 64\n",
      "Batch 912 size: 64\n",
      "Batch 913 size: 64\n",
      "Batch 914 size: 64\n",
      "Batch 915 size: 64\n",
      "Batch 916 size: 64\n",
      "Batch 917 size: 64\n",
      "Batch 918 size: 64\n",
      "Batch 919 size: 64\n",
      "Batch 920 size: 64\n",
      "Batch 921 size: 64\n",
      "Batch 922 size: 64\n",
      "Batch 923 size: 64\n",
      "Batch 924 size: 64\n",
      "Batch 925 size: 64\n",
      "Batch 926 size: 64\n",
      "Batch 927 size: 64\n",
      "Batch 928 size: 64\n",
      "Batch 929 size: 64\n",
      "Batch 930 size: 64\n",
      "Batch 931 size: 64\n",
      "Batch 932 size: 64\n",
      "Batch 933 size: 64\n",
      "Batch 934 size: 64\n",
      "Batch 935 size: 64\n",
      "Batch 936 size: 64\n",
      "Batch 937 size: 64\n",
      "Batch 938 size: 64\n",
      "Batch 939 size: 64\n",
      "Batch 940 size: 64\n",
      "Batch 941 size: 64\n",
      "Batch 942 size: 64\n",
      "Batch 943 size: 64\n",
      "Batch 944 size: 64\n",
      "Batch 945 size: 64\n",
      "Batch 946 size: 64\n",
      "Batch 947 size: 64\n",
      "Batch 948 size: 64\n",
      "Batch 949 size: 64\n",
      "Batch 950 size: 64\n",
      "Batch 951 size: 64\n",
      "Batch 952 size: 64\n",
      "Batch 953 size: 64\n",
      "Batch 954 size: 64\n",
      "Batch 955 size: 64\n",
      "Batch 956 size: 64\n",
      "Batch 957 size: 64\n",
      "Batch 958 size: 64\n",
      "Batch 959 size: 64\n",
      "Batch 960 size: 64\n",
      "Batch 961 size: 64\n",
      "Batch 962 size: 64\n",
      "Batch 963 size: 64\n",
      "Batch 964 size: 64\n",
      "Batch 965 size: 64\n",
      "Batch 966 size: 64\n",
      "Batch 967 size: 64\n",
      "Batch 968 size: 64\n",
      "Batch 969 size: 64\n",
      "Batch 970 size: 64\n",
      "Batch 971 size: 64\n",
      "Batch 972 size: 64\n",
      "Batch 973 size: 64\n",
      "Batch 974 size: 64\n",
      "Batch 975 size: 64\n",
      "Batch 976 size: 64\n",
      "Batch 977 size: 64\n",
      "Batch 978 size: 64\n",
      "Batch 979 size: 64\n",
      "Batch 980 size: 64\n",
      "Batch 981 size: 64\n",
      "Batch 982 size: 64\n",
      "Batch 983 size: 64\n",
      "Batch 984 size: 64\n",
      "Batch 985 size: 64\n",
      "Batch 986 size: 64\n",
      "Batch 987 size: 64\n",
      "Batch 988 size: 64\n",
      "Batch 989 size: 64\n",
      "Batch 990 size: 64\n",
      "Batch 991 size: 64\n",
      "Batch 992 size: 64\n",
      "Batch 993 size: 64\n",
      "Batch 994 size: 64\n",
      "Batch 995 size: 64\n",
      "Batch 996 size: 64\n",
      "Batch 997 size: 64\n",
      "Batch 998 size: 64\n",
      "Batch 999 size: 64\n",
      "Batch 1000 size: 64\n",
      "Batch 1001 size: 64\n",
      "Batch 1002 size: 64\n",
      "Batch 1003 size: 64\n",
      "Batch 1004 size: 64\n",
      "Batch 1005 size: 64\n",
      "Batch 1006 size: 64\n",
      "Batch 1007 size: 64\n",
      "Batch 1008 size: 64\n",
      "Batch 1009 size: 64\n",
      "Batch 1010 size: 64\n",
      "Batch 1011 size: 64\n",
      "Batch 1012 size: 64\n",
      "Batch 1013 size: 64\n",
      "Batch 1014 size: 64\n",
      "Batch 1015 size: 64\n",
      "Batch 1016 size: 64\n",
      "Batch 1017 size: 64\n",
      "Batch 1018 size: 64\n",
      "Batch 1019 size: 64\n",
      "Batch 1020 size: 64\n",
      "Batch 1021 size: 64\n",
      "Batch 1022 size: 64\n",
      "Batch 1023 size: 64\n",
      "Batch 1024 size: 64\n",
      "Batch 1025 size: 64\n",
      "Batch 1026 size: 64\n",
      "Batch 1027 size: 64\n",
      "Batch 1028 size: 64\n",
      "Batch 1029 size: 64\n",
      "Batch 1030 size: 64\n",
      "Batch 1031 size: 64\n",
      "Batch 1032 size: 64\n",
      "Batch 1033 size: 64\n",
      "Batch 1034 size: 64\n",
      "Batch 1035 size: 64\n",
      "Batch 1036 size: 64\n",
      "Batch 1037 size: 64\n",
      "Batch 1038 size: 64\n",
      "Batch 1039 size: 64\n",
      "Batch 1040 size: 64\n",
      "Batch 1041 size: 64\n",
      "Batch 1042 size: 64\n",
      "Batch 1043 size: 64\n",
      "Batch 1044 size: 64\n",
      "Batch 1045 size: 64\n",
      "Batch 1046 size: 64\n",
      "Batch 1047 size: 64\n",
      "Batch 1048 size: 64\n",
      "Batch 1049 size: 64\n",
      "Batch 1050 size: 64\n",
      "Batch 1051 size: 64\n",
      "Batch 1052 size: 64\n",
      "Batch 1053 size: 64\n",
      "Batch 1054 size: 64\n",
      "Batch 1055 size: 64\n",
      "Batch 1056 size: 64\n",
      "Batch 1057 size: 64\n",
      "Batch 1058 size: 64\n",
      "Batch 1059 size: 64\n",
      "Batch 1060 size: 64\n",
      "Batch 1061 size: 64\n",
      "Batch 1062 size: 64\n",
      "Batch 1063 size: 64\n",
      "Batch 1064 size: 64\n",
      "Batch 1065 size: 64\n",
      "Batch 1066 size: 64\n",
      "Batch 1067 size: 64\n",
      "Batch 1068 size: 64\n",
      "Batch 1069 size: 64\n",
      "Batch 1070 size: 64\n",
      "Batch 1071 size: 64\n",
      "Batch 1072 size: 64\n",
      "Batch 1073 size: 64\n",
      "Batch 1074 size: 64\n",
      "Batch 1075 size: 64\n",
      "Batch 1076 size: 64\n",
      "Batch 1077 size: 64\n",
      "Batch 1078 size: 64\n",
      "Batch 1079 size: 64\n",
      "Batch 1080 size: 64\n",
      "Batch 1081 size: 64\n",
      "Batch 1082 size: 64\n",
      "Batch 1083 size: 64\n",
      "Batch 1084 size: 64\n",
      "Batch 1085 size: 64\n",
      "Batch 1086 size: 64\n",
      "Batch 1087 size: 64\n",
      "Batch 1088 size: 64\n",
      "Batch 1089 size: 64\n",
      "Batch 1090 size: 64\n",
      "Batch 1091 size: 64\n",
      "Batch 1092 size: 64\n",
      "Batch 1093 size: 64\n",
      "Batch 1094 size: 64\n",
      "Batch 1095 size: 64\n",
      "Batch 1096 size: 64\n",
      "Batch 1097 size: 64\n",
      "Batch 1098 size: 64\n",
      "Batch 1099 size: 64\n",
      "Batch 1100 size: 64\n",
      "Batch 1101 size: 64\n",
      "Batch 1102 size: 64\n",
      "Batch 1103 size: 64\n",
      "Batch 1104 size: 64\n",
      "Batch 1105 size: 64\n",
      "Batch 1106 size: 64\n",
      "Batch 1107 size: 64\n",
      "Batch 1108 size: 64\n",
      "Batch 1109 size: 64\n",
      "Batch 1110 size: 64\n",
      "Batch 1111 size: 64\n",
      "Batch 1112 size: 64\n",
      "Batch 1113 size: 64\n",
      "Batch 1114 size: 64\n",
      "Batch 1115 size: 64\n",
      "Batch 1116 size: 64\n",
      "Batch 1117 size: 64\n",
      "Batch 1118 size: 64\n",
      "Batch 1119 size: 64\n",
      "Batch 1120 size: 64\n",
      "Batch 1121 size: 64\n",
      "Batch 1122 size: 64\n",
      "Batch 1123 size: 64\n",
      "Batch 1124 size: 64\n",
      "Batch 1125 size: 64\n",
      "Batch 1126 size: 64\n",
      "Batch 1127 size: 64\n",
      "Batch 1128 size: 64\n",
      "Batch 1129 size: 64\n",
      "Batch 1130 size: 64\n",
      "Batch 1131 size: 64\n",
      "Batch 1132 size: 64\n",
      "Batch 1133 size: 64\n",
      "Batch 1134 size: 64\n",
      "Batch 1135 size: 64\n",
      "Batch 1136 size: 64\n",
      "Batch 1137 size: 64\n",
      "Batch 1138 size: 64\n",
      "Batch 1139 size: 64\n",
      "Batch 1140 size: 64\n",
      "Batch 1141 size: 64\n",
      "Batch 1142 size: 64\n",
      "Batch 1143 size: 64\n",
      "Batch 1144 size: 64\n",
      "Batch 1145 size: 64\n",
      "Batch 1146 size: 64\n",
      "Batch 1147 size: 64\n",
      "Batch 1148 size: 64\n",
      "Batch 1149 size: 64\n",
      "Batch 1150 size: 64\n",
      "Batch 1151 size: 64\n",
      "Batch 1152 size: 64\n",
      "Batch 1153 size: 64\n",
      "Batch 1154 size: 64\n",
      "Batch 1155 size: 64\n",
      "Batch 1156 size: 64\n",
      "Batch 1157 size: 64\n",
      "Batch 1158 size: 64\n",
      "Batch 1159 size: 64\n",
      "Batch 1160 size: 64\n",
      "Batch 1161 size: 64\n",
      "Batch 1162 size: 64\n",
      "Batch 1163 size: 64\n",
      "Batch 1164 size: 64\n",
      "Batch 1165 size: 64\n",
      "Batch 1166 size: 64\n",
      "Batch 1167 size: 64\n",
      "Batch 1168 size: 64\n",
      "Batch 1169 size: 64\n",
      "Batch 1170 size: 64\n",
      "Batch 1171 size: 64\n",
      "Batch 1172 size: 64\n",
      "Batch 1173 size: 64\n",
      "Batch 1174 size: 64\n",
      "Batch 1175 size: 64\n",
      "Batch 1176 size: 64\n",
      "Batch 1177 size: 64\n",
      "Batch 1178 size: 64\n",
      "Batch 1179 size: 64\n",
      "Batch 1180 size: 64\n",
      "Batch 1181 size: 64\n",
      "Batch 1182 size: 64\n",
      "Batch 1183 size: 64\n",
      "Batch 1184 size: 64\n",
      "Batch 1185 size: 64\n",
      "Batch 1186 size: 64\n",
      "Batch 1187 size: 64\n",
      "Batch 1188 size: 64\n",
      "Batch 1189 size: 64\n",
      "Batch 1190 size: 64\n",
      "Batch 1191 size: 64\n",
      "Batch 1192 size: 64\n",
      "Batch 1193 size: 64\n",
      "Batch 1194 size: 64\n",
      "Batch 1195 size: 64\n",
      "Batch 1196 size: 64\n",
      "Batch 1197 size: 64\n",
      "Batch 1198 size: 64\n",
      "Batch 1199 size: 64\n",
      "Batch 1200 size: 64\n",
      "Batch 1201 size: 64\n",
      "Batch 1202 size: 64\n",
      "Batch 1203 size: 64\n",
      "Batch 1204 size: 64\n",
      "Batch 1205 size: 64\n",
      "Batch 1206 size: 64\n",
      "Batch 1207 size: 64\n",
      "Batch 1208 size: 64\n",
      "Batch 1209 size: 64\n",
      "Batch 1210 size: 64\n",
      "Batch 1211 size: 64\n",
      "Batch 1212 size: 64\n",
      "Batch 1213 size: 64\n",
      "Batch 1214 size: 64\n",
      "Batch 1215 size: 64\n",
      "Batch 1216 size: 64\n",
      "Batch 1217 size: 64\n",
      "Batch 1218 size: 64\n",
      "Batch 1219 size: 64\n",
      "Batch 1220 size: 64\n",
      "Batch 1221 size: 64\n",
      "Batch 1222 size: 64\n",
      "Batch 1223 size: 64\n",
      "Batch 1224 size: 64\n",
      "Batch 1225 size: 64\n",
      "Batch 1226 size: 64\n",
      "Batch 1227 size: 64\n",
      "Batch 1228 size: 64\n",
      "Batch 1229 size: 64\n",
      "Batch 1230 size: 64\n",
      "Batch 1231 size: 64\n",
      "Batch 1232 size: 64\n",
      "Batch 1233 size: 64\n",
      "Batch 1234 size: 64\n",
      "Batch 1235 size: 64\n",
      "Batch 1236 size: 64\n",
      "Batch 1237 size: 64\n",
      "Batch 1238 size: 64\n",
      "Batch 1239 size: 64\n",
      "Batch 1240 size: 64\n",
      "Batch 1241 size: 64\n",
      "Batch 1242 size: 64\n",
      "Batch 1243 size: 64\n",
      "Batch 1244 size: 64\n",
      "Batch 1245 size: 64\n",
      "Batch 1246 size: 64\n",
      "Batch 1247 size: 64\n",
      "Batch 1248 size: 64\n",
      "Batch 1249 size: 64\n",
      "Batch 1250 size: 64\n",
      "Batch 1251 size: 64\n",
      "Batch 1252 size: 64\n",
      "Batch 1253 size: 64\n",
      "Batch 1254 size: 64\n",
      "Batch 1255 size: 64\n",
      "Batch 1256 size: 64\n",
      "Batch 1257 size: 64\n",
      "Batch 1258 size: 64\n",
      "Batch 1259 size: 64\n",
      "Batch 1260 size: 64\n",
      "Batch 1261 size: 64\n",
      "Batch 1262 size: 64\n",
      "Batch 1263 size: 64\n",
      "Batch 1264 size: 64\n",
      "Batch 1265 size: 64\n",
      "Batch 1266 size: 64\n",
      "Batch 1267 size: 64\n",
      "Batch 1268 size: 64\n",
      "Batch 1269 size: 64\n",
      "Batch 1270 size: 64\n",
      "Batch 1271 size: 64\n",
      "Batch 1272 size: 64\n",
      "Batch 1273 size: 64\n",
      "Batch 1274 size: 64\n",
      "Batch 1275 size: 64\n",
      "Batch 1276 size: 64\n",
      "Batch 1277 size: 64\n",
      "Batch 1278 size: 64\n",
      "Batch 1279 size: 64\n",
      "Batch 1280 size: 64\n",
      "Batch 1281 size: 64\n",
      "Batch 1282 size: 64\n",
      "Batch 1283 size: 64\n",
      "Batch 1284 size: 64\n",
      "Batch 1285 size: 64\n",
      "Batch 1286 size: 64\n",
      "Batch 1287 size: 64\n",
      "Batch 1288 size: 64\n",
      "Batch 1289 size: 64\n",
      "Batch 1290 size: 64\n",
      "Batch 1291 size: 64\n",
      "Batch 1292 size: 64\n",
      "Batch 1293 size: 64\n",
      "Batch 1294 size: 64\n",
      "Batch 1295 size: 64\n",
      "Batch 1296 size: 64\n",
      "Batch 1297 size: 64\n",
      "Batch 1298 size: 64\n",
      "Batch 1299 size: 64\n",
      "Batch 1300 size: 64\n",
      "Batch 1301 size: 64\n",
      "Batch 1302 size: 64\n",
      "Batch 1303 size: 64\n",
      "Batch 1304 size: 64\n",
      "Batch 1305 size: 64\n",
      "Batch 1306 size: 64\n",
      "Batch 1307 size: 64\n",
      "Batch 1308 size: 64\n",
      "Batch 1309 size: 64\n",
      "Batch 1310 size: 64\n",
      "Batch 1311 size: 64\n",
      "Batch 1312 size: 64\n",
      "Batch 1313 size: 64\n",
      "Batch 1314 size: 64\n",
      "Batch 1315 size: 64\n",
      "Batch 1316 size: 64\n",
      "Batch 1317 size: 64\n",
      "Batch 1318 size: 64\n",
      "Batch 1319 size: 64\n",
      "Batch 1320 size: 64\n",
      "Batch 1321 size: 64\n",
      "Batch 1322 size: 64\n",
      "Batch 1323 size: 64\n",
      "Batch 1324 size: 64\n",
      "Batch 1325 size: 64\n",
      "Batch 1326 size: 64\n",
      "Batch 1327 size: 64\n",
      "Batch 1328 size: 64\n",
      "Batch 1329 size: 64\n",
      "Batch 1330 size: 64\n",
      "Batch 1331 size: 64\n",
      "Batch 1332 size: 64\n",
      "Batch 1333 size: 64\n",
      "Batch 1334 size: 64\n",
      "Batch 1335 size: 64\n",
      "Batch 1336 size: 64\n",
      "Batch 1337 size: 64\n",
      "Batch 1338 size: 64\n",
      "Batch 1339 size: 64\n",
      "Batch 1340 size: 64\n",
      "Batch 1341 size: 64\n",
      "Batch 1342 size: 64\n",
      "Batch 1343 size: 64\n",
      "Batch 1344 size: 64\n",
      "Batch 1345 size: 64\n",
      "Batch 1346 size: 64\n",
      "Batch 1347 size: 64\n",
      "Batch 1348 size: 64\n",
      "Batch 1349 size: 64\n",
      "Batch 1350 size: 64\n",
      "Batch 1351 size: 64\n",
      "Batch 1352 size: 64\n",
      "Batch 1353 size: 64\n",
      "Batch 1354 size: 64\n",
      "Batch 1355 size: 64\n",
      "Batch 1356 size: 64\n",
      "Batch 1357 size: 64\n",
      "Batch 1358 size: 64\n",
      "Batch 1359 size: 64\n",
      "Batch 1360 size: 64\n",
      "Batch 1361 size: 64\n",
      "Batch 1362 size: 64\n",
      "Batch 1363 size: 64\n",
      "Batch 1364 size: 64\n",
      "Batch 1365 size: 64\n",
      "Batch 1366 size: 64\n",
      "Batch 1367 size: 64\n",
      "Batch 1368 size: 64\n",
      "Batch 1369 size: 64\n",
      "Batch 1370 size: 64\n",
      "Batch 1371 size: 64\n",
      "Batch 1372 size: 64\n",
      "Batch 1373 size: 64\n",
      "Batch 1374 size: 64\n",
      "Batch 1375 size: 64\n",
      "Batch 1376 size: 64\n",
      "Batch 1377 size: 64\n",
      "Batch 1378 size: 64\n",
      "Batch 1379 size: 64\n",
      "Batch 1380 size: 64\n",
      "Batch 1381 size: 64\n",
      "Batch 1382 size: 64\n",
      "Batch 1383 size: 64\n",
      "Batch 1384 size: 64\n",
      "Batch 1385 size: 64\n",
      "Batch 1386 size: 64\n",
      "Batch 1387 size: 64\n",
      "Batch 1388 size: 64\n",
      "Batch 1389 size: 64\n",
      "Batch 1390 size: 64\n",
      "Batch 1391 size: 64\n",
      "Batch 1392 size: 64\n",
      "Batch 1393 size: 64\n",
      "Batch 1394 size: 64\n",
      "Batch 1395 size: 64\n",
      "Batch 1396 size: 64\n",
      "Batch 1397 size: 64\n",
      "Batch 1398 size: 64\n",
      "Batch 1399 size: 64\n",
      "Batch 1400 size: 64\n",
      "Batch 1401 size: 64\n",
      "Batch 1402 size: 64\n",
      "Batch 1403 size: 64\n",
      "Batch 1404 size: 64\n",
      "Batch 1405 size: 64\n",
      "Batch 1406 size: 64\n",
      "Batch 1407 size: 64\n",
      "Batch 1408 size: 64\n",
      "Batch 1409 size: 64\n",
      "Batch 1410 size: 64\n",
      "Batch 1411 size: 64\n",
      "Batch 1412 size: 64\n",
      "Batch 1413 size: 64\n",
      "Batch 1414 size: 64\n",
      "Batch 1415 size: 64\n",
      "Batch 1416 size: 64\n",
      "Batch 1417 size: 64\n",
      "Batch 1418 size: 64\n",
      "Batch 1419 size: 64\n",
      "Batch 1420 size: 64\n",
      "Batch 1421 size: 64\n",
      "Batch 1422 size: 64\n",
      "Batch 1423 size: 64\n",
      "Batch 1424 size: 64\n",
      "Batch 1425 size: 64\n",
      "Batch 1426 size: 64\n",
      "Batch 1427 size: 64\n",
      "Batch 1428 size: 64\n",
      "Batch 1429 size: 64\n",
      "Batch 1430 size: 64\n",
      "Batch 1431 size: 64\n",
      "Batch 1432 size: 64\n",
      "Batch 1433 size: 64\n",
      "Batch 1434 size: 64\n",
      "Batch 1435 size: 64\n",
      "Batch 1436 size: 64\n",
      "Batch 1437 size: 64\n",
      "Batch 1438 size: 64\n",
      "Batch 1439 size: 64\n",
      "Batch 1440 size: 64\n",
      "Batch 1441 size: 64\n",
      "Batch 1442 size: 64\n",
      "Batch 1443 size: 64\n",
      "Batch 1444 size: 64\n",
      "Batch 1445 size: 64\n",
      "Batch 1446 size: 64\n",
      "Batch 1447 size: 64\n",
      "Batch 1448 size: 64\n",
      "Batch 1449 size: 64\n",
      "Batch 1450 size: 64\n",
      "Batch 1451 size: 64\n",
      "Batch 1452 size: 64\n",
      "Batch 1453 size: 64\n",
      "Batch 1454 size: 64\n",
      "Batch 1455 size: 64\n",
      "Batch 1456 size: 64\n",
      "Batch 1457 size: 64\n",
      "Batch 1458 size: 64\n",
      "Batch 1459 size: 64\n",
      "Batch 1460 size: 64\n",
      "Batch 1461 size: 64\n",
      "Batch 1462 size: 64\n",
      "Batch 1463 size: 64\n",
      "Batch 1464 size: 64\n",
      "Batch 1465 size: 64\n",
      "Batch 1466 size: 64\n",
      "Batch 1467 size: 64\n",
      "Batch 1468 size: 64\n",
      "Batch 1469 size: 64\n",
      "Batch 1470 size: 64\n",
      "Batch 1471 size: 64\n",
      "Batch 1472 size: 64\n",
      "Batch 1473 size: 64\n",
      "Batch 1474 size: 64\n",
      "Batch 1475 size: 64\n",
      "Batch 1476 size: 64\n",
      "Batch 1477 size: 64\n",
      "Batch 1478 size: 64\n",
      "Batch 1479 size: 64\n",
      "Batch 1480 size: 64\n",
      "Batch 1481 size: 64\n",
      "Batch 1482 size: 64\n",
      "Batch 1483 size: 64\n",
      "Batch 1484 size: 64\n",
      "Batch 1485 size: 64\n",
      "Batch 1486 size: 64\n",
      "Batch 1487 size: 64\n",
      "Batch 1488 size: 64\n",
      "Batch 1489 size: 64\n",
      "Batch 1490 size: 64\n",
      "Batch 1491 size: 64\n",
      "Batch 1492 size: 64\n",
      "Batch 1493 size: 64\n",
      "Batch 1494 size: 64\n",
      "Batch 1495 size: 64\n",
      "Batch 1496 size: 64\n",
      "Batch 1497 size: 64\n",
      "Batch 1498 size: 64\n",
      "Batch 1499 size: 64\n",
      "Batch 1500 size: 64\n",
      "Batch 1501 size: 64\n",
      "Batch 1502 size: 64\n",
      "Batch 1503 size: 64\n",
      "Batch 1504 size: 64\n",
      "Batch 1505 size: 64\n",
      "Batch 1506 size: 64\n",
      "Batch 1507 size: 64\n",
      "Batch 1508 size: 64\n",
      "Batch 1509 size: 64\n",
      "Batch 1510 size: 64\n",
      "Batch 1511 size: 64\n",
      "Batch 1512 size: 64\n",
      "Batch 1513 size: 64\n",
      "Batch 1514 size: 64\n",
      "Batch 1515 size: 64\n",
      "Batch 1516 size: 64\n",
      "Batch 1517 size: 64\n",
      "Batch 1518 size: 64\n",
      "Batch 1519 size: 64\n",
      "Batch 1520 size: 64\n",
      "Batch 1521 size: 64\n",
      "Batch 1522 size: 64\n",
      "Batch 1523 size: 64\n",
      "Batch 1524 size: 64\n",
      "Batch 1525 size: 64\n",
      "Batch 1526 size: 64\n",
      "Batch 1527 size: 64\n",
      "Batch 1528 size: 64\n",
      "Batch 1529 size: 64\n",
      "Batch 1530 size: 64\n",
      "Batch 1531 size: 64\n",
      "Batch 1532 size: 64\n",
      "Batch 1533 size: 64\n",
      "Batch 1534 size: 64\n",
      "Batch 1535 size: 64\n",
      "Batch 1536 size: 64\n",
      "Batch 1537 size: 64\n",
      "Batch 1538 size: 64\n",
      "Batch 1539 size: 64\n",
      "Batch 1540 size: 64\n",
      "Batch 1541 size: 64\n",
      "Batch 1542 size: 64\n",
      "Batch 1543 size: 64\n",
      "Batch 1544 size: 64\n",
      "Batch 1545 size: 64\n",
      "Batch 1546 size: 64\n",
      "Batch 1547 size: 64\n",
      "Batch 1548 size: 64\n",
      "Batch 1549 size: 64\n",
      "Batch 1550 size: 64\n",
      "Batch 1551 size: 64\n",
      "Batch 1552 size: 64\n",
      "Batch 1553 size: 64\n",
      "Batch 1554 size: 64\n",
      "Batch 1555 size: 64\n",
      "Batch 1556 size: 64\n",
      "Batch 1557 size: 64\n",
      "Batch 1558 size: 64\n",
      "Batch 1559 size: 64\n",
      "Batch 1560 size: 64\n",
      "Batch 1561 size: 64\n",
      "Batch 1562 size: 64\n",
      "Batch 1563 size: 64\n",
      "Batch 1564 size: 64\n",
      "Batch 1565 size: 64\n",
      "Batch 1566 size: 64\n",
      "Batch 1567 size: 64\n",
      "Batch 1568 size: 64\n",
      "Batch 1569 size: 64\n",
      "Batch 1570 size: 64\n",
      "Batch 1571 size: 64\n",
      "Batch 1572 size: 64\n",
      "Batch 1573 size: 64\n",
      "Batch 1574 size: 64\n",
      "Batch 1575 size: 64\n",
      "Batch 1576 size: 64\n",
      "Batch 1577 size: 64\n",
      "Batch 1578 size: 64\n",
      "Batch 1579 size: 64\n",
      "Batch 1580 size: 64\n",
      "Batch 1581 size: 64\n",
      "Batch 1582 size: 64\n",
      "Batch 1583 size: 64\n",
      "Batch 1584 size: 64\n",
      "Batch 1585 size: 64\n",
      "Batch 1586 size: 64\n",
      "Batch 1587 size: 64\n",
      "Batch 1588 size: 64\n",
      "Batch 1589 size: 64\n",
      "Batch 1590 size: 64\n",
      "Batch 1591 size: 64\n",
      "Batch 1592 size: 64\n",
      "Batch 1593 size: 64\n",
      "Batch 1594 size: 64\n",
      "Batch 1595 size: 64\n",
      "Batch 1596 size: 64\n",
      "Batch 1597 size: 64\n",
      "Batch 1598 size: 64\n",
      "Batch 1599 size: 64\n",
      "Batch 1600 size: 64\n",
      "Batch 1601 size: 64\n",
      "Batch 1602 size: 64\n",
      "Batch 1603 size: 64\n",
      "Batch 1604 size: 64\n",
      "Batch 1605 size: 64\n",
      "Batch 1606 size: 64\n",
      "Batch 1607 size: 64\n",
      "Batch 1608 size: 64\n",
      "Batch 1609 size: 64\n",
      "Batch 1610 size: 64\n",
      "Batch 1611 size: 64\n",
      "Batch 1612 size: 64\n",
      "Batch 1613 size: 64\n",
      "Batch 1614 size: 64\n",
      "Batch 1615 size: 64\n",
      "Batch 1616 size: 64\n",
      "Batch 1617 size: 64\n",
      "Batch 1618 size: 64\n",
      "Batch 1619 size: 64\n",
      "Batch 1620 size: 64\n",
      "Batch 1621 size: 64\n",
      "Batch 1622 size: 64\n",
      "Batch 1623 size: 64\n",
      "Batch 1624 size: 64\n",
      "Batch 1625 size: 64\n",
      "Batch 1626 size: 64\n",
      "Batch 1627 size: 64\n",
      "Batch 1628 size: 64\n",
      "Batch 1629 size: 64\n",
      "Batch 1630 size: 64\n",
      "Batch 1631 size: 64\n",
      "Batch 1632 size: 64\n",
      "Batch 1633 size: 64\n",
      "Batch 1634 size: 64\n",
      "Batch 1635 size: 64\n",
      "Batch 1636 size: 64\n",
      "Batch 1637 size: 64\n",
      "Batch 1638 size: 64\n",
      "Batch 1639 size: 64\n",
      "Batch 1640 size: 64\n",
      "Batch 1641 size: 64\n",
      "Batch 1642 size: 64\n",
      "Batch 1643 size: 64\n",
      "Batch 1644 size: 64\n",
      "Batch 1645 size: 64\n",
      "Batch 1646 size: 64\n",
      "Batch 1647 size: 64\n",
      "Batch 1648 size: 64\n",
      "Batch 1649 size: 64\n",
      "Batch 1650 size: 64\n",
      "Batch 1651 size: 64\n",
      "Batch 1652 size: 64\n",
      "Batch 1653 size: 64\n",
      "Batch 1654 size: 64\n",
      "Batch 1655 size: 64\n",
      "Batch 1656 size: 64\n",
      "Batch 1657 size: 64\n",
      "Batch 1658 size: 64\n",
      "Batch 1659 size: 64\n",
      "Batch 1660 size: 64\n",
      "Batch 1661 size: 64\n",
      "Batch 1662 size: 64\n",
      "Batch 1663 size: 64\n",
      "Batch 1664 size: 64\n",
      "Batch 1665 size: 64\n",
      "Batch 1666 size: 64\n",
      "Batch 1667 size: 64\n",
      "Batch 1668 size: 64\n",
      "Batch 1669 size: 64\n",
      "Batch 1670 size: 64\n",
      "Batch 1671 size: 64\n",
      "Batch 1672 size: 64\n",
      "Batch 1673 size: 64\n",
      "Batch 1674 size: 64\n",
      "Batch 1675 size: 64\n",
      "Batch 1676 size: 64\n",
      "Batch 1677 size: 64\n",
      "Batch 1678 size: 64\n",
      "Batch 1679 size: 64\n",
      "Batch 1680 size: 64\n",
      "Batch 1681 size: 64\n",
      "Batch 1682 size: 64\n",
      "Batch 1683 size: 64\n",
      "Batch 1684 size: 64\n",
      "Batch 1685 size: 64\n",
      "Batch 1686 size: 64\n",
      "Batch 1687 size: 64\n",
      "Batch 1688 size: 64\n",
      "Batch 1689 size: 64\n",
      "Batch 1690 size: 64\n",
      "Batch 1691 size: 64\n",
      "Batch 1692 size: 64\n",
      "Batch 1693 size: 64\n",
      "Batch 1694 size: 64\n",
      "Batch 1695 size: 64\n",
      "Batch 1696 size: 64\n",
      "Batch 1697 size: 64\n",
      "Batch 1698 size: 64\n",
      "Batch 1699 size: 64\n",
      "Batch 1700 size: 64\n",
      "Batch 1701 size: 64\n",
      "Batch 1702 size: 64\n",
      "Batch 1703 size: 64\n",
      "Batch 1704 size: 64\n",
      "Batch 1705 size: 64\n",
      "Batch 1706 size: 64\n",
      "Batch 1707 size: 64\n",
      "Batch 1708 size: 64\n",
      "Batch 1709 size: 64\n",
      "Batch 1710 size: 64\n",
      "Batch 1711 size: 64\n",
      "Batch 1712 size: 64\n",
      "Batch 1713 size: 64\n",
      "Batch 1714 size: 64\n",
      "Batch 1715 size: 64\n",
      "Batch 1716 size: 64\n",
      "Batch 1717 size: 64\n",
      "Batch 1718 size: 64\n",
      "Batch 1719 size: 64\n",
      "Batch 1720 size: 64\n",
      "Batch 1721 size: 64\n",
      "Batch 1722 size: 64\n",
      "Batch 1723 size: 64\n",
      "Batch 1724 size: 64\n",
      "Batch 1725 size: 64\n",
      "Batch 1726 size: 64\n",
      "Batch 1727 size: 64\n",
      "Batch 1728 size: 64\n",
      "Batch 1729 size: 64\n",
      "Batch 1730 size: 64\n",
      "Batch 1731 size: 64\n",
      "Batch 1732 size: 64\n",
      "Batch 1733 size: 64\n",
      "Batch 1734 size: 64\n",
      "Batch 1735 size: 64\n",
      "Batch 1736 size: 64\n",
      "Batch 1737 size: 64\n",
      "Batch 1738 size: 64\n",
      "Batch 1739 size: 64\n",
      "Batch 1740 size: 64\n",
      "Batch 1741 size: 64\n",
      "Batch 1742 size: 64\n",
      "Batch 1743 size: 64\n",
      "Batch 1744 size: 64\n",
      "Batch 1745 size: 64\n",
      "Batch 1746 size: 64\n",
      "Batch 1747 size: 64\n",
      "Batch 1748 size: 64\n",
      "Batch 1749 size: 64\n",
      "Batch 1750 size: 64\n",
      "Batch 1751 size: 64\n",
      "Batch 1752 size: 64\n",
      "Batch 1753 size: 64\n",
      "Batch 1754 size: 64\n",
      "Batch 1755 size: 64\n",
      "Batch 1756 size: 64\n",
      "Batch 1757 size: 64\n",
      "Batch 1758 size: 64\n",
      "Batch 1759 size: 64\n",
      "Batch 1760 size: 64\n",
      "Batch 1761 size: 64\n",
      "Batch 1762 size: 64\n",
      "Batch 1763 size: 64\n",
      "Batch 1764 size: 64\n",
      "Batch 1765 size: 64\n",
      "Batch 1766 size: 64\n",
      "Batch 1767 size: 64\n",
      "Batch 1768 size: 64\n",
      "Batch 1769 size: 64\n",
      "Batch 1770 size: 64\n",
      "Batch 1771 size: 64\n",
      "Batch 1772 size: 64\n",
      "Batch 1773 size: 64\n",
      "Batch 1774 size: 64\n",
      "Batch 1775 size: 64\n",
      "Batch 1776 size: 64\n",
      "Batch 1777 size: 64\n",
      "Batch 1778 size: 64\n",
      "Batch 1779 size: 64\n",
      "Batch 1780 size: 64\n",
      "Batch 1781 size: 64\n",
      "Batch 1782 size: 64\n",
      "Batch 1783 size: 64\n",
      "Batch 1784 size: 64\n",
      "Batch 1785 size: 64\n",
      "Batch 1786 size: 64\n",
      "Batch 1787 size: 64\n",
      "Batch 1788 size: 64\n",
      "Batch 1789 size: 64\n",
      "Batch 1790 size: 64\n",
      "Batch 1791 size: 64\n",
      "Batch 1792 size: 64\n",
      "Batch 1793 size: 64\n",
      "Batch 1794 size: 64\n",
      "Batch 1795 size: 64\n",
      "Batch 1796 size: 64\n",
      "Batch 1797 size: 64\n",
      "Batch 1798 size: 64\n",
      "Batch 1799 size: 64\n",
      "Batch 1800 size: 64\n",
      "Batch 1801 size: 64\n",
      "Batch 1802 size: 64\n",
      "Batch 1803 size: 64\n",
      "Batch 1804 size: 64\n",
      "Batch 1805 size: 64\n",
      "Batch 1806 size: 64\n",
      "Batch 1807 size: 64\n",
      "Batch 1808 size: 64\n",
      "Batch 1809 size: 64\n",
      "Batch 1810 size: 64\n",
      "Batch 1811 size: 64\n",
      "Batch 1812 size: 64\n",
      "Batch 1813 size: 64\n",
      "Batch 1814 size: 64\n",
      "Batch 1815 size: 64\n",
      "Batch 1816 size: 64\n",
      "Batch 1817 size: 64\n",
      "Batch 1818 size: 64\n",
      "Batch 1819 size: 64\n",
      "Batch 1820 size: 64\n",
      "Batch 1821 size: 64\n",
      "Batch 1822 size: 64\n",
      "Batch 1823 size: 64\n",
      "Batch 1824 size: 64\n",
      "Batch 1825 size: 64\n",
      "Batch 1826 size: 64\n",
      "Batch 1827 size: 64\n",
      "Batch 1828 size: 64\n",
      "Batch 1829 size: 64\n",
      "Batch 1830 size: 64\n",
      "Batch 1831 size: 64\n",
      "Batch 1832 size: 64\n",
      "Batch 1833 size: 64\n",
      "Batch 1834 size: 64\n",
      "Batch 1835 size: 64\n",
      "Batch 1836 size: 64\n",
      "Batch 1837 size: 64\n",
      "Batch 1838 size: 64\n",
      "Batch 1839 size: 64\n",
      "Batch 1840 size: 64\n",
      "Batch 1841 size: 64\n",
      "Batch 1842 size: 64\n",
      "Batch 1843 size: 64\n",
      "Batch 1844 size: 64\n",
      "Batch 1845 size: 64\n",
      "Batch 1846 size: 64\n",
      "Batch 1847 size: 64\n",
      "Batch 1848 size: 64\n",
      "Batch 1849 size: 64\n",
      "Batch 1850 size: 64\n",
      "Batch 1851 size: 64\n",
      "Batch 1852 size: 64\n",
      "Batch 1853 size: 64\n",
      "Batch 1854 size: 64\n",
      "Batch 1855 size: 64\n",
      "Batch 1856 size: 64\n",
      "Batch 1857 size: 64\n",
      "Batch 1858 size: 64\n",
      "Batch 1859 size: 64\n",
      "Batch 1860 size: 64\n",
      "Batch 1861 size: 64\n",
      "Batch 1862 size: 64\n",
      "Batch 1863 size: 64\n",
      "Batch 1864 size: 64\n",
      "Batch 1865 size: 64\n",
      "Batch 1866 size: 64\n",
      "Batch 1867 size: 64\n",
      "Batch 1868 size: 64\n",
      "Batch 1869 size: 64\n",
      "Batch 1870 size: 64\n",
      "Batch 1871 size: 64\n",
      "Batch 1872 size: 64\n",
      "Batch 1873 size: 64\n",
      "Batch 1874 size: 64\n",
      "Batch 1875 size: 64\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(Dataloaders['Test']):\n",
    "    batch_size = len(batch[0])  # Assuming the first element of the batch is the data\n",
    "    print(f\"Batch {i+1} size: {batch_size}\")\n",
    "    if batch_size != 64:\n",
    "        print(f\"Batch {i+1} does not contain 64 records.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "mZH7kgxF9p00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1670395868510,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "mZH7kgxF9p00",
    "outputId": "3b5f2da8-cb90-43b1-ea26-b5a9fad4dd31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 size: 64\n",
      "Batch 2 size: 64\n",
      "Batch 3 size: 64\n",
      "Batch 4 size: 64\n",
      "Batch 5 size: 64\n",
      "Batch 6 size: 64\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(Dataloaders[5][0]):\n",
    "    batch_size = len(batch[0])  # Assuming the first element of the batch is the data\n",
    "    print(f\"Batch {i+1} size: {batch_size}\")\n",
    "    if batch_size != 64:\n",
    "        print(f\"Batch {i+1} does not contain 64 records.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ad4c70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.2265625]\n",
      "[7.03125]\n",
      "[7.421875]\n",
      "[7.486979166666667]\n",
      "[25.0]\n",
      "[25.0]\n",
      "[25.0]\n",
      "[25.0]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "for CLUSTER in range (1, 9):\n",
    "    DEVICE_PERCENTAGE = []\n",
    "    for DEVICE__ in range(0,NUM_CLIENTS):\n",
    "        for i, batch in enumerate(Dataloaders[CLUSTER][DEVICE__]):\n",
    "            _, labels = batch\n",
    "            class_counts = Counter(labels.numpy())\n",
    "            total_records = sum(class_counts.values())\n",
    "            class_0_count = class_counts.get(0, 0)\n",
    "            percentage_class_0 = (class_0_count / total_records) * 100\n",
    "            DEVICE_PERCENTAGE.append(percentage_class_0)\n",
    "            # print(f\"Batch {i+1}: {dict(class_counts)}\")\n",
    "            # print(f\"Percentage of class 0: {percentage_class_0:.2f}%\\n\")\n",
    "    # print(DEVICE_PERCENTAGE)        \n",
    "    chunk_size = 6\n",
    "    averages = [sum(DEVICE_PERCENTAGE[i:i + chunk_size]) / chunk_size for i in range(0, len(DEVICE_PERCENTAGE), chunk_size)]\n",
    "    # print(\"Averages of every device:\")\n",
    "    # print(averages)\n",
    "    chunk_size_4 = 4\n",
    "    averages = [sum(averages[i:i + chunk_size_4]) / chunk_size_4 for i in range(0, len(averages), chunk_size_4)]\n",
    "    # print(\"Averages of every 4 devices:\")\n",
    "    print(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d0e3a112-2124-4571-802b-61850c0be001",
   "metadata": {},
   "outputs": [],
   "source": [
    "del TrafficData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1f66c2-dd6f-458f-93e3-85b93ef1fb50",
   "metadata": {
    "id": "gjVnC-rj9gC4",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='Red'>***Neural Network***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fLD8dUBg9pyY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1670813208940,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "fLD8dUBg9pyY",
    "outputId": "6df101c4-37c9-4660-f5c7-3a42be8b8951"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()        \n",
    "        self.layer_1 = nn.Linear(98, 64)\n",
    "        self.layer_2 = nn.Linear(64, 32)\n",
    "        self.layer_out = nn.Linear(32, 15) \n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_out(x)        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ffc624e0-c16a-413d-bb70-c7fc7743def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random  = Net()\n",
    "# for param_tensor in Random.state_dict():\n",
    "#     print(param_tensor, \"\\t\", Random.state_dict()[param_tensor].size())\n",
    "# torch.save(Random.state_dict(), \"0_Input_Random_model_Net.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "SKRdGrET9pvn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1670813234860,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "SKRdGrET9pvn",
    "outputId": "fe9c8747-dad6-4606-f067-021d127d259c"
   },
   "outputs": [],
   "source": [
    "def train(net, trainloader, epochs: int, verbose=True):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    net.train()\n",
    "    prediction_matrix = []\n",
    "    actual_matrix= []\n",
    "    acc_matrix = []\n",
    "    loss_matrix=[]\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(net(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "            predictions = torch.max(outputs.data, 1)[1]\n",
    "            prediction_matrix.append(predictions.tolist())\n",
    "            actual_matrix.append(labels.tolist())\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        loss_matrix.append(epoch_loss.tolist())\n",
    "        acc_matrix.append(epoch_acc)\n",
    "    return prediction_matrix, actual_matrix, acc_matrix, loss_matrix\n",
    "def test(net, testloader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    prediction_matrix = []\n",
    "    actual_matrix= []\n",
    "    acc_matrix = []\n",
    "    loss_matrix=[]\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            prediction_matrix.append(predicted.tolist())\n",
    "            actual_matrix.append(labels.tolist())\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    loss_matrix.append(loss)\n",
    "    acc_matrix.append(accuracy)    \n",
    "    print(f\"Evaluation: eval loss {loss}, eval accuracy {accuracy}\")\n",
    "    return loss, accuracy, prediction_matrix, actual_matrix, acc_matrix, loss_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "otE9jhmS-IXU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1670813239232,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "otE9jhmS-IXU",
    "outputId": "ec01b70c-2185-4293-87ec-b3eaa8b6d92a"
   },
   "outputs": [],
   "source": [
    "prediction_dict= {}\n",
    "actual_dict= {}\n",
    "accuracy_dict= {}\n",
    "loss_dict= {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d181247",
   "metadata": {
    "tags": []
   },
   "source": [
    "***Running Centralized DNN Rounds***\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "654d0bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting centralized Round-by-Round training...\n",
      "\n",
      "========================\n",
      "   ROUND 1 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 2.648248 | Acc: 0.3880\n",
      "  Epoch 2/3 | Loss: 2.561884 | Acc: 0.5443\n",
      "  Epoch 3/3 | Loss: 2.419816 | Acc: 0.5547\n",
      "  Evaluating round 1 model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Test  Loss: 2.240329, Acc: 0.6998\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_1.pth\n",
      "========================\n",
      "   ROUND 1 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 2 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 2.193308 | Acc: 0.4688\n",
      "  Epoch 2/3 | Loss: 1.910542 | Acc: 0.4479\n",
      "  Epoch 3/3 | Loss: 1.604797 | Acc: 0.4870\n",
      "  Evaluating round 2 model...\n",
      "  Test  Loss: 1.241717, Acc: 0.6865\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_2.pth\n",
      "========================\n",
      "   ROUND 2 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 3 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 1.346509 | Acc: 0.4714\n",
      "  Epoch 2/3 | Loss: 1.132511 | Acc: 0.6068\n",
      "  Epoch 3/3 | Loss: 0.987226 | Acc: 0.6771\n",
      "  Evaluating round 3 model...\n",
      "  Test  Loss: 0.718454, Acc: 0.7428\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_3.pth\n",
      "========================\n",
      "   ROUND 3 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 4 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.864556 | Acc: 0.7214\n",
      "  Epoch 2/3 | Loss: 0.775878 | Acc: 0.7214\n",
      "  Epoch 3/3 | Loss: 0.711538 | Acc: 0.8542\n",
      "  Evaluating round 4 model...\n",
      "  Test  Loss: 0.552415, Acc: 0.8585\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_4.pth\n",
      "========================\n",
      "   ROUND 4 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 5 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.073482 | Acc: 0.9792\n",
      "  Epoch 2/3 | Loss: 0.009824 | Acc: 1.0000\n",
      "  Epoch 3/3 | Loss: 0.001840 | Acc: 1.0000\n",
      "  Evaluating round 5 model...\n",
      "  Test  Loss: 1.478460, Acc: 0.6119\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_5.pth\n",
      "========================\n",
      "   ROUND 5 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 6 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.000268 | Acc: 1.0000\n",
      "  Epoch 2/3 | Loss: 0.000028 | Acc: 1.0000\n",
      "  Epoch 3/3 | Loss: 0.000006 | Acc: 1.0000\n",
      "  Evaluating round 6 model...\n",
      "  Test  Loss: 3.640064, Acc: 0.5000\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_6.pth\n",
      "========================\n",
      "   ROUND 6 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 7 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.000001 | Acc: 1.0000\n",
      "  Epoch 2/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Epoch 3/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Evaluating round 7 model...\n",
      "  Test  Loss: 6.278809, Acc: 0.5000\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_7.pth\n",
      "========================\n",
      "   ROUND 7 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 8 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Epoch 2/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Epoch 3/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Evaluating round 8 model...\n",
      "  Test  Loss: 7.343506, Acc: 0.5000\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_8.pth\n",
      "========================\n",
      "   ROUND 8 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 9 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 3.794370 | Acc: 0.6979\n",
      "  Epoch 2/3 | Loss: 2.285649 | Acc: 0.7266\n",
      "  Epoch 3/3 | Loss: 1.375188 | Acc: 0.7656\n",
      "  Evaluating round 9 model...\n",
      "  Test  Loss: 1.632686, Acc: 0.5998\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_9.pth\n",
      "========================\n",
      "   ROUND 9 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 10 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.727679 | Acc: 0.7786\n",
      "  Epoch 2/3 | Loss: 0.375041 | Acc: 0.8776\n",
      "  Epoch 3/3 | Loss: 0.345194 | Acc: 0.9010\n",
      "  Evaluating round 10 model...\n",
      "  Test  Loss: 0.502565, Acc: 0.8459\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_10.pth\n",
      "========================\n",
      "   ROUND 10 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 11 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.334842 | Acc: 0.8672\n",
      "  Epoch 2/3 | Loss: 0.317197 | Acc: 0.8958\n",
      "  Epoch 3/3 | Loss: 0.300150 | Acc: 0.8880\n",
      "  Evaluating round 11 model...\n",
      "  Test  Loss: 0.449744, Acc: 0.8121\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_11.pth\n",
      "========================\n",
      "   ROUND 11 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 12 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.289062 | Acc: 0.9193\n",
      "  Epoch 2/3 | Loss: 0.272407 | Acc: 0.9583\n",
      "  Epoch 3/3 | Loss: 0.256020 | Acc: 0.9609\n",
      "  Evaluating round 12 model...\n",
      "  Test  Loss: 0.390449, Acc: 0.9455\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_12.pth\n",
      "========================\n",
      "   ROUND 12 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 13 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.353012 | Acc: 0.9062\n",
      "  Epoch 2/3 | Loss: 0.329900 | Acc: 0.9219\n",
      "  Epoch 3/3 | Loss: 0.311008 | Acc: 0.9375\n",
      "  Evaluating round 13 model...\n",
      "  Test  Loss: 0.320533, Acc: 0.9548\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_13.pth\n",
      "========================\n",
      "   ROUND 13 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 14 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.302632 | Acc: 0.9479\n",
      "  Epoch 2/3 | Loss: 0.280005 | Acc: 0.9635\n",
      "  Epoch 3/3 | Loss: 0.257722 | Acc: 0.9583\n",
      "  Evaluating round 14 model...\n",
      "  Test  Loss: 0.268723, Acc: 0.9565\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_14.pth\n",
      "========================\n",
      "   ROUND 14 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 15 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.265404 | Acc: 0.9635\n",
      "  Epoch 2/3 | Loss: 0.253040 | Acc: 0.9505\n",
      "  Epoch 3/3 | Loss: 0.226493 | Acc: 0.9635\n",
      "  Evaluating round 15 model...\n",
      "  Test  Loss: 0.227154, Acc: 0.9559\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_15.pth\n",
      "========================\n",
      "   ROUND 15 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 16 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.203157 | Acc: 0.9766\n",
      "  Epoch 2/3 | Loss: 0.184157 | Acc: 0.9766\n",
      "  Epoch 3/3 | Loss: 0.161963 | Acc: 0.9766\n",
      "  Evaluating round 16 model...\n",
      "  Test  Loss: 0.181612, Acc: 0.9636\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_16.pth\n",
      "========================\n",
      "   ROUND 16 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 17 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.226196 | Acc: 0.9583\n",
      "  Epoch 2/3 | Loss: 0.205505 | Acc: 0.9583\n",
      "  Epoch 3/3 | Loss: 0.186719 | Acc: 0.9609\n",
      "  Evaluating round 17 model...\n",
      "  Test  Loss: 0.154088, Acc: 0.9609\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_17.pth\n",
      "========================\n",
      "   ROUND 17 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 18 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.205265 | Acc: 0.9531\n",
      "  Epoch 2/3 | Loss: 0.189180 | Acc: 0.9505\n",
      "  Epoch 3/3 | Loss: 0.178152 | Acc: 0.9557\n",
      "  Evaluating round 18 model...\n",
      "  Test  Loss: 0.154531, Acc: 0.9505\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_18.pth\n",
      "========================\n",
      "   ROUND 18 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 19 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.182583 | Acc: 0.9531\n",
      "  Epoch 2/3 | Loss: 0.164155 | Acc: 0.9583\n",
      "  Epoch 3/3 | Loss: 0.154623 | Acc: 0.9583\n",
      "  Evaluating round 19 model...\n",
      "  Test  Loss: 0.121142, Acc: 0.9632\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_19.pth\n",
      "========================\n",
      "   ROUND 19 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 20 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.144799 | Acc: 0.9583\n",
      "  Epoch 2/3 | Loss: 0.137207 | Acc: 0.9583\n",
      "  Epoch 3/3 | Loss: 0.131028 | Acc: 0.9583\n",
      "  Evaluating round 20 model...\n",
      "  Test  Loss: 0.116116, Acc: 0.9610\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_20.pth\n",
      "========================\n",
      "   ROUND 20 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 21 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.003976 | Acc: 1.0000\n",
      "  Epoch 2/3 | Loss: 0.000047 | Acc: 1.0000\n",
      "  Epoch 3/3 | Loss: 0.000012 | Acc: 1.0000\n",
      "  Evaluating round 21 model...\n",
      "  Test  Loss: 0.660269, Acc: 0.8197\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_21.pth\n",
      "========================\n",
      "   ROUND 21 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 22 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.000038 | Acc: 1.0000\n",
      "  Epoch 2/3 | Loss: 0.000001 | Acc: 1.0000\n",
      "  Epoch 3/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Evaluating round 22 model...\n",
      "  Test  Loss: 1.935815, Acc: 0.7640\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_22.pth\n",
      "========================\n",
      "   ROUND 22 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 23 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Epoch 2/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Epoch 3/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Evaluating round 23 model...\n",
      "  Test  Loss: 3.050034, Acc: 0.7416\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_23.pth\n",
      "========================\n",
      "   ROUND 23 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 24 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Epoch 2/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Epoch 3/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Evaluating round 24 model...\n",
      "  Test  Loss: 3.111083, Acc: 0.7409\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_24.pth\n",
      "========================\n",
      "   ROUND 24 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 25 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 1.491291 | Acc: 0.8464\n",
      "  Epoch 2/3 | Loss: 0.604266 | Acc: 0.8828\n",
      "  Epoch 3/3 | Loss: 0.136182 | Acc: 0.9401\n",
      "  Evaluating round 25 model...\n",
      "  Test  Loss: 0.118013, Acc: 0.9710\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_25.pth\n",
      "========================\n",
      "   ROUND 25 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 26 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.060628 | Acc: 0.9844\n",
      "  Epoch 2/3 | Loss: 0.055767 | Acc: 0.9818\n",
      "  Epoch 3/3 | Loss: 0.053240 | Acc: 0.9844\n",
      "  Evaluating round 26 model...\n",
      "  Test  Loss: 0.125487, Acc: 0.9614\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_26.pth\n",
      "========================\n",
      "   ROUND 26 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 27 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.075832 | Acc: 0.9766\n",
      "  Epoch 2/3 | Loss: 0.065327 | Acc: 0.9818\n",
      "  Epoch 3/3 | Loss: 0.059983 | Acc: 0.9844\n",
      "  Evaluating round 27 model...\n",
      "  Test  Loss: 0.103845, Acc: 0.9713\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_27.pth\n",
      "========================\n",
      "   ROUND 27 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 28 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.071165 | Acc: 0.9792\n",
      "  Epoch 2/3 | Loss: 0.062513 | Acc: 0.9792\n",
      "  Epoch 3/3 | Loss: 0.061047 | Acc: 0.9818\n",
      "  Evaluating round 28 model...\n",
      "  Test  Loss: 0.109264, Acc: 0.9738\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_28.pth\n",
      "========================\n",
      "   ROUND 28 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 29 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.122987 | Acc: 0.9714\n",
      "  Epoch 2/3 | Loss: 0.110362 | Acc: 0.9740\n",
      "  Epoch 3/3 | Loss: 0.110876 | Acc: 0.9740\n",
      "  Evaluating round 29 model...\n",
      "  Test  Loss: 0.107193, Acc: 0.9738\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_29.pth\n",
      "========================\n",
      "   ROUND 29 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 30 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.140973 | Acc: 0.9427\n",
      "  Epoch 2/3 | Loss: 0.125502 | Acc: 0.9583\n",
      "  Epoch 3/3 | Loss: 0.121578 | Acc: 0.9583\n",
      "  Evaluating round 30 model...\n",
      "  Test  Loss: 0.091840, Acc: 0.9740\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_30.pth\n",
      "========================\n",
      "   ROUND 30 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 31 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.115201 | Acc: 0.9714\n",
      "  Epoch 2/3 | Loss: 0.105037 | Acc: 0.9766\n",
      "  Epoch 3/3 | Loss: 0.101060 | Acc: 0.9766\n",
      "  Evaluating round 31 model...\n",
      "  Test  Loss: 0.095897, Acc: 0.9713\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_31.pth\n",
      "========================\n",
      "   ROUND 31 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 32 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.116029 | Acc: 0.9688\n",
      "  Epoch 2/3 | Loss: 0.112098 | Acc: 0.9688\n",
      "  Epoch 3/3 | Loss: 0.105391 | Acc: 0.9714\n",
      "  Evaluating round 32 model...\n",
      "  Test  Loss: 0.088680, Acc: 0.9731\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_32.pth\n",
      "========================\n",
      "   ROUND 32 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 33 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.127922 | Acc: 0.9635\n",
      "  Epoch 2/3 | Loss: 0.128319 | Acc: 0.9609\n",
      "  Epoch 3/3 | Loss: 0.121288 | Acc: 0.9661\n",
      "  Evaluating round 33 model...\n",
      "  Test  Loss: 0.089958, Acc: 0.9730\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_33.pth\n",
      "========================\n",
      "   ROUND 33 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 34 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.174080 | Acc: 0.9453\n",
      "  Epoch 2/3 | Loss: 0.161111 | Acc: 0.9479\n",
      "  Epoch 3/3 | Loss: 0.157501 | Acc: 0.9505\n",
      "  Evaluating round 34 model...\n",
      "  Test  Loss: 0.093397, Acc: 0.9704\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_34.pth\n",
      "========================\n",
      "   ROUND 34 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 35 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.128698 | Acc: 0.9583\n",
      "  Epoch 2/3 | Loss: 0.121974 | Acc: 0.9609\n",
      "  Epoch 3/3 | Loss: 0.117020 | Acc: 0.9635\n",
      "  Evaluating round 35 model...\n",
      "  Test  Loss: 0.083831, Acc: 0.9734\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_35.pth\n",
      "========================\n",
      "   ROUND 35 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 36 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.143657 | Acc: 0.9583\n",
      "  Epoch 2/3 | Loss: 0.131805 | Acc: 0.9661\n",
      "  Epoch 3/3 | Loss: 0.130741 | Acc: 0.9661\n",
      "  Evaluating round 36 model...\n",
      "  Test  Loss: 0.083672, Acc: 0.9723\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_36.pth\n",
      "========================\n",
      "   ROUND 36 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 37 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.007469 | Acc: 0.9974\n",
      "  Epoch 2/3 | Loss: 0.000458 | Acc: 1.0000\n",
      "  Epoch 3/3 | Loss: 0.000079 | Acc: 1.0000\n",
      "  Evaluating round 37 model...\n",
      "  Test  Loss: 0.204606, Acc: 0.9149\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_37.pth\n",
      "========================\n",
      "   ROUND 37 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 38 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.000002 | Acc: 1.0000\n",
      "  Epoch 2/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Epoch 3/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Evaluating round 38 model...\n",
      "  Test  Loss: 1.216755, Acc: 0.8061\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_38.pth\n",
      "========================\n",
      "   ROUND 38 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 39 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Epoch 2/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Epoch 3/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Evaluating round 39 model...\n",
      "  Test  Loss: 2.718609, Acc: 0.7682\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_39.pth\n",
      "========================\n",
      "   ROUND 39 DONE\n",
      "========================\n",
      "\n",
      "========================\n",
      "   ROUND 40 START\n",
      "========================\n",
      "  Train samples this round: 384\n",
      "  Epoch 1/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Epoch 2/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Epoch 3/3 | Loss: 0.000000 | Acc: 1.0000\n",
      "  Evaluating round 40 model...\n",
      "  Test  Loss: 2.736924, Acc: 0.7677\n",
      "   Saved: DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_40.pth\n",
      "========================\n",
      "   ROUND 40 DONE\n",
      "========================\n",
      "\n",
      " Centralized Round-by-Round training completed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Centralized Round-by-Round Training (No FL)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Training / Evaluation\n",
    "# ============================================================\n",
    "\n",
    "def train_one_round(model, trainloader, epochs):\n",
    "    \"\"\"Runs EPOCHS of training on this round's dataset.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, running_loss = 0, 0, 0.0\n",
    "        for Xbatch, ybatch in trainloader:\n",
    "            Xbatch, ybatch = Xbatch.to(DEVICE), ybatch.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(Xbatch)\n",
    "            loss = criterion(outputs, ybatch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            total += ybatch.size(0)\n",
    "            correct += (torch.max(outputs, 1)[1] == ybatch).sum().item()\n",
    "\n",
    "        print(f\"  Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Loss: {running_loss/len(trainloader):.6f} | \"\n",
    "              f\"Acc: {correct/total:.4f}\")\n",
    "\n",
    "\n",
    "def test_model(model, testloader):\n",
    "    \"\"\"Runs a simple test pass (no saving).\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    correct, total, running_loss = 0, 0, 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for Xbatch, ybatch in testloader:\n",
    "            Xbatch, ybatch = Xbatch.to(DEVICE), ybatch.to(DEVICE)\n",
    "            outputs = model(Xbatch)\n",
    "            loss = criterion(outputs, ybatch).item()\n",
    "\n",
    "            running_loss += loss\n",
    "            total += ybatch.size(0)\n",
    "            correct += (torch.max(outputs, 1)[1] == ybatch).sum().item()\n",
    "\n",
    "    print(f\"  Test  Loss: {running_loss/len(testloader):.6f}, \"\n",
    "          f\"Acc: {correct/total:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Centralized Training Loop\n",
    "# ============================================================\n",
    "\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "print(\" Starting centralized Round-by-Round training...\\n\")\n",
    "\n",
    "model = Net().to(DEVICE)\n",
    "\n",
    "for Round in range(1, ROUNDS + 1):\n",
    "\n",
    "    print(\"========================\")\n",
    "    print(f\"   ROUND {Round} START\")\n",
    "    print(\"========================\")\n",
    "\n",
    "    # Build round dataset from client partitions\n",
    "    client_datasets = [\n",
    "        loader.dataset for loader in Dataloaders[Round]\n",
    "        if hasattr(loader, \"dataset\") and len(loader.dataset) > 0\n",
    "    ]\n",
    "\n",
    "    round_dataset = ConcatDataset(client_datasets)\n",
    "    trainloader = DataLoader(round_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    print(f\"  Train samples this round: {len(round_dataset)}\")\n",
    "\n",
    "    # Train\n",
    "    train_one_round(model, trainloader, EPOCHS)\n",
    "\n",
    "    # Evaluate (optional)\n",
    "    print(f\"  Evaluating round {Round} model...\")\n",
    "    test_model(model, Dataloaders[\"Test\"])\n",
    "\n",
    "    # Save model\n",
    "    save_path = f\"{PATH}/GlobalModel_{Round}.pth\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"   Saved: {save_path}\")\n",
    "\n",
    "    print(\"========================\")\n",
    "    print(f\"   ROUND {Round} DONE\")\n",
    "    print(\"========================\\n\")\n",
    "\n",
    "print(\" Centralized Round-by-Round training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555eb63-e297-4c7b-8e1b-5985d2cde0e9",
   "metadata": {
    "id": "l0PgqJTEwwWk",
    "tags": []
   },
   "source": [
    "<font color='Grey'>***Performance Testing***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e9f39255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_1.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_2.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_3.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_4.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_5.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_6.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_7.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_8.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_9.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_10.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_11.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_12.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_13.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_14.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_15.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_16.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_17.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_18.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_19.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_20.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_21.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_22.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_23.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_24.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_25.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_26.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_27.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_28.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_29.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_30.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_31.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_32.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_33.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_34.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_35.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_36.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_37.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_38.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_39.pth\n",
      "Loading DNNCent5atk_40_rounds_1_clients_3_epochs_64_batch_0.0018_lr_120_data_groups//GlobalModel_40.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the directory and file pattern\n",
    "directory = PATH + '/'\n",
    "pattern = \"GlobalModel_*.pth\"\n",
    "\n",
    "# Find all matching files\n",
    "files = glob.glob(os.path.join(directory, pattern))\n",
    "\n",
    "# Extract numbers from file names\n",
    "numbers = []\n",
    "for file in files:\n",
    "    base_name = os.path.basename(file)\n",
    "    num_str = base_name.replace(\"GlobalModel_\", \"\").replace(\".pth\", \"\")\n",
    "    try:\n",
    "        numbers.append(int(num_str))\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "# Determine the maximum number\n",
    "max_num = max(numbers) if numbers else 0\n",
    "print(max_num)\n",
    "\n",
    "# Use the max_num in a loop\n",
    "for num in range(1, max_num + 1):\n",
    "    file_path = f\"{PATH}/GlobalModel_{num}.pth\"\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the file or perform any operation you need\n",
    "        print(f\"Loading {file_path}\")\n",
    "    else:\n",
    "        print(f\"File {file_path} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c9ec610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_pickle(path, G=None, suffix=None):\n",
    "    print(G)\n",
    "    if G is not None:\n",
    "        rounds = [G]\n",
    "    else:\n",
    "        # Auto-detect how many Global_X files exist\n",
    "        rounds = sorted({\n",
    "            int(f.split(\"_\")[1])\n",
    "            for f in os.listdir(path)\n",
    "            if f.startswith(\"Global_\") and f.split(\"_\")[1].isdigit()\n",
    "        })\n",
    "\n",
    "    suffixes = [suffix] if suffix else [\"pred\", \"actual\", \"accurracy\", \"loss\"]\n",
    "\n",
    "    for g in rounds:\n",
    "        for s in suffixes:\n",
    "            filename = os.path.join(path, f\"Global_{g}_{s}\")\n",
    "            if not os.path.exists(filename):\n",
    "                print(f\"Missing file: {filename}\")\n",
    "                continue\n",
    "            try:\n",
    "                with open(filename, \"rb\") as f:\n",
    "                    data = pickle.load(f)\n",
    "                print(f\"File: {filename}\")\n",
    "                if isinstance(data, list) and len(data) > 2:\n",
    "                    pprint(data[:2])\n",
    "                    print(f\"... ({len(data)} total items)\")\n",
    "                else:\n",
    "                    pprint(data)\n",
    "            except Exception as e:\n",
    "                print(f\" Error reading {filename}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "Q7O6Uj-at0L7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 364,
     "status": "error",
     "timestamp": 1670839612569,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "Q7O6Uj-at0L7",
    "outputId": "e4d532e2-d5a1-4a0c-b486-f5975af6cd0e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_test = {}\n",
    "actual_test = {}\n",
    "accuracy_test = {}\n",
    "loss_test = {}\n",
    "G = 0\n",
    "\n",
    "for num in range(1, max_num+1):\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load(f\"{PATH}/GlobalModel_{num}.pth\"))\n",
    "    model.eval()\n",
    "    \n",
    "    prediction_matrix = []\n",
    "    actual_matrix= []\n",
    "    acc_matrix = []\n",
    "    loss_matrix=[]\n",
    "    G = G + 1\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in Dataloaders['Test']:\n",
    "            outputs = model(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            prediction_matrix.append(predicted.tolist())\n",
    "            actual_matrix.append(labels.tolist())\n",
    "    loss /= len(Dataloaders['Test'].dataset)\n",
    "    accuracy = correct / total\n",
    "    loss_matrix.append(loss)\n",
    "    acc_matrix.append(accuracy) \n",
    "\n",
    "    pred_test[f'Global_{G}'] = prediction_matrix\n",
    "    actual_test[f'Global_{G}'] = actual_matrix\n",
    "    accuracy_test[f'Global_{G}'] = acc_matrix\n",
    "    loss_test[f'Global_{G}'] = loss_matrix \n",
    "\n",
    "    filename = f'{PATH}/Global_{G}_pred'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(pred_test[f'Global_{G}'],outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    filename = f'{PATH}/Global_{G}_actual'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(actual_test[f'Global_{G}'],outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    filename = f'{PATH}/Global_{G}_accurracy'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(accuracy_test[f'Global_{G}'],outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    filename = f'{PATH}/Global_{G}_loss'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(loss_test[f'Global_{G}'],outfile)\n",
    "    outfile.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "bcoA2oGB6Kyi",
    "-hiMkZhLVT63",
    "Fg6LBRPK-E6_",
    "gjVnC-rj9gC4",
    "KNYdybJu5uhn",
    "LwpshLHduBDL",
    "Df92buFDvAl8",
    "D8dfI-OyvOBt",
    "s02Uob4Bvebm",
    "3H-0K0GXvuQo",
    "Y3LLG0dvwAFl",
    "thyLEKcswNst",
    "scNSl39rwhZu"
   ],
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

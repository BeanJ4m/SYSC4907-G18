{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcoA2oGB6Kyi",
   "metadata": {
    "id": "bcoA2oGB6Kyi",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='green'>***Installation and Libraries Import***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b0b1a7-d1b9-403f-8c98-84e601860c8f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# %pip install flwr\n",
    "# %pip install ray \n",
    "# %pip install --upgrade pip\n",
    "# %pip install torch torchvision matplotlib\n",
    "# %pip install async-timeout\n",
    "# %pip install async-numpy\n",
    "# %pip install pandas\n",
    "# %pip install datasets\n",
    "# %pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8316c4a3-e1c6-44ac-b41b-0da26527d7cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import flwr as fl\n",
    "import torch, ray, pandas, sklearn\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# print(\"All modules loaded successfully!\")\n",
    "# print(\"FLWR version:\", fl.__version__)\n",
    "# print(\"Ray version:\", ray.__version__)\n",
    "# print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "# libraries_to_uninstall = [\n",
    "#     \"tb-nightly==2.18.0a20240701\",\n",
    "#     \"tensorboard==2.16.2\",\n",
    "#     \"tensorboard-data-server==0.7.2\",\n",
    "#     \"tensorboard-plugin-wit==1.8.1\",\n",
    "#     \"tensorflow==2.16.2\",\n",
    "#     \"tensorflow-io-gcs-filesystem==0.37.0\",\n",
    "#     \"termcolor==2.4.0\",\n",
    "#     \"terminado==0.18.1\",\n",
    "#     \"tf-estimator-nightly==2.8.0.dev2021122109\",\n",
    "#     \"tf_keras-nightly==2.18.0.dev2024070109\",\n",
    "#     \"tf-nightly==2.18.0.dev20240626\"\n",
    "# ]\n",
    "# for library in libraries_to_uninstall:\n",
    "#     os.system(f\"pip uninstall -y {library}\")\n",
    "# print(\"All modules loaded successfully!\")\n",
    "# print(\"FLWR version:\", fl.__version__)\n",
    "# print(\"Ray version:\", ray.__version__)\n",
    "# print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e677784-3ccb-49de-9aa0-339fdfa926ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, WeightedRandomSampler\n",
    "import flwr as fl\n",
    "from flwr.common import Metrics\n",
    "# from flwr_datasets import FederatedDataset\n",
    "from sklearn.preprocessing import MinMaxScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import csv\n",
    "import copy\n",
    "print(fl.__version__)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fg6LBRPK-E6_",
   "metadata": {
    "id": "Fg6LBRPK-E6_",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='Brown'>***Constants***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZFQQhlvc-c4P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 181,
     "status": "ok",
     "timestamp": 1674161131349,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "ZFQQhlvc-c4P",
    "outputId": "b14bdc45-179f-4a11-fb12-49e6e350ee3f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DEVICE\n",
    "NUM_CLIENTS = 48 #48\n",
    "ROUNDS = 40 #40\n",
    "BATCH_SIZE = 64 #C\n",
    "LEARNING_RATE = 0.0025 #C\n",
    "EPOCHS = 3 #C\n",
    "DATA_GROUPS = 120\n",
    "BATCH_ROUND = 40\n",
    "SIZE_ROUND = int(BATCH_ROUND * BATCH_SIZE * NUM_CLIENTS)\n",
    "NUM_ATCKS = 11\n",
    "FL = True\n",
    "PATH = f'DNNCent{NUM_CLIENTS}-clients-{NUM_ATCKS}atk-{ROUNDS}-rounds-{EPOCHS}-epochs-{LEARNING_RATE}-lr-{DATA_GROUPS}-groups'\n",
    "G = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gjVnC-rj9gC4",
   "metadata": {
    "id": "gjVnC-rj9gC4",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='Light Blue'>***Dataset Preparations***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6e75b-01b6-4bc4-a0e3-af41e9e15d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrafficData = {}\n",
    "TrafficData['Dataset']={}\n",
    "sets_names = ['30','100','70','50','120']\n",
    "for  DATA_NUM in sets_names:\n",
    "    TrafficData['Dataset'][DATA_NUM]=pd.read_csv(f'data/2_Dataset_{NUM_ATCKS}_Attack_{DATA_NUM}_normal.csv', low_memory=False, quoting=csv.QUOTE_NONE, on_bad_lines='skip')\n",
    "    print(DATA_NUM, TrafficData['Dataset'][DATA_NUM].shape)\n",
    "for DATA_NUM in TrafficData['Dataset']:\n",
    "    TrafficData['Dataset'][DATA_NUM]=TrafficData['Dataset'][DATA_NUM].sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2857fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrafficData['Split'] = {}\n",
    "sets_training =  ['30','100','70','50']\n",
    "for DATA_NUM in sets_training:\n",
    "    TrafficData['Split'][DATA_NUM] = np.array_split(TrafficData['Dataset'][DATA_NUM],DATA_GROUPS)\n",
    "\n",
    "TrafficData['Combined'] = pd.concat([TrafficData['Split']['30'][0], TrafficData['Split']['100'][0], TrafficData['Split']['70'][0], TrafficData['Split']['50'][0]]).reset_index(drop=True)\n",
    "for GROUP in range(1, DATA_GROUPS):\n",
    "    TrafficData['Combined'] = pd.concat([TrafficData['Combined'], TrafficData['Split']['30'][GROUP], TrafficData['Split']['100'][GROUP], TrafficData['Split']['70'][GROUP], TrafficData['Split']['50'][GROUP]]).reset_index(drop=True)\n",
    "print(TrafficData['Combined'].shape)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d74ea7-0942-4de4-aa49-cb0c9032e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrafficData['Train'] = {}\n",
    "TrafficData['Train']['X'] = TrafficData['Combined'].iloc[:, 0:-1]\n",
    "TrafficData['Train']['y'] = TrafficData['Combined'].iloc[:, -1]\n",
    "print(TrafficData['Train']['X'].shape)\n",
    "print(TrafficData['Train']['y'].shape)\n",
    "\n",
    "TrafficData['Test'] = {}\n",
    "TrafficData['Test']['X']=TrafficData['Dataset']['120'].iloc[:, 0:-1]\n",
    "TrafficData['Test']['y']=TrafficData['Dataset']['120'].iloc[:, -1]\n",
    "print(TrafficData['Test']['X'].shape)\n",
    "print(TrafficData['Test']['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f406c44d-9a2b-4831-a713-ce5a95134832",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "model = scaler.fit(TrafficData['Train']['X'])\n",
    "TrafficData['Train']['X'] = model.transform(TrafficData['Train']['X'])\n",
    "TrafficData['Test']['X'] = model.transform(TrafficData['Test']['X'])\n",
    "\n",
    "TrafficData['Train']['X'], TrafficData['Train']['y']= np.array(TrafficData['Train']['X']), np.array(TrafficData['Train']['y'])\n",
    "print(type(TrafficData['Train']['X']),type(TrafficData['Train']['y']))\n",
    "print(TrafficData['Train']['X'].shape,TrafficData['Train']['y'].shape)\n",
    "TrafficData['Test']['X'], TrafficData['Test']['y']= np.array(TrafficData['Test']['X']), np.array(TrafficData['Test']['y'])\n",
    "print(type(TrafficData['Test']['X']),type(TrafficData['Test']['y']))\n",
    "print(TrafficData['Test']['X'].shape,TrafficData['Test']['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b2901-2954-4d8c-9185-8e6ac28ad965",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrafficData['ROUNDS']={}\n",
    "for ROUND in range(1, ROUNDS+1):\n",
    "    TrafficData['ROUNDS'][ROUND]={}\n",
    "\n",
    "SIZE_Demo = SIZE_ROUND\n",
    "for ROUND in range(1,ROUNDS+1):\n",
    "    if ROUND == 1:\n",
    "        TrafficData['ROUNDS'][ROUND]['X']= TrafficData['Train']['X'][:SIZE_Demo]\n",
    "        TrafficData['ROUNDS'][ROUND]['y']= TrafficData['Train']['y'][:SIZE_Demo]\n",
    "    else:\n",
    "        print((SIZE_Demo - SIZE_ROUND),SIZE_Demo)\n",
    "        TrafficData['ROUNDS'][ROUND]['X']= TrafficData['Train']['X'][(SIZE_Demo - SIZE_ROUND):SIZE_Demo]\n",
    "        TrafficData['ROUNDS'][ROUND]['y']= TrafficData['Train']['y'][(SIZE_Demo - SIZE_ROUND):SIZE_Demo]\n",
    "    SIZE_Demo = SIZE_Demo + SIZE_ROUND\n",
    "for ROUND in TrafficData['ROUNDS']:\n",
    "    print(\"ROUND: \", ROUND, TrafficData['ROUNDS'][ROUND]['X'].shape, TrafficData['ROUNDS'][ROUND]['y'].shape)\n",
    "print(SIZE_Demo, SIZE_ROUND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f73dc-662e-441e-9361-32fbca07af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ade83a-b7fd-450d-82cb-0649af87fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrafficData['trainsets']={}\n",
    "for ROUND in range(1, ROUNDS+1):\n",
    "    TrafficData['trainsets'][ROUND]= ClassifierDataset(torch.from_numpy(TrafficData['ROUNDS'][ROUND]['X']).float(), torch.from_numpy(TrafficData['ROUNDS'][ROUND]['y']).long())\n",
    "TrafficData['testset'] = ClassifierDataset(torch.from_numpy(TrafficData['Test']['X']).float(), torch.from_numpy(TrafficData['Test']['y']).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d6a1f-95bc-4173-a8b4-8961e3d2875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train(numberofclients, ROUND):    \n",
    "    # Use the actual dataset length to compute balanced portions per client\n",
    "    dataset_len = len(TrafficData['trainsets'][ROUND])\n",
    "    if dataset_len == 0:\n",
    "        # Return an empty list of loaders to avoid indexing errors downstream\n",
    "        return [DataLoader(Subset(TrafficData['trainsets'][ROUND], []), batch_size=BATCH_SIZE, shuffle=False) for _ in range(numberofclients)]\n",
    "    \n",
    "    # Distribute samples as evenly as possible across clients (handle remainders)\n",
    "    num_portions = int(numberofclients)\n",
    "    base_portion = dataset_len // num_portions\n",
    "    remainder = dataset_len % num_portions\n",
    "    portion_indices = []\n",
    "    start_idx = 0\n",
    "    for i in range(num_portions):\n",
    "        # First `remainder` clients receive one extra sample\n",
    "        sz = base_portion + (1 if i < remainder else 0)\n",
    "        end_idx = start_idx + sz\n",
    "        if sz > 0:\n",
    "            portion_indices.append(list(range(start_idx, end_idx)))\n",
    "        else:\n",
    "            portion_indices.append([])\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    # Build Subset and DataLoader for each client (safe: indices are within [0, dataset_len))\n",
    "    portion_datasets = [Subset(TrafficData['trainsets'][ROUND], indices) for indices in portion_indices]\n",
    "    portion_loaders = [DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False) for dataset in portion_datasets]            \n",
    "    return portion_loaders\n",
    "def load_test(numberofclients):    \n",
    "    testloader = DataLoader(TrafficData['testset'], batch_size=BATCH_SIZE, shuffle=False)\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e2e5d8-60da-4260-80bc-4850460279c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataloaders = {}\n",
    "for ROUND in range(1, ROUNDS+1):\n",
    "    Dataloaders[ROUND] = load_train(NUM_CLIENTS, ROUND)\n",
    "Dataloaders['Test'] = load_test(NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ba673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: print per-client dataset sizes for the first few rounds to verify splits\n",
    "for ROUND in range(1, min(6, ROUNDS+1)):\n",
    "    loaders = Dataloaders.get(ROUND, None)\n",
    "    if loaders is None:\n",
    "        print(f\"Dataloaders for Round {ROUND} not found (run the rebuild cell).\")\n",
    "        continue\n",
    "    sizes = [len(loader.dataset) for loader in loaders]\n",
    "    print(f\"Round {ROUND}: per-client sizes (first 12): {sizes[:12]}\")\n",
    "    print(f\"  Total samples this round: {sum(sizes)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6455bfc-e8b5-4d01-84d6-3f484a2c8ad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, batch in enumerate(Dataloaders['Test']):\n",
    "    batch_size = len(batch[0])  # Assuming the first element of the batch is the data\n",
    "    print(f\"Batch {i+1} size: {batch_size}\")\n",
    "    if batch_size != 64:\n",
    "        print(f\"Batch {i+1} does not contain 64 records.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mZH7kgxF9p00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1670395868510,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "mZH7kgxF9p00",
    "outputId": "3b5f2da8-cb90-43b1-ea26-b5a9fad4dd31"
   },
   "outputs": [],
   "source": [
    "for i, batch in enumerate(Dataloaders[5][0]):\n",
    "    batch_size = len(batch[0])  # Assuming the first element of the batch is the data\n",
    "    print(f\"Batch {i+1} size: {batch_size}\")\n",
    "    if batch_size != 64:\n",
    "        print(f\"Batch {i+1} does not contain 64 records.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "for CLUSTER in range (1, 9):\n",
    "    DEVICE_PERCENTAGE = []\n",
    "    for DEVICE__ in range(0,NUM_CLIENTS):\n",
    "        for i, batch in enumerate(Dataloaders[CLUSTER][DEVICE__]):\n",
    "            _, labels = batch\n",
    "            class_counts = Counter(labels.numpy())\n",
    "            total_records = sum(class_counts.values())\n",
    "            class_0_count = class_counts.get(0, 0)\n",
    "            percentage_class_0 = (class_0_count / total_records) * 100\n",
    "            DEVICE_PERCENTAGE.append(percentage_class_0)\n",
    "            # print(f\"Batch {i+1}: {dict(class_counts)}\")\n",
    "            # print(f\"Percentage of class 0: {percentage_class_0:.2f}%\\n\")\n",
    "    # print(DEVICE_PERCENTAGE)        \n",
    "    chunk_size = 6\n",
    "    averages = [sum(DEVICE_PERCENTAGE[i:i + chunk_size]) / chunk_size for i in range(0, len(DEVICE_PERCENTAGE), chunk_size)]\n",
    "    # print(\"Averages of every device:\")\n",
    "    # print(averages)\n",
    "    chunk_size_4 = 4\n",
    "    averages = [sum(averages[i:i + chunk_size_4]) / chunk_size_4 for i in range(0, len(averages), chunk_size_4)]\n",
    "    # print(\"Averages of every 4 devices:\")\n",
    "    print(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3a112-2124-4571-802b-61850c0be001",
   "metadata": {},
   "outputs": [],
   "source": [
    "del TrafficData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1f66c2-dd6f-458f-93e3-85b93ef1fb50",
   "metadata": {
    "id": "gjVnC-rj9gC4",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='Red'>***Neural Network***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fLD8dUBg9pyY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1670813208940,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "fLD8dUBg9pyY",
    "outputId": "6df101c4-37c9-4660-f5c7-3a42be8b8951"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()        \n",
    "        self.layer_1 = nn.Linear(98, 64)\n",
    "        self.layer_2 = nn.Linear(64, 32)\n",
    "        self.layer_out = nn.Linear(32, 15) \n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_out(x)        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc624e0-c16a-413d-bb70-c7fc7743def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random  = Net()\n",
    "# for param_tensor in Random.state_dict():\n",
    "#     print(param_tensor, \"\\t\", Random.state_dict()[param_tensor].size())\n",
    "# torch.save(Random.state_dict(), \"0_Input_Random_model_Net.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SKRdGrET9pvn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1670813234860,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "SKRdGrET9pvn",
    "outputId": "fe9c8747-dad6-4606-f067-021d127d259c"
   },
   "outputs": [],
   "source": [
    "def train(net, trainloader, epochs: int, verbose=True):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    net.train()\n",
    "    prediction_matrix = []\n",
    "    actual_matrix = []\n",
    "    acc_matrix = []\n",
    "    loss_matrix = []\n",
    "    # Safeguard: if the trainloader has no samples, skip training\n",
    "    try:\n",
    "        dataset_len = len(trainloader.dataset)\n",
    "    except Exception:\n",
    "        dataset_len = 0\n",
    "    if dataset_len == 0:\n",
    "        if verbose:\n",
    "            print(\"Warning: trainloader has 0 samples, skipping training.\")\n",
    "        return prediction_matrix, actual_matrix, acc_matrix, loss_matrix\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Accumulate loss as scalar sum over samples\n",
    "            epoch_loss += loss.item() * labels.size(0)\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "            predictions = torch.max(outputs.data, 1)[1]\n",
    "            prediction_matrix.append(predictions.tolist())\n",
    "            actual_matrix.append(labels.tolist())\n",
    "        # Avoid division by zero if total==0 for safety\n",
    "        if total == 0:\n",
    "            epoch_loss_val = 0.0\n",
    "            epoch_acc = 0.0\n",
    "        else:\n",
    "            epoch_loss_val = epoch_loss / total\n",
    "            epoch_acc = correct / total\n",
    "        loss_matrix.append(epoch_loss_val)\n",
    "        acc_matrix.append(epoch_acc)\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss_val:.6f} | Acc: {epoch_acc:.4f}\")\n",
    "    return prediction_matrix, actual_matrix, acc_matrix, loss_matrix\n",
    "def test(net, testloader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct, total, loss_sum = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    prediction_matrix = []\n",
    "    actual_matrix = []\n",
    "    acc_matrix = []\n",
    "    loss_matrix = []\n",
    "    try:\n",
    "        test_len = len(testloader.dataset)\n",
    "    except Exception:\n",
    "        test_len = 0\n",
    "    if test_len == 0:\n",
    "        print(\"Warning: testloader has 0 samples, skipping evaluation.\")\n",
    "        return 0.0, 0.0, prediction_matrix, actual_matrix, acc_matrix, loss_matrix\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss_sum += criterion(outputs, labels).item() * labels.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            prediction_matrix.append(predicted.tolist())\n",
    "            actual_matrix.append(labels.tolist())\n",
    "    if total == 0:\n",
    "        avg_loss = 0.0\n",
    "        accuracy = 0.0\n",
    "    else:\n",
    "        avg_loss = loss_sum / total\n",
    "        accuracy = correct / total\n",
    "    loss_matrix.append(avg_loss)\n",
    "    acc_matrix.append(accuracy)\n",
    "    print(f\"Evaluation: eval loss {avg_loss}, eval accuracy {accuracy}\")\n",
    "    return avg_loss, accuracy, prediction_matrix, actual_matrix, acc_matrix, loss_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otE9jhmS-IXU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1670813239232,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "otE9jhmS-IXU",
    "outputId": "ec01b70c-2185-4293-87ec-b3eaa8b6d92a"
   },
   "outputs": [],
   "source": [
    "prediction_dict= {}\n",
    "actual_dict= {}\n",
    "accuracy_dict= {}\n",
    "loss_dict= {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab0bf27",
   "metadata": {},
   "source": [
    "<font color='Brown'>***Federated Learning Classes***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8065eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a2f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, FL_Update):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.FL_Update = FL_Update\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "    def fit(self, parameters, config):\n",
    "        local_epochs = config[\"local_epochs\"]\n",
    "        print(f\"[Client {self.cid}, round {self.FL_Update}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        _1, _2, _3, _4 = train(self.net, self.trainloader, epochs=local_epochs)\n",
    "        prediction_dict[f'C{self.cid}R{self.FL_Update}'] = _1\n",
    "        actual_dict[f'C{self.cid}R{self.FL_Update}'] = _2\n",
    "        accuracy_dict[f'C{self.cid}R{self.FL_Update}'] = _3\n",
    "        loss_dict[f'C{self.cid}R{self.FL_Update}'] = _4\n",
    "        # Save model updates (parameters)\n",
    "        # update_filename = f'EdgeCooperation/Performance/Results/C{self.cid}R{self.FL_Update}_update.pkl'\n",
    "        # with open(update_filename, 'wb') as update_outfile:\n",
    "        #     pickle.dump(get_parameters(self.net), update_outfile)        \n",
    "\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}\")\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc50896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_sort_client_updates(global_model, round_number, client_ids):\n",
    "    client_updates = {}\n",
    "    for cid in client_ids:\n",
    "        update_filename = f'EdgeCooperation/Performance/Results/C{cid}R{round_number}_update.pkl'\n",
    "        with open(update_filename, 'rb') as update_file:\n",
    "            client_update = pickle.load(update_file)\n",
    "            client_updates[cid] = client_update\n",
    "    client_contributions = {cid: calculate_weight_magnitude(global_model, update) for cid, update in client_updates.items()}\n",
    "    sorted_clients = sorted(client_contributions.items(), key=lambda x: x[1], reverse=True)\n",
    "    least_contributing_clients = sorted_clients[-3:]\n",
    "    return sorted_clients, least_contributing_clients\n",
    "\n",
    "def calculate_weight_magnitude(global_model, client_update):\n",
    "    \"\"\"\n",
    "    Calculate the L2 norm of the weight difference between the global model and client's updated model.\n",
    "    \n",
    "    Args:\n",
    "    global_model (nn.Module): The global model before client update.\n",
    "    client_update (list): List of numpy arrays representing client's updated model parameters.\n",
    "\n",
    "    Returns:\n",
    "    float: The L2 norm of the weight difference.\n",
    "    \"\"\"\n",
    "    weight_diff = 0.0\n",
    "    global_parameters = [param.detach().cpu().numpy() for param in global_model.parameters()]\n",
    "    for global_param, client_param in zip(global_parameters, client_update):\n",
    "        weight_diff += np.linalg.norm(global_param - client_param)\n",
    "    return weight_diff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec73f424",
   "metadata": {},
   "source": [
    "<font color='Brown'>***Clients Functions***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abaa847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def General_Client():\n",
    "    def client_fn(cid: int, Round: int) -> FlowerClient:\n",
    "        clients_ids_list = TrainingListPerRound[Round]\n",
    "        if int(cid) in clients_ids_list:\n",
    "            net = Net().to(DEVICE)\n",
    "            trainloader = Dataloaders[Round][int(cid)]\n",
    "            arg_ = Round\n",
    "            return FlowerClient(cid, net, trainloader, arg_)\n",
    "        else:\n",
    "            raise ValueError(f\"Client ID {cid} not found in the list for round {Round}\")\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf018325",
   "metadata": {},
   "source": [
    "<font color='Brown'>***FL Strategy***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe1c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "Global_Models = {}\n",
    "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
    "    def __init__(self, additional_argument, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.additional_argument = additional_argument\n",
    "    def aggregate_fit(self, rnd: int, results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]], failures: List[BaseException]) -> Optional[fl.common.NDArrays]:\n",
    "        aggregated_parameters_tuple = super().aggregate_fit(rnd, results, failures)\n",
    "        aggregated_parameters, _ = aggregated_parameters_tuple\n",
    "        if aggregated_parameters is not None:\n",
    "            # print(f\"Saving round {rnd} aggregated_parameters...\")\n",
    "            # Convert `Parameters` to `List[np.ndarray]`\n",
    "            aggregated_weights: List[np.ndarray] = fl.common.parameters_to_ndarrays(aggregated_parameters)\n",
    "            # Convert `List[np.ndarray]` to PyTorch`state_dict`\n",
    "            Global_Models[self.additional_argument] = Net()\n",
    "            params_dict = zip(Global_Models[self.additional_argument].state_dict().keys(), aggregated_weights)\n",
    "            state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "            Global_Models[self.additional_argument].load_state_dict(state_dict, strict=True)\n",
    "            torch.save(Global_Models[self.additional_argument].state_dict(), f\"{PATH}/GlobalModel_{self.additional_argument}.pth\")\n",
    "        return aggregated_parameters_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e639db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_config(server_round: int):\n",
    "    \"\"\"Return training configuration dict for each round.\n",
    "    Perform two rounds of training with one local epoch, increase to two local\n",
    "    epochs afterwards.\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"current_round\": server_round,  # The current round of federated learning\n",
    "        \"local_epochs\": EPOCHS #1 if rnd < 2 else 2,  # \n",
    "    }\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d181247",
   "metadata": {
    "tags": []
   },
   "source": [
    "***Running Centralized DNN Rounds***\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654d0bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Centralized Round-by-Round Training (No FL)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Training / Evaluation\n",
    "# ============================================================\n",
    "\n",
    "def train_one_round(model, trainloader, epochs):\n",
    "    \"\"\"Runs EPOCHS of training on this round's dataset.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, running_loss = 0, 0, 0.0\n",
    "        for Xbatch, ybatch in trainloader:\n",
    "            Xbatch, ybatch = Xbatch.to(DEVICE), ybatch.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(Xbatch)\n",
    "            loss = criterion(outputs, ybatch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            total += ybatch.size(0)\n",
    "            correct += (torch.max(outputs, 1)[1] == ybatch).sum().item()\n",
    "\n",
    "        print(f\"  Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Loss: {running_loss/len(trainloader):.6f} | \"\n",
    "              f\"Acc: {correct/total:.4f}\")\n",
    "\n",
    "\n",
    "def test_model(model, testloader):\n",
    "    \"\"\"Runs a simple test pass (no saving).\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    correct, total, running_loss = 0, 0, 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for Xbatch, ybatch in testloader:\n",
    "            Xbatch, ybatch = Xbatch.to(DEVICE), ybatch.to(DEVICE)\n",
    "            outputs = model(Xbatch)\n",
    "            loss = criterion(outputs, ybatch).item()\n",
    "\n",
    "            running_loss += loss\n",
    "            total += ybatch.size(0)\n",
    "            correct += (torch.max(outputs, 1)[1] == ybatch).sum().item()\n",
    "\n",
    "    print(f\"  Test ‚Üí Loss: {running_loss/len(testloader):.6f}, \"\n",
    "          f\"Acc: {correct/total:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Centralized Training Loop\n",
    "# ============================================================\n",
    "if not FL:\n",
    "    os.makedirs(PATH, exist_ok=True)\n",
    "    print(\"üî• Starting centralized Round-by-Round training...\\n\")\n",
    "\n",
    "    model = Net().to(DEVICE)\n",
    "\n",
    "    for Round in range(1, ROUNDS + 1):\n",
    "\n",
    "        print(\"========================\")\n",
    "        print(f\"   ROUND {Round} START\")\n",
    "        print(\"========================\")\n",
    "\n",
    "        # Build round dataset from client partitions\n",
    "        client_datasets = [\n",
    "            loader.dataset for loader in Dataloaders[Round]\n",
    "            if hasattr(loader, \"dataset\") and len(loader.dataset) > 0\n",
    "        ]\n",
    "\n",
    "        round_dataset = ConcatDataset(client_datasets)\n",
    "        trainloader = DataLoader(round_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "        print(f\"  Train samples this round: {len(round_dataset)}\")\n",
    "\n",
    "        # Train\n",
    "        train_one_round(model, trainloader, EPOCHS)\n",
    "\n",
    "        # Evaluate (optional)\n",
    "        print(f\"  Evaluating round {Round} model...\")\n",
    "        test_model(model, Dataloaders[\"Test\"])\n",
    "\n",
    "        # Save model\n",
    "        save_path = f\"{PATH}/GlobalModel_{Round}.pth\"\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"  üíæ Saved: {save_path}\")\n",
    "\n",
    "        print(\"========================\")\n",
    "        print(f\"   ROUND {Round} DONE\")\n",
    "        print(\"========================\\n\")\n",
    "\n",
    "    print(\"üèÅ Centralized Round-by-Round training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c60146",
   "metadata": {},
   "source": [
    "***Running the Generalized FL DNN Round***\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f7e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FL:\n",
    "    os.makedirs(PATH, exist_ok=True)\n",
    "    print(\"Loading Initial Global Model\")\n",
    "    Global_Models[0] = Net()\n",
    "    Global_Models[0].load_state_dict(torch.load(\"0_Input_Random_model_Net.pth\"))\n",
    "    Global_Models[0].train()\n",
    "\n",
    "    TrainingListPerRound = {}\n",
    "    for Round in range(1, ROUNDS+1):\n",
    "        TrainingListPerRound[Round] = []     \n",
    "        for CLIENT in range (NUM_CLIENTS):\n",
    "            TrainingListPerRound[Round].append(int(CLIENT))\n",
    "\n",
    "    for Round in range(1, ROUNDS+1):\n",
    "        print(\"Starting FL Round: \", Round)\n",
    "        strategy = SaveModelStrategy(\n",
    "                fraction_fit=1.0,  # Sample 100% of available clients for training\n",
    "                fraction_evaluate=0,  # Sample 50% of available clients for evaluation\n",
    "                min_fit_clients=2,  # Never sample less than 10 clients for training\n",
    "                min_evaluate_clients=0,  # Never sample less than 5 clients for evaluation\n",
    "                min_available_clients=2,  # Wait until all 10 clients are available\n",
    "                on_fit_config_fn=fit_config,\n",
    "                initial_parameters=fl.common.ndarrays_to_parameters(get_parameters(Global_Models[Round-1])),\n",
    "                additional_argument = Round\n",
    "        )\n",
    "        # print(f'Current training nodes at round {Round}: ', len(TrainingListPerRound[Round]), ' ', TrainingListPerRound[Round])\n",
    "        client_fn = General_Client()\n",
    "        fl.simulation.start_simulation(\n",
    "            client_fn=lambda cid: client_fn(cid, int(Round)),\n",
    "            num_clients=int(NUM_CLIENTS),\n",
    "            config=fl.server.ServerConfig(num_rounds=int(1)),\n",
    "            client_resources={\"num_cpus\":16, \"num_gpus\":1}, \n",
    "            ray_init_args = {'num_cpus': 16, 'num_gpus': 1},\n",
    "            strategy=strategy\n",
    "        )\n",
    "        print(\"End of FL Round: \", Round)\n",
    "        print(\"Loading Global Model: \", Round)\n",
    "        Global_Models[Round] = Net()\n",
    "        Global_Models[Round].load_state_dict(torch.load(f\"{PATH}/GlobalModel_{Round}.pth\"))\n",
    "        Global_Models[Round].train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555eb63-e297-4c7b-8e1b-5985d2cde0e9",
   "metadata": {
    "id": "l0PgqJTEwwWk",
    "tags": []
   },
   "source": [
    "<font color='Grey'>***Performance Testing***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f39255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the directory and file pattern\n",
    "directory = PATH + '/'\n",
    "pattern = \"GlobalModel_*.pth\"\n",
    "\n",
    "# Find all matching files\n",
    "files = glob.glob(os.path.join(directory, pattern))\n",
    "\n",
    "# Extract numbers from file names\n",
    "numbers = []\n",
    "for file in files:\n",
    "    base_name = os.path.basename(file)\n",
    "    num_str = base_name.replace(\"GlobalModel_\", \"\").replace(\".pth\", \"\")\n",
    "    try:\n",
    "        numbers.append(int(num_str))\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "# Determine the maximum number\n",
    "max_num = max(numbers) if numbers else 0\n",
    "print(max_num)\n",
    "\n",
    "# Use the max_num in a loop\n",
    "for num in range(1, max_num + 1):\n",
    "    file_path = f\"{PATH}/GlobalModel_{num}.pth\"\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the file or perform any operation you need\n",
    "        print(f\"Loading {file_path}\")\n",
    "    else:\n",
    "        print(f\"File {file_path} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ec610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_pickle(path, G=None, suffix=None):\n",
    "    print(G)\n",
    "    if G is not None:\n",
    "        rounds = [G]\n",
    "    else:\n",
    "        # Auto-detect how many Global_X files exist\n",
    "        rounds = sorted({\n",
    "            int(f.split(\"_\")[1])\n",
    "            for f in os.listdir(path)\n",
    "            if f.startswith(\"Global_\") and f.split(\"_\")[1].isdigit()\n",
    "        })\n",
    "\n",
    "    suffixes = [suffix] if suffix else [\"pred\", \"actual\", \"accurracy\", \"loss\"]\n",
    "\n",
    "    for g in rounds:\n",
    "        for s in suffixes:\n",
    "            filename = os.path.join(path, f\"Global_{g}_{s}\")\n",
    "            if not os.path.exists(filename):\n",
    "                print(f\"Missing file: {filename}\")\n",
    "                continue\n",
    "            try:\n",
    "                with open(filename, \"rb\") as f:\n",
    "                    data = pickle.load(f)\n",
    "                print(f\"File: {filename}\")\n",
    "                if isinstance(data, list) and len(data) > 2:\n",
    "                    pprint(data[:2])\n",
    "                    print(f\"... ({len(data)} total items)\")\n",
    "                else:\n",
    "                    pprint(data)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error reading {filename}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q7O6Uj-at0L7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 364,
     "status": "error",
     "timestamp": 1670839612569,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "Q7O6Uj-at0L7",
    "outputId": "e4d532e2-d5a1-4a0c-b486-f5975af6cd0e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_test = {}\n",
    "actual_test = {}\n",
    "accuracy_test = {}\n",
    "loss_test = {}\n",
    "G = 0\n",
    "\n",
    "for num in range(1, max_num+1):\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load(f\"{PATH}/GlobalModel_{num}.pth\"))\n",
    "    model.eval()\n",
    "    \n",
    "    prediction_matrix = []\n",
    "    actual_matrix= []\n",
    "    acc_matrix = []\n",
    "    loss_matrix=[]\n",
    "    G = G + 1\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in Dataloaders['Test']:\n",
    "            outputs = model(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            prediction_matrix.append(predicted.tolist())\n",
    "            actual_matrix.append(labels.tolist())\n",
    "    loss /= len(Dataloaders['Test'].dataset)\n",
    "    accuracy = correct / total\n",
    "    loss_matrix.append(loss)\n",
    "    acc_matrix.append(accuracy) \n",
    "\n",
    "    pred_test[f'Global_{G}'] = prediction_matrix\n",
    "    actual_test[f'Global_{G}'] = actual_matrix\n",
    "    accuracy_test[f'Global_{G}'] = acc_matrix\n",
    "    loss_test[f'Global_{G}'] = loss_matrix \n",
    "\n",
    "    filename = f'{PATH}/Global_{G}_pred'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(pred_test[f'Global_{G}'],outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    filename = f'{PATH}/Global_{G}_actual'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(actual_test[f'Global_{G}'],outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    filename = f'{PATH}/Global_{G}_accurracy'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(accuracy_test[f'Global_{G}'],outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    filename = f'{PATH}/Global_{G}_loss'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(loss_test[f'Global_{G}'],outfile)\n",
    "    outfile.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "bcoA2oGB6Kyi",
    "-hiMkZhLVT63",
    "Fg6LBRPK-E6_",
    "gjVnC-rj9gC4",
    "KNYdybJu5uhn",
    "LwpshLHduBDL",
    "Df92buFDvAl8",
    "D8dfI-OyvOBt",
    "s02Uob4Bvebm",
    "3H-0K0GXvuQo",
    "Y3LLG0dvwAFl",
    "thyLEKcswNst",
    "scNSl39rwhZu"
   ],
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

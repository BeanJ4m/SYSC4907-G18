{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a01474e1",
   "metadata": {},
   "source": [
    "<font color='White'>***Libraries and Constants***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad3ba4d7-0489-464a-a4a3-4fa9196b1b7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34e1429d-c81f-4d1f-9322-c97f4e526a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "ROUNDS = 40 \n",
    "SPLITS = 1\n",
    "CLASS_COUNT = 5\n",
    "# Define a variable for the base path to use for all use cases\n",
    "PATH = \"FL:TrueDNN48-clients-11atk-40-rounds-3-epochs-0.0025-lr-500-groups\"  # Change this to your desired path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce9748",
   "metadata": {},
   "source": [
    "<font color='Green'>***Metrics and Functions***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf430e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AUTO-DETECT CLASSCOUNT METRIC FUNCTIONS (DROP-IN SAFE)\n",
    "# ============================================================\n",
    "\n",
    "def calculate_accuracy(actual_values, predicted_values):\n",
    "    \"\"\"Fraction of correct predictions.\"\"\"\n",
    "    if len(actual_values) == 0:\n",
    "        return 0.0\n",
    "    return sum(a == p for a, p in zip(actual_values, predicted_values)) / len(actual_values)\n",
    "\n",
    "\n",
    "def _build_label_index(actual_values, predicted_values):\n",
    "    \"\"\"\n",
    "    Auto-detect all class labels and map them to 0..K-1.\n",
    "    This avoids IndexErrors and handles any label format.\n",
    "    \"\"\"\n",
    "    unique_labels = sorted(set(actual_values) | set(predicted_values))\n",
    "    label_to_idx = {lab: i for i, lab in enumerate(unique_labels)}\n",
    "    return unique_labels, label_to_idx\n",
    "\n",
    "\n",
    "def calculate_weighted_precision(actual_values, predicted_values):\n",
    "    if len(actual_values) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    labels, idx = _build_label_index(actual_values, predicted_values)\n",
    "    K = len(labels)\n",
    "\n",
    "    tp = [0] * K\n",
    "    fp = [0] * K\n",
    "    actual_count = [0] * K\n",
    "\n",
    "    for a, p in zip(actual_values, predicted_values):\n",
    "        ai = idx[a]\n",
    "        pi = idx[p]\n",
    "        actual_count[ai] += 1\n",
    "        if a == p:\n",
    "            tp[ai] += 1\n",
    "        else:\n",
    "            fp[pi] += 1\n",
    "\n",
    "    total = len(actual_values)\n",
    "    precision_sum = 0.0\n",
    "\n",
    "    for i in range(K):\n",
    "        denom = tp[i] + fp[i]\n",
    "        precision_i = tp[i] / denom if denom > 0 else 0.0\n",
    "        weight_i = actual_count[i] / total\n",
    "        precision_sum += precision_i * weight_i\n",
    "\n",
    "    return precision_sum\n",
    "\n",
    "\n",
    "def calculate_weighted_recall(actual_values, predicted_values):\n",
    "    if len(actual_values) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    labels, idx = _build_label_index(actual_values, predicted_values)\n",
    "    K = len(labels)\n",
    "\n",
    "    tp = [0] * K\n",
    "    fn = [0] * K\n",
    "    actual_count = [0] * K\n",
    "\n",
    "    for a, p in zip(actual_values, predicted_values):\n",
    "        ai = idx[a]\n",
    "        actual_count[ai] += 1\n",
    "        if a == p:\n",
    "            tp[ai] += 1\n",
    "        else:\n",
    "            fn[ai] += 1\n",
    "\n",
    "    total = len(actual_values)\n",
    "    recall_sum = 0.0\n",
    "\n",
    "    for i in range(K):\n",
    "        denom = tp[i] + fn[i]\n",
    "        recall_i = tp[i] / denom if denom > 0 else 0.0\n",
    "        weight_i = actual_count[i] / total\n",
    "        recall_sum += recall_i * weight_i\n",
    "\n",
    "    return recall_sum\n",
    "\n",
    "\n",
    "def calculate_weighted_f1(actual_values, predicted_values):\n",
    "    if len(actual_values) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    labels, idx = _build_label_index(actual_values, predicted_values)\n",
    "    K = len(labels)\n",
    "\n",
    "    tp = [0] * K\n",
    "    fp = [0] * K\n",
    "    fn = [0] * K\n",
    "\n",
    "    for a, p in zip(actual_values, predicted_values):\n",
    "        ai = idx[a]\n",
    "        pi = idx[p]\n",
    "        if a == p:\n",
    "            tp[ai] += 1\n",
    "        else:\n",
    "            fp[pi] += 1\n",
    "            fn[ai] += 1\n",
    "\n",
    "    total = len(actual_values)\n",
    "    f1_sum = 0.0\n",
    "\n",
    "    for i in range(K):\n",
    "        prec_i = tp[i] / (tp[i] + fp[i]) if (tp[i] + fp[i]) > 0 else 0.0\n",
    "        rec_i  = tp[i] / (tp[i] + fn[i]) if (tp[i] + fn[i]) > 0 else 0.0\n",
    "\n",
    "        f1_i = (2 * prec_i * rec_i / (prec_i + rec_i)) if (prec_i + rec_i) > 0 else 0.0\n",
    "\n",
    "        weight_i = (tp[i] + fn[i]) / total\n",
    "        f1_sum += f1_i * weight_i\n",
    "\n",
    "    return f1_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea905ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_values(values_list):\n",
    "    return [value if value >= 0.5 else 0.5 for value in values_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d25dd4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list(values_list):\n",
    "    while len(values_list) < ROUNDS:\n",
    "        values_list.append(values_list[-1])\n",
    "    return values_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3db4a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_report(actual, predicted, class_labels=None):\n",
    "    if class_labels is None:\n",
    "        class_labels = sorted(set(actual) | set(predicted))\n",
    "    report = classification_report(actual, predicted, target_names=[str(label) for label in class_labels])\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "965c2014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_classification_report(actual, predicted, file_path, class_labels=None):\n",
    "    if class_labels is None:\n",
    "        class_labels = sorted(set(actual) | set(predicted))\n",
    "    report = classification_report(actual, predicted, target_names=[str(label) for label in class_labels])\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(\"Classification Report:\\n\\n\")\n",
    "        file.write(report)\n",
    "    print(f\"Classification report saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb979e",
   "metadata": {},
   "source": [
    "<font color='Orange'>***Confusion Matrix***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b368f177",
   "metadata": {},
   "outputs": [],
   "source": [
    "Label_names = ['Normal', 'DDoS_UDP', 'DDoS_ICMP', 'DDoS_TCP', 'DDoS_HTTP', 'Password', 'Vulnerability_scanner', 'SQL_injection']#, 'Uploading', 'Backdoor', 'Port_Scanning', 'XSS', 'Ransomware', 'MITM', 'OS_Fingerprinting']\n",
    "Label_numbers = [0, 1, 2, 3, 4, 5, 6, 7]#, 8, 9, 10, 11, 12, 13, 14]\n",
    "\n",
    "def plot_confusion_matrix_with_names(actual, predicted, label_numbers, label_names, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix with label names instead of numbers.\n",
    "\n",
    "    Args:\n",
    "        actual (list): The list of actual class labels.\n",
    "        predicted (list): The list of predicted class labels.\n",
    "        label_numbers (list): The list of numeric label identifiers.\n",
    "        label_names (list): The corresponding list of label names.\n",
    "        title (str): The title of the confusion matrix plot.\n",
    "    \"\"\"\n",
    "    # Map numeric labels to their names\n",
    "    label_map = dict(zip(label_numbers, label_names))\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(actual, predicted, labels=label_numbers)\n",
    "    \n",
    "    # Replace numeric labels with names for the axes\n",
    "    class_labels = [label_map[num] for num in label_numbers]\n",
    "    \n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='magma', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    \n",
    "    # Add labels, title, and a color bar\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"Actual Labels\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58d9e6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_confusion_matrix_with_names(Data[\"Perfect Post-Detection\"][\"Actual\"][160], Data[\"Perfect Post-Detection\"][\"Predictions\"][160], \n",
    "#                                  Label_numbers, Label_names, title=\"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc67a4",
   "metadata": {},
   "source": [
    "<font color='Red'>***Calculating and Saving Results***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af40fcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Centralized': {'Actual': {},\n",
      "                 'Path': 'DNNCent5atk_40_rounds_48_clients_3_epochs_64_batch_0.0025_lr_40_data_groups',\n",
      "                 'Predictions': {}}}\n"
     ]
    }
   ],
   "source": [
    "Data = {}\n",
    "Data[\"Centralized\"] = {}\n",
    "Data[\"Centralized\"][\"Path\"] = \"DNNCent5atk_40_rounds_48_clients_3_epochs_64_batch_0.0025_lr_40_data_groups\"  # Your centralized model output directory\n",
    "Data[\"Centralized\"][\"Actual\"] = {}\n",
    "Data[\"Centralized\"][\"Predictions\"] = {}\n",
    "# Data[\"Perfect Post-Detection\"] = {}\n",
    "# Data[\"Confidence Thresholding-Round 4\"] = {}\n",
    "# Data[\"Outlier Removing-Round 4\"] = {}\n",
    "# Data[\"No Post-Detection-Round 4\"] = {}\n",
    "# Data[\"Confidence Thresholding-Round 10\"] = {}\n",
    "# Data[\"Outlier Removing-Round 10\"] = {}\n",
    "# Data[\"No Post-Detection-Round 10\"] = {}\n",
    "# Data[\"Perfect Post-Detection\"]['Path'] = \"Ideal_20250206\"\n",
    "# Data[\"Confidence Thresholding-Round 4\"]['Path'] = \"Confidence_20250214_4\"\n",
    "# Data[\"Outlier Removing-Round 4\"]['Path'] = \"Outlier_20250214_4\"\n",
    "# Data[\"No Post-Detection-Round 4\"]['Path'] = \"NoPD_20250214_4\"\n",
    "# Data[\"Confidence Thresholding-Round 10\"]['Path'] = \"Confidence_20250214_10\"\n",
    "# Data[\"Outlier Removing-Round 10\"]['Path'] = \"Outlier_20250214_10\"\n",
    "# Data[\"No Post-Detection-Round 10\"]['Path'] = \"NoPD_20250214_10\"\n",
    "for Usecase in Data:\n",
    "    Data[Usecase]['Actual'] = {}\n",
    "    Data[Usecase]['Predictions'] = {}\n",
    "pprint.pprint(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a07de356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for Usecase in Data:\n",
    "    for i in range(1, ROUNDS*SPLITS+1):\n",
    "        try:\n",
    "            filename = f\"{Data[Usecase]['Path']}/Global_{i}_actual\"\n",
    "            with open(filename, 'rb') as file:\n",
    "                Actual = pickle.load(file)\n",
    "            Data[Usecase]['Actual'][i] = [item for sublist in Actual for item in sublist]\n",
    "            filename = f\"{Data[Usecase]['Path']}/Global_{i}_pred\"\n",
    "            with open(filename, 'rb') as file:\n",
    "                Pred = pickle.load(file)\n",
    "            Data[Usecase]['Predictions'][i] = [item for sublist in Pred for item in sublist]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing file {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "440714d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# value_counts = Counter(Data['Confidence Thresholding']['Predictions'][160])\n",
    "# for value, count in value_counts.items():\n",
    "#     print(f\"Value: {value}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9f8459b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Centralized': {'Accuracy': [], 'F1_Score': [], 'Precision': [], 'Recall': []}}\n"
     ]
    }
   ],
   "source": [
    "Results = {}\n",
    "for Usecase in Data:\n",
    "    Results[Usecase] = {}\n",
    "for Usecase in Results:\n",
    "    Results[Usecase]['Accuracy'] = []\n",
    "    Results[Usecase]['Recall'] = []\n",
    "    Results[Usecase]['Precision'] = [] \n",
    "    Results[Usecase]['F1_Score'] = []\n",
    "pprint.pprint(Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a094c2be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for Usecase in Results:\n",
    "    for Round in range(1, ROUNDS*SPLITS+1):\n",
    "        try:\n",
    "            Results[Usecase]['Accuracy'].append(calculate_accuracy(Data[Usecase]['Actual'][Round], Data[Usecase]['Predictions'][Round]))\n",
    "            Results[Usecase]['Precision'].append(calculate_weighted_precision(Data[Usecase]['Actual'][Round], Data[Usecase]['Predictions'][Round]))\n",
    "            Results[Usecase]['Recall'].append(calculate_weighted_recall(Data[Usecase]['Actual'][Round], Data[Usecase]['Predictions'][Round]))\n",
    "            Results[Usecase]['F1_Score'].append(calculate_weighted_f1(Data[Usecase]['Actual'][Round], Data[Usecase]['Predictions'][Round]))\n",
    "        except KeyError:\n",
    "            print('Usecase:', Usecase, 'Round:', Round)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b32df4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Usecase in Results:\n",
    "#     Results[Usecase]['Accuracy'] = update_values(Results[Usecase]['Accuracy'])\n",
    "#     Results[Usecase]['Recall'] = update_values(Results[Usecase]['Recall'])\n",
    "#     Results[Usecase]['Precision'] = update_values(Results[Usecase]['Precision'])    \n",
    "#     Results[Usecase]['F1_Score'] = update_values(Results[Usecase]['F1_Score'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f56712c",
   "metadata": {},
   "source": [
    "<font color='Light Blue'>***Save/ Load Calculated Results***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a866d11-6b4a-4bec-9630-d3454ef24489",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{PATH}/PD_Results.pkl\", \"wb\") as file:\n",
    "    pickle.dump(Results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8befbcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"PD_Results.pkl\", \"rb\") as file:\n",
    "#     Results = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c9984",
   "metadata": {},
   "source": [
    "<font color='Green'>***Plotting Results***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fd732d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 6400x4800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "LineStyle = ['ro-', 'b*-', 'ks-', 'gh-', 'm<-', 'yp-', 'b*-', 'gh-', 'rH-', 'c+-', 'mx-', 'ro-', 'b*-', 'ks-', 'gh-', 'y<-']\n",
    "fig = plt.figure(figsize=(17.2, 13), dpi=450)\n",
    "axx = fig.add_subplot(1,1,1)\n",
    "plt.figure(dpi=1000)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "\n",
    "x_axis = np.array(np.arange(1, ROUNDS+1, 1).tolist())\n",
    "x_axis = np.insert(x_axis, 0, 0)\n",
    "y_points = {}\n",
    "for Usecase in Data:\n",
    "    y_points[Usecase] = np.array(Results[Usecase]['Accuracy'])\n",
    "    y_points[Usecase] = np.insert(y_points[Usecase], 0, 0.5)\n",
    "index = 0\n",
    "for Usecase in Data:\n",
    "    axx.plot(x_axis, y_points[Usecase],LineStyle[index], label = Usecase, linewidth=2.5,  markersize=10)\n",
    "    index += 1\n",
    "    \n",
    "axx.set_xlabel(f'{PATH}', fontdict={'fontsize': 36})\n",
    "axx.set_ylabel('Detection Accuracy', fontdict={'fontsize': 36})\n",
    "axx.set_xticks(np.arange(0, ROUNDS+1, 5).tolist()) \n",
    "axx.set_yticks(np.arange(0.30, 1.05, 0.05).tolist())\n",
    "axx.legend(loc = 'lower right', prop={'size': 16})\n",
    "axx.tick_params(axis='x', which='both', bottom=True, top=True, labelbottom=True, labeltop=True, labelsize=20, colors='black')\n",
    "axx.xaxis.set_ticks_position('both')\n",
    "axx.tick_params(axis='y', which='both', left=True, right=True, labelleft=True, labelright=True, labelsize=20, colors='black')\n",
    "axx.yaxis.set_ticks_position('both')\n",
    "axx.xaxis.label.set_color('black')\n",
    "axx.yaxis.label.set_color('black')\n",
    "fig.savefig(f\"{PATH}/1_Accuracy.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41f13e63-48da-4c9b-9456-7b32b5c3a430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 6400x4800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "LineStyle = ['ro-', 'b*-', 'ks-', 'gh-', 'm<-', 'yp-', 'b*-', 'gh-', 'rH-', 'c+-', 'mx-', 'ro-', 'b*-', 'ks-', 'gh-', 'y<-']\n",
    "fig = plt.figure(figsize=(17.2, 13), dpi=450)\n",
    "axx = fig.add_subplot(1,1,1)\n",
    "plt.figure(dpi=1000)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "\n",
    "x_axis = np.array(np.arange(1, ROUNDS+1, 1).tolist())\n",
    "x_axis = np.insert(x_axis, 0, 0)\n",
    "y_points = {}\n",
    "for Usecase in Data:\n",
    "    y_points[Usecase] = np.array(Results[Usecase]['Recall'])\n",
    "    y_points[Usecase] = np.insert(y_points[Usecase], 0, 0.5)\n",
    "\n",
    "index = 0\n",
    "for Usecase in Data:\n",
    "    axx.plot(x_axis, y_points[Usecase],LineStyle[index], label = Usecase, linewidth=2.5,  markersize=10)\n",
    "    index += 1\n",
    "    \n",
    "axx.set_xlabel(f'{PATH}', fontdict={'fontsize': 36})\n",
    "axx.set_ylabel('Detection Recall', fontdict={'fontsize': 36})\n",
    "axx.set_xticks(np.arange(0, ROUNDS+1, 5).tolist()) \n",
    "axx.set_yticks(np.arange(0.30, 1.05, 0.05).tolist())\n",
    "axx.legend(loc = 'lower right', prop={'size': 16})\n",
    "axx.tick_params(axis='x', which='both', bottom=True, top=True, labelbottom=True, labeltop=True, labelsize=20, colors='black')\n",
    "axx.xaxis.set_ticks_position('both')\n",
    "axx.tick_params(axis='y', which='both', left=True, right=True, labelleft=True, labelright=True, labelsize=20, colors='black')\n",
    "axx.yaxis.set_ticks_position('both')\n",
    "axx.xaxis.label.set_color('black')\n",
    "axx.yaxis.label.set_color('black')\n",
    "fig.savefig(f\"{PATH}/1_Wieghted_Recall.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9813c1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 6400x4800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "LineStyle = ['ro-', 'b*-', 'ks-', 'gh-', 'm<-', 'yp-', 'b*-', 'gh-', 'rH-', 'c+-', 'mx-', 'ro-', 'b*-', 'ks-', 'gh-', 'y<-']\n",
    "fig = plt.figure(figsize=(17.2, 13), dpi=450)\n",
    "axx = fig.add_subplot(1,1,1)\n",
    "plt.figure(dpi=1000)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "\n",
    "x_axis = np.array(np.arange(1, ROUNDS+1, 1).tolist())\n",
    "x_axis = np.insert(x_axis, 0, 0)\n",
    "y_points = {}\n",
    "for Usecase in Data:\n",
    "    y_points[Usecase] = np.array(Results[Usecase]['Precision'])\n",
    "    y_points[Usecase] = np.insert(y_points[Usecase], 0, 0.5)\n",
    "\n",
    "index = 0\n",
    "for Usecase in Data:\n",
    "    axx.plot(x_axis, y_points[Usecase],LineStyle[index], label = Usecase, linewidth=2.5,  markersize=10)\n",
    "    index += 1\n",
    "    \n",
    "axx.set_xlabel(f'{PATH}', fontdict={'fontsize': 36})\n",
    "axx.set_ylabel('Detection Precision', fontdict={'fontsize': 36})\n",
    "axx.set_xticks(np.arange(0, ROUNDS+1, 5).tolist()) \n",
    "axx.set_yticks(np.arange(0.30, 1.05, 0.05).tolist())\n",
    "axx.legend(loc = 'lower right', prop={'size': 16})\n",
    "axx.tick_params(axis='x', which='both', bottom=True, top=True, labelbottom=True, labeltop=True, labelsize=20, colors='black')\n",
    "axx.xaxis.set_ticks_position('both')\n",
    "axx.tick_params(axis='y', which='both', left=True, right=True, labelleft=True, labelright=True, labelsize=20, colors='black')\n",
    "axx.yaxis.set_ticks_position('both')\n",
    "axx.xaxis.label.set_color('black')\n",
    "axx.yaxis.label.set_color('black')\n",
    "fig.savefig(f\"{PATH}/1_Wieghted_Precision.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a5eaf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 6400x4800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "LineStyle = ['ro-', 'b*-', 'ks-', 'gh-', 'm<-', 'yp-', 'b*-', 'gh-', 'rH-', 'c+-', 'mx-', 'ro-', 'b*-', 'ks-', 'gh-', 'y<-']\n",
    "fig = plt.figure(figsize=(17.2, 13), dpi=450)\n",
    "axx = fig.add_subplot(1,1,1)\n",
    "plt.figure(dpi=1000)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "\n",
    "x_axis = np.array(np.arange(1, ROUNDS+1, 1).tolist())\n",
    "x_axis = np.insert(x_axis, 0, 0)\n",
    "y_points = {}\n",
    "for Usecase in Data:\n",
    "    y_points[Usecase] = np.array(Results[Usecase]['F1_Score'])\n",
    "    y_points[Usecase] = np.insert(y_points[Usecase], 0, 0.5)\n",
    "\n",
    "index = 0\n",
    "for Usecase in Data:\n",
    "    axx.plot(x_axis, y_points[Usecase],LineStyle[index], label = Usecase, linewidth=2.5,  markersize=10)\n",
    "    index += 1\n",
    "    \n",
    "axx.set_xlabel(f'{PATH}', fontdict={'fontsize': 36})\n",
    "axx.set_ylabel('Detection F1_Score', fontdict={'fontsize': 36})\n",
    "axx.set_xticks(np.arange(0, ROUNDS+1, 5).tolist()) \n",
    "axx.set_yticks(np.arange(0.30, 1.05, 0.05).tolist())\n",
    "axx.legend(loc = 'lower right', prop={'size': 16})\n",
    "axx.tick_params(axis='x', which='both', bottom=True, top=True, labelbottom=True, labeltop=True, labelsize=20, colors='black')\n",
    "axx.xaxis.set_ticks_position('both')\n",
    "axx.tick_params(axis='y', which='both', left=True, right=True, labelleft=True, labelright=True, labelsize=20, colors='black')\n",
    "axx.yaxis.set_ticks_position('both')\n",
    "axx.xaxis.label.set_color('black')\n",
    "axx.yaxis.label.set_color('black')\n",
    "fig.savefig(f\"{PATH}/1_Wieghted_F1_Score.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44bb9f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] 1 vs 3 Epochs_Accuracy.pdf\n",
      "[Saved] 1 vs 3 Epochs_Recall.pdf\n",
      "[Saved] 1 vs 3 Epochs_Precision.pdf\n",
      "[Saved] 1 vs 3 Epochs_F1.pdf\n"
     ]
    }
   ],
   "source": [
    "# Multi plot merger\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_results_from_path(path):\n",
    "    \"\"\"Load Results dictionary from PATH/PD_Results.pkl\"\"\"\n",
    "    with open(f\"{path}/PD_Results.pkl\", \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_merged_metric(paths, metric_name, labels=None, output=None):\n",
    "    \"\"\"\n",
    "    Merge metric graphs from multiple experiment folders into one graph.\n",
    "\n",
    "    paths      : list of folder paths (each folder must contain PD_Results.pkl)\n",
    "    metric_name: \"Accuracy\", \"Recall\", \"Precision\", \"F1_Score\"\n",
    "    labels     : legend names (optional)\n",
    "    output     : file output name (.pdf). If None â†’ auto-generate name.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.style.use('ggplot')\n",
    "    fig = plt.figure(figsize=(17.2, 13), dpi=450)\n",
    "    axx = fig.add_subplot(1,1,1)\n",
    "\n",
    "    LineStyle = ['ro-', 'b*-', 'ks-', 'gh-', 'm<-', 'yp-', 'c+-', 'mx-'] * 4\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [f\"Run {i+1}\" for i in range(len(paths))]\n",
    "\n",
    "    for idx, path in enumerate(paths):\n",
    "        results = load_results_from_path(path)\n",
    "        usecase = next(iter(results.keys()))  \n",
    "\n",
    "        y = np.array(results[usecase][metric_name])\n",
    "        y = np.insert(y, 0, 0.5)            \n",
    "        x = np.arange(len(y))\n",
    "\n",
    "        axx.plot(\n",
    "            x, y,\n",
    "            LineStyle[idx],\n",
    "            label=labels[idx],\n",
    "            linewidth=2.5,\n",
    "            markersize=10\n",
    "        )\n",
    "\n",
    "    axx.set_xlabel(\"Rounds\", fontdict={'fontsize': 36})\n",
    "    axx.set_ylabel(f\"Detection {metric_name}\", fontdict={'fontsize': 36})\n",
    "    axx.set_xticks(np.arange(0, len(y), 5))\n",
    "    axx.set_yticks(np.arange(0.30, 1.05, 0.05))\n",
    "    axx.legend(loc='lower right', prop={'size': 16})\n",
    "\n",
    "    axx.tick_params(axis='x', which='both', bottom=True, top=True,\n",
    "                    labelbottom=True, labeltop=True, labelsize=20, colors='black')\n",
    "    axx.xaxis.set_ticks_position('both')\n",
    "    axx.tick_params(axis='y', which='both', left=True, right=True,\n",
    "                    labelleft=True, labelright=True, labelsize=20, colors='black')\n",
    "    axx.yaxis.set_ticks_position('both')\n",
    "    axx.xaxis.label.set_color('black')\n",
    "    axx.yaxis.label.set_color('black')\n",
    "\n",
    "    if output is None:\n",
    "        output = f\"Merged_{metric_name}.pdf\"\n",
    "\n",
    "    fig.savefig(output, format=\"pdf\", bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"[Saved] {output}\")\n",
    "\n",
    "\n",
    "def merge_all_metrics(paths, labels=None, out_prefix=\"Merged\"):\n",
    "    \"\"\"Generate Accuracy, Recall, Precision, F1 merged graphs for all paths.\"\"\"\n",
    "    plot_merged_metric(paths, \"Accuracy\",    labels, f\"{out_prefix}_Accuracy.pdf\")\n",
    "    plot_merged_metric(paths, \"Recall\",      labels, f\"{out_prefix}_Recall.pdf\")\n",
    "    plot_merged_metric(paths, \"Precision\",   labels, f\"{out_prefix}_Precision.pdf\")\n",
    "    plot_merged_metric(paths, \"F1_Score\",    labels, f\"{out_prefix}_F1.pdf\")\n",
    "\n",
    "################################################\n",
    "################################################\n",
    "# Change paths and out_prefix as needed.\n",
    "# Paths can contain any number of paths\n",
    "paths = [\n",
    "    \"DNNCent5atk-40-rounds-1-epochs-0.0015-lr-40-batch-2560-size\",\n",
    "    \"DNNCent5atk-40-rounds-3-epochs-0.0015-lr-40-batch-2560-size\"\n",
    "]\n",
    "labels = [\n",
    "    \"1 Epoch\",\n",
    "    \"3 Epochs\"\n",
    "]\n",
    "merge_all_metrics(paths, labels, out_prefix=\"1 vs 3 Epochs\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

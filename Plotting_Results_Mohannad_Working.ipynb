{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a01474e1",
   "metadata": {},
   "source": [
    "<font color='White'>***Libraries and Constants***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad3ba4d7-0489-464a-a4a3-4fa9196b1b7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34e1429d-c81f-4d1f-9322-c97f4e526a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "ROUNDS = 40 \n",
    "SPLITS = 1\n",
    "# Define a variable for the base path to use for all use cases\n",
    "PATH = \"DNNCent5atk_40_rounds_3_epochs_0.0015_lr\"  # Change this to your desired path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce9748",
   "metadata": {},
   "source": [
    "<font color='Green'>***Metrics and Functions***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf430e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AUTO-DETECT CLASSCOUNT METRIC FUNCTIONS (DROP-IN SAFE)\n",
    "# ============================================================\n",
    "\n",
    "def calculate_accuracy(actual_values, predicted_values):\n",
    "    \"\"\"Fraction of correct predictions.\"\"\"\n",
    "    if len(actual_values) == 0:\n",
    "        return 0.0\n",
    "    return sum(a == p for a, p in zip(actual_values, predicted_values)) / len(actual_values)\n",
    "\n",
    "\n",
    "def _build_label_index(actual_values, predicted_values):\n",
    "    \"\"\"\n",
    "    Auto-detect all class labels and map them to 0..K-1.\n",
    "    This avoids IndexErrors and handles any label format.\n",
    "    \"\"\"\n",
    "    unique_labels = sorted(set(actual_values) | set(predicted_values))\n",
    "    label_to_idx = {lab: i for i, lab in enumerate(unique_labels)}\n",
    "    return unique_labels, label_to_idx\n",
    "\n",
    "\n",
    "def calculate_weighted_precision(actual_values, predicted_values, _unused_classcount=None):\n",
    "    if len(actual_values) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    labels, idx = _build_label_index(actual_values, predicted_values)\n",
    "    K = len(labels)\n",
    "\n",
    "    tp = [0] * K\n",
    "    fp = [0] * K\n",
    "    actual_count = [0] * K\n",
    "\n",
    "    for a, p in zip(actual_values, predicted_values):\n",
    "        ai = idx[a]\n",
    "        pi = idx[p]\n",
    "        actual_count[ai] += 1\n",
    "        if a == p:\n",
    "            tp[ai] += 1\n",
    "        else:\n",
    "            fp[pi] += 1\n",
    "\n",
    "    total = len(actual_values)\n",
    "    precision_sum = 0.0\n",
    "\n",
    "    for i in range(K):\n",
    "        denom = tp[i] + fp[i]\n",
    "        precision_i = tp[i] / denom if denom > 0 else 0.0\n",
    "        weight_i = actual_count[i] / total\n",
    "        precision_sum += precision_i * weight_i\n",
    "\n",
    "    return precision_sum\n",
    "\n",
    "\n",
    "def calculate_weighted_recall(actual_values, predicted_values, _unused_classcount=None):\n",
    "    if len(actual_values) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    labels, idx = _build_label_index(actual_values, predicted_values)\n",
    "    K = len(labels)\n",
    "\n",
    "    tp = [0] * K\n",
    "    fn = [0] * K\n",
    "    actual_count = [0] * K\n",
    "\n",
    "    for a, p in zip(actual_values, predicted_values):\n",
    "        ai = idx[a]\n",
    "        actual_count[ai] += 1\n",
    "        if a == p:\n",
    "            tp[ai] += 1\n",
    "        else:\n",
    "            fn[ai] += 1\n",
    "\n",
    "    total = len(actual_values)\n",
    "    recall_sum = 0.0\n",
    "\n",
    "    for i in range(K):\n",
    "        denom = tp[i] + fn[i]\n",
    "        recall_i = tp[i] / denom if denom > 0 else 0.0\n",
    "        weight_i = actual_count[i] / total\n",
    "        recall_sum += recall_i * weight_i\n",
    "\n",
    "    return recall_sum\n",
    "\n",
    "\n",
    "def calculate_weighted_f1(actual_values, predicted_values, _unused_classcount=None):\n",
    "    if len(actual_values) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    labels, idx = _build_label_index(actual_values, predicted_values)\n",
    "    K = len(labels)\n",
    "\n",
    "    tp = [0] * K\n",
    "    fp = [0] * K\n",
    "    fn = [0] * K\n",
    "\n",
    "    for a, p in zip(actual_values, predicted_values):\n",
    "        ai = idx[a]\n",
    "        pi = idx[p]\n",
    "        if a == p:\n",
    "            tp[ai] += 1\n",
    "        else:\n",
    "            fp[pi] += 1\n",
    "            fn[ai] += 1\n",
    "\n",
    "    total = len(actual_values)\n",
    "    f1_sum = 0.0\n",
    "\n",
    "    for i in range(K):\n",
    "        prec_i = tp[i] / (tp[i] + fp[i]) if (tp[i] + fp[i]) > 0 else 0.0\n",
    "        rec_i  = tp[i] / (tp[i] + fn[i]) if (tp[i] + fn[i]) > 0 else 0.0\n",
    "\n",
    "        f1_i = (2 * prec_i * rec_i / (prec_i + rec_i)) if (prec_i + rec_i) > 0 else 0.0\n",
    "\n",
    "        weight_i = (tp[i] + fn[i]) / total\n",
    "        f1_sum += f1_i * weight_i\n",
    "\n",
    "    return f1_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea905ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_values(values_list):\n",
    "    return [value if value >= 0.5 else 0.5 for value in values_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d25dd4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list(values_list):\n",
    "    while len(values_list) < ROUNDS:\n",
    "        values_list.append(values_list[-1])\n",
    "    return values_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3db4a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_report(actual, predicted, class_labels=None):\n",
    "    if class_labels is None:\n",
    "        class_labels = sorted(set(actual) | set(predicted))\n",
    "    report = classification_report(actual, predicted, target_names=[str(label) for label in class_labels])\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "965c2014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_classification_report(actual, predicted, file_path, class_labels=None):\n",
    "    if class_labels is None:\n",
    "        class_labels = sorted(set(actual) | set(predicted))\n",
    "    report = classification_report(actual, predicted, target_names=[str(label) for label in class_labels])\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(\"Classification Report:\\n\\n\")\n",
    "        file.write(report)\n",
    "    print(f\"Classification report saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb979e",
   "metadata": {},
   "source": [
    "<font color='Orange'>***Confusion Matrix***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b368f177",
   "metadata": {},
   "outputs": [],
   "source": [
    "Label_names = ['Normal', 'DDoS_UDP', 'DDoS_ICMP', 'DDoS_TCP', 'DDoS_HTTP', 'Password', 'Vulnerability_scanner', 'SQL_injection']#, 'Uploading', 'Backdoor', 'Port_Scanning', 'XSS', 'Ransomware', 'MITM', 'OS_Fingerprinting']\n",
    "Label_numbers = [0, 1, 2, 3, 4, 5, 6, 7]#, 8, 9, 10, 11, 12, 13, 14]\n",
    "\n",
    "def plot_confusion_matrix_with_names(actual, predicted, label_numbers, label_names, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix with label names instead of numbers.\n",
    "\n",
    "    Args:\n",
    "        actual (list): The list of actual class labels.\n",
    "        predicted (list): The list of predicted class labels.\n",
    "        label_numbers (list): The list of numeric label identifiers.\n",
    "        label_names (list): The corresponding list of label names.\n",
    "        title (str): The title of the confusion matrix plot.\n",
    "    \"\"\"\n",
    "    # Map numeric labels to their names\n",
    "    label_map = dict(zip(label_numbers, label_names))\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(actual, predicted, labels=label_numbers)\n",
    "    \n",
    "    # Replace numeric labels with names for the axes\n",
    "    class_labels = [label_map[num] for num in label_numbers]\n",
    "    \n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='magma', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    \n",
    "    # Add labels, title, and a color bar\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"Actual Labels\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58d9e6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_confusion_matrix_with_names(Data[\"Perfect Post-Detection\"][\"Actual\"][160], Data[\"Perfect Post-Detection\"][\"Predictions\"][160], \n",
    "#                                  Label_numbers, Label_names, title=\"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc67a4",
   "metadata": {},
   "source": [
    "<font color='Red'>***Calculating and Saving Results***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af40fcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Centralized': {'Actual': {},\n",
      "                 'Path': 'DNNCent5atk_40_rounds_3_epochs_0.0015_lr',\n",
      "                 'Predictions': {}}}\n"
     ]
    }
   ],
   "source": [
    "Data = {}\n",
    "Data[\"Centralized\"] = {}\n",
    "Data[\"Centralized\"][\"Path\"] = PATH  # Your centralized model output directory\n",
    "Data[\"Centralized\"][\"Actual\"] = {}\n",
    "Data[\"Centralized\"][\"Predictions\"] = {}\n",
    "# Data[\"Perfect Post-Detection\"] = {}\n",
    "# Data[\"Confidence Thresholding-Round 4\"] = {}\n",
    "# Data[\"Outlier Removing-Round 4\"] = {}\n",
    "# Data[\"No Post-Detection-Round 4\"] = {}\n",
    "# Data[\"Confidence Thresholding-Round 10\"] = {}\n",
    "# Data[\"Outlier Removing-Round 10\"] = {}\n",
    "# Data[\"No Post-Detection-Round 10\"] = {}\n",
    "# Data[\"Perfect Post-Detection\"]['Path'] = \"Ideal_20250206\"\n",
    "# Data[\"Confidence Thresholding-Round 4\"]['Path'] = \"Confidence_20250214_4\"\n",
    "# Data[\"Outlier Removing-Round 4\"]['Path'] = \"Outlier_20250214_4\"\n",
    "# Data[\"No Post-Detection-Round 4\"]['Path'] = \"NoPD_20250214_4\"\n",
    "# Data[\"Confidence Thresholding-Round 10\"]['Path'] = \"Confidence_20250214_10\"\n",
    "# Data[\"Outlier Removing-Round 10\"]['Path'] = \"Outlier_20250214_10\"\n",
    "# Data[\"No Post-Detection-Round 10\"]['Path'] = \"NoPD_20250214_10\"\n",
    "for Usecase in Data:\n",
    "    Data[Usecase]['Actual'] = {}\n",
    "    Data[Usecase]['Predictions'] = {}\n",
    "pprint.pprint(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a07de356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for Usecase in Data:\n",
    "    for i in range(1, ROUNDS*SPLITS+1):\n",
    "        try:\n",
    "            filename = f\"{Data[Usecase]['Path']}/Global_{i}_actual\"\n",
    "            with open(filename, 'rb') as file:\n",
    "                Actual = pickle.load(file)\n",
    "            Data[Usecase]['Actual'][i] = [item for sublist in Actual for item in sublist]\n",
    "            filename = f\"{Data[Usecase]['Path']}/Global_{i}_pred\"\n",
    "            with open(filename, 'rb') as file:\n",
    "                Pred = pickle.load(file)\n",
    "            Data[Usecase]['Predictions'][i] = [item for sublist in Pred for item in sublist]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing file {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "440714d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# value_counts = Counter(Data['Confidence Thresholding']['Predictions'][160])\n",
    "# for value, count in value_counts.items():\n",
    "#     print(f\"Value: {value}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9f8459b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Centralized': {'Accuracy': [], 'F1_Score': [], 'Precision': [], 'Recall': []}}\n"
     ]
    }
   ],
   "source": [
    "Results = {}\n",
    "for Usecase in Data:\n",
    "    Results[Usecase] = {}\n",
    "for Usecase in Results:\n",
    "    Results[Usecase]['Accuracy'] = []\n",
    "    Results[Usecase]['Recall'] = []\n",
    "    Results[Usecase]['Precision'] = [] \n",
    "    Results[Usecase]['F1_Score'] = []\n",
    "pprint.pprint(Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a094c2be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for Usecase in Results:\n",
    "    for Round in range(1, ROUNDS*SPLITS+1):\n",
    "        try:\n",
    "            Results[Usecase]['Accuracy'].append(calculate_accuracy(Data[Usecase]['Actual'][Round], Data[Usecase]['Predictions'][Round]))\n",
    "            Results[Usecase]['Precision'].append(calculate_weighted_precision(Data[Usecase]['Actual'][Round], Data[Usecase]['Predictions'][Round]))\n",
    "            Results[Usecase]['Recall'].append(calculate_weighted_recall(Data[Usecase]['Actual'][Round], Data[Usecase]['Predictions'][Round]))\n",
    "            Results[Usecase]['F1_Score'].append(calculate_weighted_f1(Data[Usecase]['Actual'][Round], Data[Usecase]['Predictions'][Round]))\n",
    "        except KeyError:\n",
    "            print('Usecase:', Usecase, 'Round:', Round)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b32df4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Usecase in Results:\n",
    "#     Results[Usecase]['Accuracy'] = update_values(Results[Usecase]['Accuracy'])\n",
    "#     Results[Usecase]['Recall'] = update_values(Results[Usecase]['Recall'])\n",
    "#     Results[Usecase]['Precision'] = update_values(Results[Usecase]['Precision'])    \n",
    "#     Results[Usecase]['F1_Score'] = update_values(Results[Usecase]['F1_Score'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f56712c",
   "metadata": {},
   "source": [
    "<font color='Light Blue'>***Save/ Load Calculated Results***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a866d11-6b4a-4bec-9630-d3454ef24489",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{PATH}/PD_Results.pkl\", \"wb\") as file:\n",
    "    pickle.dump(Results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8befbcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"PD_Results.pkl\", \"rb\") as file:\n",
    "#     Results = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c9984",
   "metadata": {},
   "source": [
    "<font color='Green'>***Plotting Results***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fd732d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 6400x4800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "LineStyle = ['ro-', 'b*-', 'ks-', 'gh-', 'm<-', 'yp-', 'b*-', 'gh-', 'rH-', 'c+-', 'mx-', 'ro-', 'b*-', 'ks-', 'gh-', 'y<-']\n",
    "fig = plt.figure(figsize=(17.2, 13), dpi=450)\n",
    "axx = fig.add_subplot(1,1,1)\n",
    "plt.figure(dpi=1000)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "\n",
    "x_axis = np.array(np.arange(1, ROUNDS+1, 1).tolist())\n",
    "x_axis = np.insert(x_axis, 0, 0)\n",
    "y_points = {}\n",
    "for Usecase in Data:\n",
    "    y_points[Usecase] = np.array(Results[Usecase]['Accuracy'])\n",
    "    y_points[Usecase] = np.insert(y_points[Usecase], 0, 0.5)\n",
    "index = 0\n",
    "for Usecase in Data:\n",
    "    axx.plot(x_axis, y_points[Usecase],LineStyle[index], label = Usecase, linewidth=2.5,  markersize=10)\n",
    "    index += 1\n",
    "    \n",
    "axx.set_xlabel(f'{PATH}', fontdict={'fontsize': 36})\n",
    "axx.set_ylabel('Detection Accuracy', fontdict={'fontsize': 36})\n",
    "axx.set_xticks(np.arange(0, ROUNDS+1, 5).tolist()) \n",
    "axx.set_yticks(np.arange(0.30, 1.05, 0.05).tolist())\n",
    "axx.legend(loc = 'lower right', prop={'size': 16})\n",
    "axx.tick_params(axis='x', which='both', bottom=True, top=True, labelbottom=True, labeltop=True, labelsize=20, colors='black')\n",
    "axx.xaxis.set_ticks_position('both')\n",
    "axx.tick_params(axis='y', which='both', left=True, right=True, labelleft=True, labelright=True, labelsize=20, colors='black')\n",
    "axx.yaxis.set_ticks_position('both')\n",
    "axx.xaxis.label.set_color('black')\n",
    "axx.yaxis.label.set_color('black')\n",
    "fig.savefig(f\"{PATH}/1_Accuracy.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41f13e63-48da-4c9b-9456-7b32b5c3a430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 6400x4800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "LineStyle = ['ro-', 'b*-', 'ks-', 'gh-', 'm<-', 'yp-', 'b*-', 'gh-', 'rH-', 'c+-', 'mx-', 'ro-', 'b*-', 'ks-', 'gh-', 'y<-']\n",
    "fig = plt.figure(figsize=(17.2, 13), dpi=450)\n",
    "axx = fig.add_subplot(1,1,1)\n",
    "plt.figure(dpi=1000)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "\n",
    "x_axis = np.array(np.arange(1, ROUNDS+1, 1).tolist())\n",
    "x_axis = np.insert(x_axis, 0, 0)\n",
    "y_points = {}\n",
    "for Usecase in Data:\n",
    "    y_points[Usecase] = np.array(Results[Usecase]['Recall'])\n",
    "    y_points[Usecase] = np.insert(y_points[Usecase], 0, 0.5)\n",
    "\n",
    "index = 0\n",
    "for Usecase in Data:\n",
    "    axx.plot(x_axis, y_points[Usecase],LineStyle[index], label = Usecase, linewidth=2.5,  markersize=10)\n",
    "    index += 1\n",
    "    \n",
    "axx.set_xlabel(f'{PATH}', fontdict={'fontsize': 36})\n",
    "axx.set_ylabel('Detection Recall', fontdict={'fontsize': 36})\n",
    "axx.set_xticks(np.arange(0, ROUNDS+1, 5).tolist()) \n",
    "axx.set_yticks(np.arange(0.30, 1.05, 0.05).tolist())\n",
    "axx.legend(loc = 'lower right', prop={'size': 16})\n",
    "axx.tick_params(axis='x', which='both', bottom=True, top=True, labelbottom=True, labeltop=True, labelsize=20, colors='black')\n",
    "axx.xaxis.set_ticks_position('both')\n",
    "axx.tick_params(axis='y', which='both', left=True, right=True, labelleft=True, labelright=True, labelsize=20, colors='black')\n",
    "axx.yaxis.set_ticks_position('both')\n",
    "axx.xaxis.label.set_color('black')\n",
    "axx.yaxis.label.set_color('black')\n",
    "fig.savefig(f\"{PATH}/1_Wieghted_Recall.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9813c1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 6400x4800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "LineStyle = ['ro-', 'b*-', 'ks-', 'gh-', 'm<-', 'yp-', 'b*-', 'gh-', 'rH-', 'c+-', 'mx-', 'ro-', 'b*-', 'ks-', 'gh-', 'y<-']\n",
    "fig = plt.figure(figsize=(17.2, 13), dpi=450)\n",
    "axx = fig.add_subplot(1,1,1)\n",
    "plt.figure(dpi=1000)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "\n",
    "x_axis = np.array(np.arange(1, ROUNDS+1, 1).tolist())\n",
    "x_axis = np.insert(x_axis, 0, 0)\n",
    "y_points = {}\n",
    "for Usecase in Data:\n",
    "    y_points[Usecase] = np.array(Results[Usecase]['Precision'])\n",
    "    y_points[Usecase] = np.insert(y_points[Usecase], 0, 0.5)\n",
    "\n",
    "index = 0\n",
    "for Usecase in Data:\n",
    "    axx.plot(x_axis, y_points[Usecase],LineStyle[index], label = Usecase, linewidth=2.5,  markersize=10)\n",
    "    index += 1\n",
    "    \n",
    "axx.set_xlabel(f'{PATH}', fontdict={'fontsize': 36})\n",
    "axx.set_ylabel('Detection Precision', fontdict={'fontsize': 36})\n",
    "axx.set_xticks(np.arange(0, ROUNDS+1, 5).tolist()) \n",
    "axx.set_yticks(np.arange(0.30, 1.05, 0.05).tolist())\n",
    "axx.legend(loc = 'lower right', prop={'size': 16})\n",
    "axx.tick_params(axis='x', which='both', bottom=True, top=True, labelbottom=True, labeltop=True, labelsize=20, colors='black')\n",
    "axx.xaxis.set_ticks_position('both')\n",
    "axx.tick_params(axis='y', which='both', left=True, right=True, labelleft=True, labelright=True, labelsize=20, colors='black')\n",
    "axx.yaxis.set_ticks_position('both')\n",
    "axx.xaxis.label.set_color('black')\n",
    "axx.yaxis.label.set_color('black')\n",
    "fig.savefig(f\"{PATH}/1_Wieghted_Precision.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a5eaf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 6400x4800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "LineStyle = ['ro-', 'b*-', 'ks-', 'gh-', 'm<-', 'yp-', 'b*-', 'gh-', 'rH-', 'c+-', 'mx-', 'ro-', 'b*-', 'ks-', 'gh-', 'y<-']\n",
    "fig = plt.figure(figsize=(17.2, 13), dpi=450)\n",
    "axx = fig.add_subplot(1,1,1)\n",
    "plt.figure(dpi=1000)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "\n",
    "x_axis = np.array(np.arange(1, ROUNDS+1, 1).tolist())\n",
    "x_axis = np.insert(x_axis, 0, 0)\n",
    "y_points = {}\n",
    "for Usecase in Data:\n",
    "    y_points[Usecase] = np.array(Results[Usecase]['F1_Score'])\n",
    "    y_points[Usecase] = np.insert(y_points[Usecase], 0, 0.5)\n",
    "\n",
    "index = 0\n",
    "for Usecase in Data:\n",
    "    axx.plot(x_axis, y_points[Usecase],LineStyle[index], label = Usecase, linewidth=2.5,  markersize=10)\n",
    "    index += 1\n",
    "    \n",
    "axx.set_xlabel(f'{PATH}', fontdict={'fontsize': 36})\n",
    "axx.set_ylabel('Detection F1_Score', fontdict={'fontsize': 36})\n",
    "axx.set_xticks(np.arange(0, ROUNDS+1, 5).tolist()) \n",
    "axx.set_yticks(np.arange(0.30, 1.05, 0.05).tolist())\n",
    "axx.legend(loc = 'lower right', prop={'size': 16})\n",
    "axx.tick_params(axis='x', which='both', bottom=True, top=True, labelbottom=True, labeltop=True, labelsize=20, colors='black')\n",
    "axx.xaxis.set_ticks_position('both')\n",
    "axx.tick_params(axis='y', which='both', left=True, right=True, labelleft=True, labelright=True, labelsize=20, colors='black')\n",
    "axx.yaxis.set_ticks_position('both')\n",
    "axx.xaxis.label.set_color('black')\n",
    "axx.yaxis.label.set_color('black')\n",
    "fig.savefig(f\"{PATH}/1_Wieghted_F1_Score.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

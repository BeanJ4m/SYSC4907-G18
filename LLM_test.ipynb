{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) Imports & basic config\n",
    "# ============================================================\n",
    "import os, json, csv, pickle, re\n",
    "from typing import List\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.common import parameters_to_ndarrays, ndarrays_to_parameters\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CPU-only for training/eval; do NOT hide GPU globally ----\n",
    "import torch\n",
    "\n",
    "DEVICE_CLIENT = torch.device(\"cpu\")   # single source of truth for ALL training/eval\n",
    "DEVICE = DEVICE_CLIENT                # used by train/evaluate/Flower code\n",
    "DEVICE_LLM = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------- Paths & constants -------\n",
    "PATH         = \"llm-test-1\"\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "ROUNDS       = 3\n",
    "NUM_CLIENTS  = 10\n",
    "BATCH_SIZE   = 64\n",
    "BATCH_ROUND  = 16       # total batches per client segment (BATCH_ROUND * BATCH_SIZE = per-client slice length)\n",
    "SIZE_ROUND   = int(BATCH_ROUND * BATCH_SIZE * NUM_CLIENTS)\n",
    "DATA_GROUPS  = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 (184320, 99)\n",
      "100 (184320, 99)\n",
      "70 (184320, 99)\n",
      "50 (184320, 99)\n",
      "testing (120000, 99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined shape: (737280, 99)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1) Load IoT dataset (your exact recipe)\n",
    "# ============================================================\n",
    "TrafficData = {'Dataset': {}}\n",
    "sets_names = ['30','100','70','50','testing']\n",
    "\n",
    "for DATA_NUM in sets_names:\n",
    "    TrafficData['Dataset'][DATA_NUM] = pd.read_csv(\n",
    "        f'data/2_Dataset_5_Attack_{DATA_NUM}_normal.csv',\n",
    "        low_memory=False, quoting=csv.QUOTE_NONE, on_bad_lines='skip'\n",
    "    )\n",
    "    print(DATA_NUM, TrafficData['Dataset'][DATA_NUM].shape)\n",
    "\n",
    "# shuffle for randomness\n",
    "for DATA_NUM in TrafficData['Dataset']:\n",
    "    TrafficData['Dataset'][DATA_NUM] = (\n",
    "        TrafficData['Dataset'][DATA_NUM].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "# split each training set into DATA_GROUPS parts\n",
    "TrafficData['Split'] = {}\n",
    "sets_training = ['30','100','70','50']\n",
    "for DATA_NUM in sets_training:\n",
    "    TrafficData['Split'][DATA_NUM] = np.array_split(TrafficData['Dataset'][DATA_NUM], DATA_GROUPS)\n",
    "\n",
    "# interleave groups vertically (faster: build list and concat once)\n",
    "frames = []\n",
    "for GROUP in range(DATA_GROUPS):\n",
    "    frames.extend([\n",
    "        TrafficData['Split']['30'][GROUP],\n",
    "        TrafficData['Split']['100'][GROUP],\n",
    "        TrafficData['Split']['70'][GROUP],\n",
    "        TrafficData['Split']['50'][GROUP],\n",
    "    ])\n",
    "TrafficData['Combined'] = pd.concat(frames, ignore_index=True)\n",
    "print(\"Combined shape:\", TrafficData['Combined'].shape)\n",
    "\n",
    "# split into Train/Test (features: all but last; label: last column)\n",
    "TrafficData['Train'] = {\n",
    "    'X': TrafficData['Combined'].iloc[:, :-1],\n",
    "    'y': TrafficData['Combined'].iloc[:, -1]\n",
    "}\n",
    "TrafficData['Test'] = {\n",
    "    'X': TrafficData['Dataset']['testing'].iloc[:, :-1],\n",
    "    'y': TrafficData['Dataset']['testing'].iloc[:, -1]\n",
    "}\n",
    "\n",
    "# scale features\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(TrafficData['Train']['X'])\n",
    "TrafficData['Train']['X'] = scaler.transform(TrafficData['Train']['X'])\n",
    "TrafficData['Test']['X']  = scaler.transform(TrafficData['Test']['X'])\n",
    "\n",
    "# to numpy arrays\n",
    "TrafficData['Train']['X'] = np.array(TrafficData['Train']['X'])\n",
    "TrafficData['Train']['y'] = np.array(TrafficData['Train']['y'])\n",
    "TrafficData['Test']['X']  = np.array(TrafficData['Test']['X'])\n",
    "TrafficData['Test']['y']  = np.array(TrafficData['Test']['y'])\n",
    "\n",
    "# round windows\n",
    "TrafficData['ROUNDS'] = {}\n",
    "SIZE_Demo = SIZE_ROUND\n",
    "for ROUND in range(1, ROUNDS + 1):\n",
    "    if ROUND == 1:\n",
    "        TrafficData['ROUNDS'][ROUND] = {\n",
    "            'X': TrafficData['Train']['X'][:SIZE_Demo],\n",
    "            'y': TrafficData['Train']['y'][:SIZE_Demo],\n",
    "        }\n",
    "    else:\n",
    "        TrafficData['ROUNDS'][ROUND] = {\n",
    "            'X': TrafficData['Train']['X'][(SIZE_Demo - SIZE_ROUND):SIZE_Demo],\n",
    "            'y': TrafficData['Train']['y'][(SIZE_Demo - SIZE_ROUND):SIZE_Demo],\n",
    "        }\n",
    "    SIZE_Demo += SIZE_ROUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2) Torch Datasets & Federated loaders\n",
    "# ============================================================\n",
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = torch.from_numpy(X_data).float()\n",
    "        self.y_data = torch.from_numpy(y_data).long()\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_data[idx], self.y_data[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "TrafficData['trainsets'] = {\n",
    "    r: ClassifierDataset(TrafficData['ROUNDS'][r]['X'], TrafficData['ROUNDS'][r]['y'])\n",
    "    for r in range(1, ROUNDS + 1)\n",
    "}\n",
    "TrafficData['testset'] = ClassifierDataset(TrafficData['Test']['X'], TrafficData['Test']['y'])\n",
    "\n",
    "def load_train(num_clients, ROUND):\n",
    "    # Each client gets a contiguous slice of SIZE_ROUND\n",
    "    portion_size = int(BATCH_ROUND * BATCH_SIZE)  # per-client samples\n",
    "    portion_indices = [\n",
    "        list(range(i * portion_size, min((i + 1) * portion_size, SIZE_ROUND)))\n",
    "        for i in range(num_clients)\n",
    "    ]\n",
    "    portion_datasets = [Subset(TrafficData['trainsets'][ROUND], idx) for idx in portion_indices]\n",
    "    return [DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False) for ds in portion_datasets]\n",
    "\n",
    "def load_test():\n",
    "    return DataLoader(TrafficData['testset'], batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "Dataloaders = {r: load_train(NUM_CLIENTS, r) for r in range(1, ROUNDS + 1)}\n",
    "Dataloaders['Test'] = load_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3) Model, train/test\n",
    "# ============================================================\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super().__init__()\n",
    "        # 98 features, 15 classes (per your earlier Net)\n",
    "        self.layer_1 = nn.Linear(98, 64)\n",
    "        self.layer_2 = nn.Linear(64, 32)\n",
    "        self.layer_out = nn.Linear(32, 15)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x); x = self.relu(x); x = self.dropout(x)\n",
    "        x = self.layer_2(x); x = self.relu(x); x = self.dropout(x)\n",
    "        return self.layer_out(x)\n",
    "\n",
    "def train_once(net, loader, epochs, lr, weight_decay=0.0):\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    opt  = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    net.train()\n",
    "    for ep in range(epochs):\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            out = net(X)\n",
    "            loss = crit(out, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            loss_sum += loss.item()\n",
    "            total += y.size(0)\n",
    "            correct += (torch.max(out, 1)[1] == y).sum().item()\n",
    "        print(f\"  Epoch {ep+1}/{epochs} — Loss={loss_sum:.4f}, Acc={correct/total:.4f}\")\n",
    "\n",
    "def evaluate(net, loader):\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    net.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "            out = net(X)\n",
    "            loss_sum += crit(out, y).item()\n",
    "            pred = torch.max(out, 1)[1]\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    acc  = correct / total\n",
    "    loss = loss_sum / total\n",
    "    print(f\"Validation — Loss={loss:.4f}, Acc={acc:.4f}\")\n",
    "    return {\"val_accuracy\": float(acc), \"val_loss\": float(loss)}\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [v.detach().cpu().numpy() for _, v in net.state_dict().items()]\n",
    "\n",
    "def set_parameters(net, params: List[np.ndarray]):\n",
    "    net.load_state_dict(OrderedDict({k: torch.tensor(v) for k, v in zip(net.state_dict().keys(), params)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading microsoft/Phi-3-mini-4k-instruct with mixed device_map (GPU+CPU fallback)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c294b0c2314e0ba56ae9625cdfa191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Phi-3 text-generation pipeline ready (hybrid GPU/CPU).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4) LLM controller (Phi-3-mini – GPU/CPU hybrid safe loader)\n",
    "# ============================================================\n",
    "import os, re, json, torch\n",
    "from functools import lru_cache\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "USE_OFFLINE_LLM = True\n",
    "if USE_OFFLINE_LLM:\n",
    "    os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "    os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "os.environ.setdefault(\"TRANSFORMERS_CACHE\", \"/tmp/hf_cache\")\n",
    "\n",
    "LLM_MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "device_llm = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype_llm  = torch.float16 if device_llm == \"cuda\" else torch.float32\n",
    "print(f\"Loading {LLM_MODEL_ID} with mixed device_map (GPU+CPU fallback)...\")\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _get_llm_and_tok():\n",
    "    tok = AutoTokenizer.from_pretrained(LLM_MODEL_ID, use_fast=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    # ✅ Safe load: use accelerate to automatically split model across GPU/CPU\n",
    "    llm = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL_ID,\n",
    "        dtype=dtype_llm,\n",
    "        device_map=\"auto\",             # let Accelerate place layers automatically\n",
    "        offload_folder=\"/tmp/phi3_offload\",  # offload overflow to CPU RAM\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    return llm, tok\n",
    "\n",
    "llm, tok = _get_llm_and_tok()\n",
    "# ⚠️ Do NOT set device manually; accelerate already handles placement\n",
    "llm_pipe = pipeline(\"text-generation\", model=llm, tokenizer=tok, dtype=dtype_llm)\n",
    "\n",
    "print(\"✅ Phi-3 text-generation pipeline ready (hybrid GPU/CPU).\")\n",
    "\n",
    "GEN_KW = dict(max_new_tokens=60, temperature=0.0, do_sample=False)\n",
    "\n",
    "def _extract_json(text: str) -> dict:\n",
    "    m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n",
    "    if not m:\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(m.group(0))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def llm_review_and_suggest(cfg: dict, metrics: dict, data_info: dict) -> dict:\n",
    "    compact_metrics = {k: v for k, v in metrics.items() if isinstance(v, (int, float))}\n",
    "    prompt = (\n",
    "        \"You are an ML optimization assistant for a federated learning system.\\n\"\n",
    "        \"Optimize hyperparameters to improve validation accuracy and stability.\\n\"\n",
    "        \"Avoid overfitting. Return JSON ONLY with: learning_rate, epochs, dropout, weight_decay.\\n\\n\"\n",
    "        f\"CURRENT_CONFIG:\\n{json.dumps(cfg)}\\n\\n\"\n",
    "        f\"METRICS:\\n{json.dumps(compact_metrics)}\\n\\n\"\n",
    "        f\"DATA_INFO:\\n{json.dumps(data_info)}\\n\"\n",
    "    )\n",
    "    out = llm_pipe(prompt, **GEN_KW)[0][\"generated_text\"]\n",
    "    res = _extract_json(out)\n",
    "    return res or {\n",
    "        \"learning_rate\": cfg.get(\"learning_rate\", 1e-3),\n",
    "        \"epochs\": cfg.get(\"epochs\", 1),\n",
    "        \"dropout\": cfg.get(\"dropout\", 0.2),\n",
    "        \"weight_decay\": cfg.get(\"weight_decay\", 0.0),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5) Flower client & custom strategy that saves GlobalModel_{round}.pth\n",
    "# ============================================================\n",
    "import gc, ray\n",
    "\n",
    "# ---- Do NOT hide GPU globally; keep training on CPU via DEVICE and client_resources ----\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "DEVICE_CLIENT = DEVICE\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    ray.shutdown()\n",
    "\n",
    "\n",
    "\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, loader, round_id):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.loader = loader\n",
    "        self.round_id = round_id\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        set_parameters(self.net, parameters)\n",
    "        train_once(\n",
    "            self.net,\n",
    "            self.loader,\n",
    "            epochs=int(config[\"local_epochs\"]),\n",
    "            lr=float(config[\"learning_rate\"]),\n",
    "            weight_decay=float(TRAIN_CFG.get(\"weight_decay\", 0.0)),\n",
    "        )\n",
    "        return get_parameters(self.net), len(self.loader.dataset), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        # not used for centralized eval in this setup\n",
    "        return 0.0, len(self.loader.dataset), {}\n",
    "\n",
    "\n",
    "Global_Models = {}\n",
    "\n",
    "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
    "    def __init__(self, round_id, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.round_id = round_id\n",
    "\n",
    "    def aggregate_fit(self, rnd, results, failures):\n",
    "        aggregated_parameters_tuple = super().aggregate_fit(rnd, results, failures)\n",
    "        if aggregated_parameters_tuple is None:\n",
    "            return None\n",
    "\n",
    "        aggregated_parameters, _ = aggregated_parameters_tuple\n",
    "        if aggregated_parameters is not None:\n",
    "            weights = parameters_to_ndarrays(aggregated_parameters)\n",
    "            model = Net(dropout=TRAIN_CFG[\"dropout\"]).to(DEVICE)\n",
    "            set_parameters(model, weights)\n",
    "            os.makedirs(PATH, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"{PATH}/GlobalModel_{self.round_id}.pth\")\n",
    "            Global_Models[self.round_id] = model\n",
    "        return aggregated_parameters_tuple\n",
    "\n",
    "\n",
    "def fit_config(server_round: int):\n",
    "    return {\n",
    "        \"current_round\": server_round,\n",
    "        \"local_epochs\": TRAIN_CFG[\"epochs\"],\n",
    "        \"learning_rate\": TRAIN_CFG[\"learning_rate\"],\n",
    "        \"batch_size\": TRAIN_CFG[\"batch_size\"],\n",
    "        \"dropout\": TRAIN_CFG[\"dropout\"],\n",
    "        \"weight_decay\": TRAIN_CFG[\"weight_decay\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Training configuration (LLM will adjust this each round)\n",
    "# ============================================================\n",
    "TRAIN_CFG = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"epochs\": 1,\n",
    "    \"dropout\": 0.2,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "}\n",
    "\n",
    "def _clip(v, lo, hi):\n",
    "    try:\n",
    "        return max(lo, min(hi, v))\n",
    "    except:\n",
    "        return v\n",
    "\n",
    "def apply_llm_suggestions(sugg: dict, cfg: dict) -> dict:\n",
    "    new = dict(cfg)\n",
    "    if \"learning_rate\" in sugg:\n",
    "        new[\"learning_rate\"] = float(_clip(float(sugg[\"learning_rate\"]), 1e-5, 5e-3))\n",
    "    if \"epochs\" in sugg:\n",
    "        new[\"epochs\"] = int(_clip(int(sugg[\"epochs\"]), 1, 5))\n",
    "    if \"dropout\" in sugg:\n",
    "        new[\"dropout\"] = float(_clip(float(sugg[\"dropout\"]), 0.0, 0.6))\n",
    "    if \"weight_decay\" in sugg:\n",
    "        new[\"weight_decay\"] = float(_clip(float(sugg[\"weight_decay\"]), 0.0, 0.1))\n",
    "    return new\n",
    "\n",
    "def on_round_end(round_idx: int, round_metrics: dict, data_info: dict):\n",
    "    global TRAIN_CFG\n",
    "    print(f\"\\n[LLM] Reviewing round {round_idx}…\")\n",
    "    sugg = llm_review_and_suggest(TRAIN_CFG, round_metrics, data_info)\n",
    "    if not sugg:\n",
    "        print(\"[LLM] No JSON suggestions; keeping config.\")\n",
    "        return TRAIN_CFG\n",
    "    TRAIN_CFG = apply_llm_suggestions(sugg, TRAIN_CFG)\n",
    "    print(f\"[LLM] Updated TRAIN_CFG: {TRAIN_CFG}\")\n",
    "    return TRAIN_CFG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2025-11-05 02:57:22,032 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=1, round_timeout=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Starting Federated Training =====\n",
      "\n",
      "--- Round 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 02:57:28,874\tINFO worker.py:2012 -- Started a local Ray instance.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "INFO flwr 2025-11-05 02:57:31,207 | app.py:210 | Flower VCE: Ray initialized with resources: {'object_store_memory': 2324035584.0, 'memory': 5422749696.0, 'node:10.192.11.216': 1.0, 'CPU': 1.0, 'node:__internal_head__': 1.0}\n",
      "INFO flwr 2025-11-05 02:57:31,207 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0}\n",
      "INFO flwr 2025-11-05 02:57:31,376 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors\n",
      "INFO flwr 2025-11-05 02:57:31,379 | server.py:89 | Initializing global parameters\n",
      "INFO flwr 2025-11-05 02:57:31,380 | server.py:272 | Using initial parameters provided by strategy\n",
      "INFO flwr 2025-11-05 02:57:31,381 | server.py:91 | Evaluating initial parameters\n",
      "INFO flwr 2025-11-05 02:57:31,381 | server.py:104 | FL starting\n",
      "DEBUG flwr 2025-11-05 02:57:31,383 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 10)\n",
      "\u001b[36m(DefaultActor pid=138066)\u001b[0m /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "\u001b[36m(DefaultActor pid=138066)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DefaultActor pid=138066)\u001b[0m   Epoch 1/1 — Loss=41.3321, Acc=0.2568\n",
      "\u001b[36m(DefaultActor pid=138066)\u001b[0m   Epoch 1/1 — Loss=41.3030, Acc=0.2578\n",
      "\u001b[36m(DefaultActor pid=138066)\u001b[0m   Epoch 1/1 — Loss=41.2510, Acc=0.2686\n",
      "\u001b[36m(DefaultActor pid=138066)\u001b[0m   Epoch 1/1 — Loss=41.2953, Acc=0.2607\n",
      "\u001b[36m(DefaultActor pid=138066)\u001b[0m   Epoch 1/1 — Loss=41.3466, Acc=0.2666\n",
      "\u001b[36m(DefaultActor pid=138066)\u001b[0m   Epoch 1/1 — Loss=41.3159, Acc=0.2637\n",
      "\u001b[36m(DefaultActor pid=138066)\u001b[0m   Epoch 1/1 — Loss=41.2820, Acc=0.2510\n",
      "\u001b[36m(DefaultActor pid=138066)\u001b[0m   Epoch 1/1 — Loss=41.3848, Acc=0.2666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2025-11-05 02:57:38,330 | server.py:236 | fit_round 1 received 10 results and 0 failures\n",
      "WARNING flwr 2025-11-05 02:57:38,358 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2025-11-05 02:57:38,373 | server.py:173 | evaluate_round 1: strategy sampled 10 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DefaultActor pid=138066)\u001b[0m   Epoch 1/1 — Loss=41.3051, Acc=0.2568\n",
      "\u001b[36m(DefaultActor pid=138066)\u001b[0m   Epoch 1/1 — Loss=41.3202, Acc=0.2656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2025-11-05 02:57:40,915 | server.py:187 | evaluate_round 1 received 10 results and 0 failures\n",
      "WARNING flwr 2025-11-05 02:57:40,917 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided\n",
      "INFO flwr 2025-11-05 02:57:40,917 | server.py:153 | FL finished in 9.535465009999825\n",
      "INFO flwr 2025-11-05 02:57:40,919 | app.py:225 | app_fit: losses_distributed [(1, 0.0)]\n",
      "INFO flwr 2025-11-05 02:57:40,921 | app.py:226 | app_fit: metrics_distributed_fit {}\n",
      "INFO flwr 2025-11-05 02:57:40,923 | app.py:227 | app_fit: metrics_distributed {}\n",
      "INFO flwr 2025-11-05 02:57:40,923 | app.py:228 | app_fit: losses_centralized []\n",
      "INFO flwr 2025-11-05 02:57:40,925 | app.py:229 | app_fit: metrics_centralized {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation — Loss=0.0383, Acc=0.5000\n",
      "\n",
      "[LLM] Reviewing round 1…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-11-05 02:57:52,619 E 137443 137443] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-11-05 02:57:57,890 E 137864 137864] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2025-11-05 02:58:01,196 E 127744 138063] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(DefaultActor pid=138066)\u001b[0m [2025-11-05 02:58:01,114 E 138066 138338] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLM] Updated TRAIN_CFG: {'learning_rate': 0.001, 'epochs': 1, 'dropout': 0.2, 'weight_decay': 0.0, 'batch_size': 64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2025-11-05 03:20:03,649 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=1, round_timeout=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Round 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-11-05 03:20:07,606\tINFO worker.py:2012 -- Started a local Ray instance.\n",
      "INFO flwr 2025-11-05 03:20:10,631 | app.py:210 | Flower VCE: Ray initialized with resources: {'node:10.192.11.216': 1.0, 'memory': 4812500583.0, 'object_store_memory': 2062500249.0, 'node:__internal_head__': 1.0, 'CPU': 1.0}\n",
      "INFO flwr 2025-11-05 03:20:10,631 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0}\n",
      "INFO flwr 2025-11-05 03:20:10,645 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors\n",
      "INFO flwr 2025-11-05 03:20:10,647 | server.py:89 | Initializing global parameters\n",
      "INFO flwr 2025-11-05 03:20:10,647 | server.py:272 | Using initial parameters provided by strategy\n",
      "INFO flwr 2025-11-05 03:20:10,648 | server.py:91 | Evaluating initial parameters\n",
      "INFO flwr 2025-11-05 03:20:10,649 | server.py:104 | FL starting\n",
      "DEBUG flwr 2025-11-05 03:20:10,649 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 10)\n",
      "\u001b[36m(DefaultActor pid=192184)\u001b[0m /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "\u001b[36m(DefaultActor pid=192184)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DefaultActor pid=192184)\u001b[0m   Epoch 1/1 — Loss=37.9567, Acc=0.3828\n",
      "\u001b[36m(DefaultActor pid=192184)\u001b[0m   Epoch 1/1 — Loss=38.1049, Acc=0.3486\n",
      "\u001b[36m(DefaultActor pid=192184)\u001b[0m   Epoch 1/1 — Loss=38.0187, Acc=0.3701\n",
      "\u001b[36m(DefaultActor pid=192184)\u001b[0m   Epoch 1/1 — Loss=37.8806, Acc=0.3750\n",
      "\u001b[36m(DefaultActor pid=192184)\u001b[0m   Epoch 1/1 — Loss=37.9843, Acc=0.3789\n",
      "\u001b[36m(DefaultActor pid=192184)\u001b[0m   Epoch 1/1 — Loss=38.1612, Acc=0.3652\n",
      "\u001b[36m(DefaultActor pid=192184)\u001b[0m   Epoch 1/1 — Loss=38.0750, Acc=0.3721\n",
      "\u001b[36m(DefaultActor pid=192184)\u001b[0m   Epoch 1/1 — Loss=37.9250, Acc=0.3838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2025-11-05 03:20:16,421 | server.py:236 | fit_round 1 received 10 results and 0 failures\n",
      "WARNING flwr 2025-11-05 03:20:16,427 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2025-11-05 03:20:16,431 | server.py:173 | evaluate_round 1: strategy sampled 10 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DefaultActor pid=192184)\u001b[0m   Epoch 1/1 — Loss=37.8788, Acc=0.3867\n",
      "\u001b[36m(DefaultActor pid=192184)\u001b[0m   Epoch 1/1 — Loss=38.1172, Acc=0.3564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2025-11-05 03:20:18,701 | server.py:187 | evaluate_round 1 received 10 results and 0 failures\n",
      "WARNING flwr 2025-11-05 03:20:18,702 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided\n",
      "INFO flwr 2025-11-05 03:20:18,703 | server.py:153 | FL finished in 8.053580404000058\n",
      "INFO flwr 2025-11-05 03:20:18,704 | app.py:225 | app_fit: losses_distributed [(1, 0.0)]\n",
      "INFO flwr 2025-11-05 03:20:18,704 | app.py:226 | app_fit: metrics_distributed_fit {}\n",
      "INFO flwr 2025-11-05 03:20:18,705 | app.py:227 | app_fit: metrics_distributed {}\n",
      "INFO flwr 2025-11-05 03:20:18,705 | app.py:228 | app_fit: losses_centralized []\n",
      "INFO flwr 2025-11-05 03:20:18,706 | app.py:229 | app_fit: metrics_centralized {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation — Loss=0.0325, Acc=0.5860\n",
      "\n",
      "[LLM] Reviewing round 2…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-11-05 03:20:34,024 E 191722 191722] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-11-05 03:20:37,541 E 192140 192140] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2025-11-05 03:20:40,631 E 127744 192183] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(DefaultActor pid=192184)\u001b[0m [2025-11-05 03:20:40,617 E 192184 192266] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-11-05 03:39:07,558 E 192140 192140] (raylet) node_manager.cc:3252: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 2fbb91b145e1003d92433e06e5b0939163a9a25372c2c4236c226b0b, IP: 10.192.11.216) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.192.11.216`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLM] Updated TRAIN_CFG: {'learning_rate': 0.001, 'epochs': 1, 'dropout': 0.2, 'weight_decay': 0.0, 'batch_size': 64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2025-11-05 03:42:45,710 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=1, round_timeout=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Round 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-11-05 03:42:50,997\tINFO worker.py:2012 -- Started a local Ray instance.\n",
      "INFO flwr 2025-11-05 03:42:54,100 | app.py:210 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'object_store_memory': 1121542963.0, 'CPU': 1.0, 'memory': 2616933581.0, 'node:10.192.11.216': 1.0}\n",
      "INFO flwr 2025-11-05 03:42:54,101 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0}\n",
      "INFO flwr 2025-11-05 03:42:54,123 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors\n",
      "INFO flwr 2025-11-05 03:42:54,123 | server.py:89 | Initializing global parameters\n",
      "INFO flwr 2025-11-05 03:42:54,125 | server.py:272 | Using initial parameters provided by strategy\n",
      "INFO flwr 2025-11-05 03:42:54,126 | server.py:91 | Evaluating initial parameters\n",
      "INFO flwr 2025-11-05 03:42:54,126 | server.py:104 | FL starting\n",
      "DEBUG flwr 2025-11-05 03:42:54,127 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 10)\n",
      "\u001b[36m(DefaultActor pid=251946)\u001b[0m /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "\u001b[36m(DefaultActor pid=251946)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DefaultActor pid=251946)\u001b[0m   Epoch 1/1 — Loss=26.4977, Acc=0.6934\n",
      "\u001b[36m(DefaultActor pid=251946)\u001b[0m   Epoch 1/1 — Loss=32.2567, Acc=0.3965\n",
      "\u001b[36m(DefaultActor pid=251946)\u001b[0m   Epoch 1/1 — Loss=21.2901, Acc=0.9990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2025-11-05 03:42:58,541 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2025-11-05 03:42:58,542 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2025-11-05 03:42:59,257 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2025-11-05 03:42:59,258 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2025-11-05 03:42:59,258 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2025-11-05 03:42:59,259 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2025-11-05 03:42:59,259 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2025-11-05 03:42:59,260 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2025-11-05 03:42:59,261 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2025-11-05 03:42:59,261 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2025-11-05 03:42:59,262 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2025-11-05 03:42:59,263 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2025-11-05 03:42:59,264 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2025-11-05 03:42:59,265 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "DEBUG flwr 2025-11-05 03:42:59,270 | server.py:236 | fit_round 1 received 3 results and 7 failures\n",
      "WARNING flwr 2025-11-05 03:42:59,272 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2025-11-05 03:42:59,276 | server.py:173 | evaluate_round 1: strategy sampled 10 clients (out of 10)\n",
      "ERROR flwr 2025-11-05 03:42:59,538 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2025-11-05 03:42:59,539 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2025-11-05 03:42:59,673 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2025-11-05 03:42:59,674 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2025-11-05 03:43:00,048 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2025-11-05 03:43:00,049 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2025-11-05 03:43:00,050 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2025-11-05 03:43:00,051 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2025-11-05 03:43:00,166 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2025-11-05 03:43:00,171 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2025-11-05 03:43:00,188 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2025-11-05 03:43:00,277 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2025-11-05 03:43:00,416 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2025-11-05 03:43:00,416 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2025-11-05 03:43:00,434 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2025-11-05 03:43:00,483 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2025-11-05 03:43:00,556 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2025-11-05 03:43:00,557 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2025-11-05 03:43:00,558 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2025-11-05 03:43:00,559 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 10.192.11.216, ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b) where the lease (lease ID: 00000000eb5699f2c49b3b88bcbb1beaeac7be664eda6c04c10ed91d3974332c, name=DefaultActor.__init__, pid=251946, memory used=0.49GB) was running was 14.69GB / 15.42GB (0.952519), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.192.11.216`. To see the logs of the worker, use `ray logs worker-6fdbc1bd4932ed258dfb0eb6826134fe1e1eb70d213141aaf3593e36*out -ip 10.192.11.216. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "127744\t8.16\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "198869\t1.89\t/home/zeus/miniconda3/envs/cloudspace/bin/python -m ipykernel_launcher -f /home/zeus/.local/share/ju...\n",
      "250896\t0.83\tray::DefaultActor\n",
      "251946\t0.49\tray::DefaultActor\n",
      "250801\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "251841\t0.09\t/home/zeus/miniconda3/envs/cloudspace/bin/python /home/zeus/miniconda3/envs/cloudspace/lib/python3.1...\n",
      "250875\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251926\t0.08\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "250799\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "251836\t0.07\t/home/zeus/miniconda3/envs/cloudspace/bin/python -u /home/zeus/miniconda3/envs/cloudspace/lib/python...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "DEBUG flwr 2025-11-05 03:43:00,561 | server.py:187 | evaluate_round 1 received 0 results and 10 failures\n",
      "INFO flwr 2025-11-05 03:43:00,561 | server.py:153 | FL finished in 6.434105964999617\n",
      "INFO flwr 2025-11-05 03:43:00,562 | app.py:225 | app_fit: losses_distributed []\n",
      "INFO flwr 2025-11-05 03:43:00,562 | app.py:226 | app_fit: metrics_distributed_fit {}\n",
      "INFO flwr 2025-11-05 03:43:00,562 | app.py:227 | app_fit: metrics_distributed {}\n",
      "INFO flwr 2025-11-05 03:43:00,563 | app.py:228 | app_fit: losses_centralized []\n",
      "INFO flwr 2025-11-05 03:43:00,563 | app.py:229 | app_fit: metrics_centralized {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation — Loss=0.0246, Acc=0.5000\n",
      "\n",
      "[LLM] Reviewing round 3…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-11-05 03:43:16,199 E 251789 251789] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-11-05 03:43:20,934 E 251902 251902] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2025-11-05 03:43:24,102 E 127744 251945] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-11-05 03:43:50,938 E 251902 251902] (raylet) node_manager.cc:3252: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: f172c77716653371b1bc1afc850eb613d21d3be430b2795bcfcd538b, IP: 10.192.11.216) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.192.11.216`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLM] Updated TRAIN_CFG: {'learning_rate': 0.001, 'epochs': 1, 'dropout': 0.2, 'weight_decay': 0.0, 'batch_size': 64}\n",
      "\n",
      "===== Training Complete =====\n",
      "Saved models under: llm-test-1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6) Orchestration: run FL rounds, evaluate, LLM tune\n",
    "# ============================================================\n",
    "Global_Models[0] = Net(dropout=TRAIN_CFG[\"dropout\"]).to(DEVICE)\n",
    "init_path = f\"{PATH}/0_Input_Random_model_Net.pth\"\n",
    "if not os.path.exists(init_path):\n",
    "    torch.save(Global_Models[0].state_dict(), init_path)\n",
    "else:\n",
    "    Global_Models[0].load_state_dict(torch.load(init_path, map_location=DEVICE))\n",
    "Global_Models[0].train()\n",
    "\n",
    "print(\"\\n===== Starting Federated Training =====\")\n",
    "for rnd in range(1, ROUNDS + 1):\n",
    "    print(f\"\\n--- Round {rnd} ---\")\n",
    "\n",
    "    # custom strategy per round\n",
    "    strategy = SaveModelStrategy(\n",
    "        round_id=rnd,\n",
    "        fraction_fit=1.0,\n",
    "        min_fit_clients=NUM_CLIENTS,\n",
    "        min_available_clients=NUM_CLIENTS,\n",
    "        on_fit_config_fn=fit_config,\n",
    "        initial_parameters=ndarrays_to_parameters(get_parameters(Global_Models[rnd - 1])),\n",
    "    )\n",
    "\n",
    "    def client_fn(cid: str, round_id=rnd):\n",
    "        cid_int = int(cid)\n",
    "        loader = Dataloaders[round_id][cid_int % NUM_CLIENTS]\n",
    "        net = Net(dropout=TRAIN_CFG[\"dropout\"]).to(DEVICE)\n",
    "        return FlowerClient(cid, net, loader, round_id)\n",
    "\n",
    "    # ---- safer simulation setup ----\n",
    "    fl.simulation.start_simulation(\n",
    "        client_fn=client_fn,\n",
    "        num_clients=NUM_CLIENTS,\n",
    "        config=fl.server.ServerConfig(num_rounds=1),\n",
    "        strategy=strategy,\n",
    "        client_resources={\"num_cpus\": 1, \"num_gpus\": 0},\n",
    "        ray_init_args={\"num_cpus\": 1, \"num_gpus\": 0, \"include_dashboard\": False},\n",
    "    )\n",
    "\n",
    "    # ---- load aggregated global model ----\n",
    "    model_path = f\"{PATH}/GlobalModel_{rnd}.pth\"\n",
    "    if os.path.exists(model_path):\n",
    "        Global_Models[rnd] = Net(dropout=TRAIN_CFG[\"dropout\"]).to(DEVICE)\n",
    "        Global_Models[rnd].load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "        Global_Models[rnd].eval()\n",
    "    else:\n",
    "        print(f\"⚠️ WARNING: {model_path} not found, reusing previous model.\")\n",
    "        Global_Models[rnd] = Global_Models[rnd - 1]\n",
    "\n",
    "    # ---- Evaluate safely ----\n",
    "    metrics = evaluate(Global_Models[rnd], Dataloaders['Test'])\n",
    "\n",
    "    # if zero division risk (no test samples), patch result\n",
    "    if not metrics or \"val_accuracy\" not in metrics or np.isnan(metrics[\"val_accuracy\"]):\n",
    "        metrics = {\"val_accuracy\": 0.0, \"val_loss\": 1.0}\n",
    "\n",
    "    # ---- LLM tuning step ----\n",
    "    cfg_snapshot = dict(TRAIN_CFG)\n",
    "    data_info = {\n",
    "        \"n_clients\": NUM_CLIENTS,\n",
    "        \"round_size\": SIZE_ROUND,\n",
    "        \"batch_size\": TRAIN_CFG[\"batch_size\"],\n",
    "        \"n_features\": int(TrafficData['Test']['X'].shape[1]),\n",
    "        \"n_classes\": 15,\n",
    "    }\n",
    "\n",
    "    TRAIN_CFG = on_round_end(rnd, metrics, data_info)\n",
    "    clear_memory()\n",
    "\n",
    "print(\"\\n===== Training Complete =====\")\n",
    "print(\"Saved models under:\", PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcoA2oGB6Kyi",
   "metadata": {
    "id": "bcoA2oGB6Kyi",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='green'>***Installation and Libraries Import***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b0b1a7-d1b9-403f-8c98-84e601860c8f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flwr==1.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.5.0)\n",
      "Requirement already satisfied: cryptography<42.0.0,>=41.0.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from flwr==1.5.0) (41.0.7)\n",
      "Requirement already satisfied: grpcio!=1.52.0,<2.0.0,>=1.48.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from flwr==1.5.0) (1.75.1)\n",
      "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from flwr==1.5.0) (0.0.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.21.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from flwr==1.5.0) (1.26.4)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.19.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from flwr==1.5.0) (3.20.3)\n",
      "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from flwr==1.5.0) (3.23.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from cryptography<42.0.0,>=41.0.2->flwr==1.5.0) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from grpcio!=1.52.0,<2.0.0,>=1.48.2->flwr==1.5.0) (4.15.0)\n",
      "Requirement already satisfied: pycparser in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from cffi>=1.12->cryptography<42.0.0,>=41.0.2->flwr==1.5.0) (2.23)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: flwr==1.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from flwr[simulation]==1.5.0) (1.5.0)\n",
      "Requirement already satisfied: cryptography<42.0.0,>=41.0.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from flwr==1.5.0->flwr[simulation]==1.5.0) (41.0.7)\n",
      "Requirement already satisfied: grpcio!=1.52.0,<2.0.0,>=1.48.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from flwr==1.5.0->flwr[simulation]==1.5.0) (1.75.1)\n",
      "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from flwr==1.5.0->flwr[simulation]==1.5.0) (0.0.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.21.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from flwr==1.5.0->flwr[simulation]==1.5.0) (1.26.4)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.19.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from flwr==1.5.0->flwr[simulation]==1.5.0) (3.20.3)\n",
      "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from flwr==1.5.0->flwr[simulation]==1.5.0) (3.23.0)\n",
      "Collecting pydantic<2.0.0 (from flwr[simulation]==1.5.0)\n",
      "  Downloading pydantic-1.10.24-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (154 kB)\n",
      "INFO: pip is looking at multiple versions of flwr[simulation] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting flwr==1.5.0 (from flwr[simulation]==1.5.0)\n",
      "  Downloading flwr-1.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement ray==2.6.3; extra == \"simulation\" (from flwr[simulation]) (from versions: 2.31.0, 2.32.0rc0, 2.32.0, 2.33.0, 2.34.0, 2.35.0, 2.36.0, 2.36.1, 2.37.0, 2.38.0, 2.39.0, 2.40.0, 2.41.0, 2.42.0, 2.42.1, 2.43.0, 2.44.0, 2.44.1, 2.45.0, 2.46.0, 2.47.0, 2.47.1, 2.48.0, 2.49.0, 2.49.1, 2.49.2, 2.50.0, 2.50.1, 2.51.0, 2.51.1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for ray==2.6.3; extra == \"simulation\"\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pip in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (25.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: torchvision in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.18.0)\n",
      "Requirement already satisfied: matplotlib in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (3.8.2)\n",
      "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.93)\n",
      "Requirement already satisfied: numpy in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: async-timeout in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install flwr==1.5.0\n",
    "%pip install 'flwr[simulation]==1.5.0'\n",
    "%pip install --upgrade pip\n",
    "%pip install torch torchvision matplotlib\n",
    "%pip install async-timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043b6e36-6396-493a-afb7-d1d689de827e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping tb-nightly as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorboard as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorboard-data-server as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorboard-plugin-wit as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-io-gcs-filesystem as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping termcolor as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping terminado as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tf-estimator-nightly as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tf_keras-nightly as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tf-nightly as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "libraries_to_uninstall = [\n",
    "    \"tb-nightly==2.18.0a20240701\",\n",
    "    \"tensorboard==2.16.2\",\n",
    "    \"tensorboard-data-server==0.7.2\",\n",
    "    \"tensorboard-plugin-wit==1.8.1\",\n",
    "    \"tensorflow==2.16.2\",\n",
    "    \"tensorflow-io-gcs-filesystem==0.37.0\",\n",
    "    \"termcolor==2.4.0\",\n",
    "    \"terminado==0.18.1\",\n",
    "    \"tf-estimator-nightly==2.8.0.dev2021122109\",\n",
    "    \"tf_keras-nightly==2.18.0.dev2024070109\",\n",
    "    \"tf-nightly==2.18.0.dev20240626\"\n",
    "]\n",
    "for library in libraries_to_uninstall:\n",
    "    os.system(f\"pip uninstall -y {library}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e677784-3ccb-49de-9aa0-339fdfa926ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, WeightedRandomSampler\n",
    "import flwr as fl\n",
    "from flwr.common import Metrics\n",
    "# from flwr_datasets import FederatedDataset\n",
    "from sklearn.preprocessing import MinMaxScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import csv\n",
    "import copy\n",
    "print(fl.__version__)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fg6LBRPK-E6_",
   "metadata": {
    "id": "Fg6LBRPK-E6_",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='Brown'>***FL Constants***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZFQQhlvc-c4P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 181,
     "status": "ok",
     "timestamp": 1674161131349,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "ZFQQhlvc-c4P",
    "outputId": "b14bdc45-179f-4a11-fb12-49e6e350ee3f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DEVICE\n",
    "NUM_CLIENTS = 48\n",
    "ROUNDS = 40\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0025\n",
    "EPOCHS = 3\n",
    "DATA_GROUPS = 120\n",
    "BATCH_ROUND = 6\n",
    "SIZE_ROUND = int(BATCH_ROUND * BATCH_SIZE * NUM_CLIENTS)\n",
    "PATH = '20241215_3Epochs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gjVnC-rj9gC4",
   "metadata": {
    "id": "gjVnC-rj9gC4",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='Light Blue'>***Dataset Preparations***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6e75b-01b6-4bc4-a0e3-af41e9e15d50",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Datasets/3_Generalization_20241201_30.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m sets_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m30\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m70\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m50\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m  DATA_NUM \u001b[38;5;129;01min\u001b[39;00m sets_names:\n\u001b[0;32m----> 5\u001b[0m     TrafficData[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m][DATA_NUM]\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDatasets/3_Generalization_20241201_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATA_NUM\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQUOTE_NONE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_bad_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(DATA_NUM, TrafficData[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m][DATA_NUM]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m DATA_NUM \u001b[38;5;129;01min\u001b[39;00m TrafficData[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Datasets/3_Generalization_20241201_30.csv'"
     ]
    }
   ],
   "source": [
    "TrafficData = {}\n",
    "TrafficData['Dataset']={}\n",
    "sets_names = ['30','100','70','50','testing']\n",
    "for  DATA_NUM in sets_names:\n",
    "    TrafficData['Dataset'][DATA_NUM]=pd.read_csv(f'Datasets/3_Generalization_20241201_{DATA_NUM}.csv', low_memory=False, quoting=csv.QUOTE_NONE, on_bad_lines='skip')\n",
    "    print(DATA_NUM, TrafficData['Dataset'][DATA_NUM].shape)\n",
    "for DATA_NUM in TrafficData['Dataset']:\n",
    "    TrafficData['Dataset'][DATA_NUM]=TrafficData['Dataset'][DATA_NUM].sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2857fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(737280, 99)\n"
     ]
    }
   ],
   "source": [
    "TrafficData['Split'] = {}\n",
    "sets_training =  ['30','100','70','50']\n",
    "for DATA_NUM in sets_training:\n",
    "    TrafficData['Split'][DATA_NUM] = np.array_split(TrafficData['Dataset'][DATA_NUM],DATA_GROUPS)\n",
    "\n",
    "TrafficData['Combined'] = pd.concat([TrafficData['Split']['30'][0], TrafficData['Split']['100'][0], TrafficData['Split']['70'][0], TrafficData['Split']['50'][0]]).reset_index(drop=True)\n",
    "for GROUP in range(1, DATA_GROUPS):\n",
    "    TrafficData['Combined'] = pd.concat([TrafficData['Combined'], TrafficData['Split']['30'][GROUP], TrafficData['Split']['100'][GROUP], TrafficData['Split']['70'][GROUP], TrafficData['Split']['50'][GROUP]]).reset_index(drop=True)\n",
    "print(TrafficData['Combined'].shape)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d74ea7-0942-4de4-aa49-cb0c9032e148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(737280, 98)\n",
      "(737280,)\n",
      "(200000, 98)\n",
      "(200000,)\n"
     ]
    }
   ],
   "source": [
    "TrafficData['Train'] = {}\n",
    "TrafficData['Train']['X'] = TrafficData['Combined'].iloc[:, 0:-1]\n",
    "TrafficData['Train']['y'] = TrafficData['Combined'].iloc[:, -1]\n",
    "print(TrafficData['Train']['X'].shape)\n",
    "print(TrafficData['Train']['y'].shape)\n",
    "\n",
    "TrafficData['Test'] = {}\n",
    "TrafficData['Test']['X']=TrafficData['Dataset']['testing'].iloc[:, 0:-1]\n",
    "TrafficData['Test']['y']=TrafficData['Dataset']['testing'].iloc[:, -1]\n",
    "print(TrafficData['Test']['X'].shape)\n",
    "print(TrafficData['Test']['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f406c44d-9a2b-4831-a713-ce5a95134832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(737280, 98) (737280,)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(200000, 98) (200000,)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "model = scaler.fit(TrafficData['Train']['X'])\n",
    "TrafficData['Train']['X'] = model.transform(TrafficData['Train']['X'])\n",
    "TrafficData['Test']['X'] = model.transform(TrafficData['Test']['X'])\n",
    "\n",
    "TrafficData['Train']['X'], TrafficData['Train']['y']= np.array(TrafficData['Train']['X']), np.array(TrafficData['Train']['y'])\n",
    "print(type(TrafficData['Train']['X']),type(TrafficData['Train']['y']))\n",
    "print(TrafficData['Train']['X'].shape,TrafficData['Train']['y'].shape)\n",
    "TrafficData['Test']['X'], TrafficData['Test']['y']= np.array(TrafficData['Test']['X']), np.array(TrafficData['Test']['y'])\n",
    "print(type(TrafficData['Test']['X']),type(TrafficData['Test']['y']))\n",
    "print(TrafficData['Test']['X'].shape,TrafficData['Test']['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b2901-2954-4d8c-9185-8e6ac28ad965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4608 9216\n",
      "9216 13824\n",
      "13824 18432\n",
      "18432 23040\n",
      "23040 27648\n",
      "27648 32256\n",
      "32256 36864\n",
      "36864 41472\n",
      "41472 46080\n",
      "46080 50688\n",
      "50688 55296\n",
      "55296 59904\n",
      "59904 64512\n",
      "64512 69120\n",
      "69120 73728\n",
      "73728 78336\n",
      "78336 82944\n",
      "82944 87552\n",
      "87552 92160\n",
      "92160 96768\n",
      "96768 101376\n",
      "101376 105984\n",
      "105984 110592\n",
      "110592 115200\n",
      "115200 119808\n",
      "119808 124416\n",
      "124416 129024\n",
      "129024 133632\n",
      "133632 138240\n",
      "138240 142848\n",
      "142848 147456\n",
      "147456 152064\n",
      "152064 156672\n",
      "156672 161280\n",
      "161280 165888\n",
      "165888 170496\n",
      "170496 175104\n",
      "175104 179712\n",
      "179712 184320\n",
      "184320 188928\n",
      "188928 193536\n",
      "193536 198144\n",
      "198144 202752\n",
      "202752 207360\n",
      "207360 211968\n",
      "211968 216576\n",
      "216576 221184\n",
      "221184 225792\n",
      "225792 230400\n",
      "230400 235008\n",
      "235008 239616\n",
      "239616 244224\n",
      "244224 248832\n",
      "248832 253440\n",
      "253440 258048\n",
      "258048 262656\n",
      "262656 267264\n",
      "267264 271872\n",
      "271872 276480\n",
      "276480 281088\n",
      "281088 285696\n",
      "285696 290304\n",
      "290304 294912\n",
      "294912 299520\n",
      "299520 304128\n",
      "304128 308736\n",
      "308736 313344\n",
      "313344 317952\n",
      "317952 322560\n",
      "322560 327168\n",
      "327168 331776\n",
      "331776 336384\n",
      "336384 340992\n",
      "340992 345600\n",
      "345600 350208\n",
      "350208 354816\n",
      "354816 359424\n",
      "359424 364032\n",
      "364032 368640\n",
      "368640 373248\n",
      "373248 377856\n",
      "377856 382464\n",
      "382464 387072\n",
      "387072 391680\n",
      "391680 396288\n",
      "396288 400896\n",
      "400896 405504\n",
      "405504 410112\n",
      "410112 414720\n",
      "414720 419328\n",
      "419328 423936\n",
      "423936 428544\n",
      "428544 433152\n",
      "433152 437760\n",
      "437760 442368\n",
      "442368 446976\n",
      "446976 451584\n",
      "451584 456192\n",
      "456192 460800\n",
      "460800 465408\n",
      "465408 470016\n",
      "470016 474624\n",
      "474624 479232\n",
      "479232 483840\n",
      "483840 488448\n",
      "488448 493056\n",
      "493056 497664\n",
      "497664 502272\n",
      "502272 506880\n",
      "506880 511488\n",
      "511488 516096\n",
      "516096 520704\n",
      "520704 525312\n",
      "525312 529920\n",
      "529920 534528\n",
      "534528 539136\n",
      "539136 543744\n",
      "543744 548352\n",
      "548352 552960\n",
      "552960 557568\n",
      "557568 562176\n",
      "562176 566784\n",
      "566784 571392\n",
      "571392 576000\n",
      "576000 580608\n",
      "580608 585216\n",
      "585216 589824\n",
      "589824 594432\n",
      "594432 599040\n",
      "599040 603648\n",
      "603648 608256\n",
      "608256 612864\n",
      "612864 617472\n",
      "617472 622080\n",
      "622080 626688\n",
      "626688 631296\n",
      "631296 635904\n",
      "635904 640512\n",
      "640512 645120\n",
      "645120 649728\n",
      "649728 654336\n",
      "654336 658944\n",
      "658944 663552\n",
      "663552 668160\n",
      "668160 672768\n",
      "672768 677376\n",
      "677376 681984\n",
      "681984 686592\n",
      "686592 691200\n",
      "691200 695808\n",
      "695808 700416\n",
      "700416 705024\n",
      "705024 709632\n",
      "709632 714240\n",
      "714240 718848\n",
      "718848 723456\n",
      "723456 728064\n",
      "728064 732672\n",
      "732672 737280\n",
      "ROUND:  1 (4608, 98) (4608,)\n",
      "ROUND:  2 (4608, 98) (4608,)\n",
      "ROUND:  3 (4608, 98) (4608,)\n",
      "ROUND:  4 (4608, 98) (4608,)\n",
      "ROUND:  5 (4608, 98) (4608,)\n",
      "ROUND:  6 (4608, 98) (4608,)\n",
      "ROUND:  7 (4608, 98) (4608,)\n",
      "ROUND:  8 (4608, 98) (4608,)\n",
      "ROUND:  9 (4608, 98) (4608,)\n",
      "ROUND:  10 (4608, 98) (4608,)\n",
      "ROUND:  11 (4608, 98) (4608,)\n",
      "ROUND:  12 (4608, 98) (4608,)\n",
      "ROUND:  13 (4608, 98) (4608,)\n",
      "ROUND:  14 (4608, 98) (4608,)\n",
      "ROUND:  15 (4608, 98) (4608,)\n",
      "ROUND:  16 (4608, 98) (4608,)\n",
      "ROUND:  17 (4608, 98) (4608,)\n",
      "ROUND:  18 (4608, 98) (4608,)\n",
      "ROUND:  19 (4608, 98) (4608,)\n",
      "ROUND:  20 (4608, 98) (4608,)\n",
      "ROUND:  21 (4608, 98) (4608,)\n",
      "ROUND:  22 (4608, 98) (4608,)\n",
      "ROUND:  23 (4608, 98) (4608,)\n",
      "ROUND:  24 (4608, 98) (4608,)\n",
      "ROUND:  25 (4608, 98) (4608,)\n",
      "ROUND:  26 (4608, 98) (4608,)\n",
      "ROUND:  27 (4608, 98) (4608,)\n",
      "ROUND:  28 (4608, 98) (4608,)\n",
      "ROUND:  29 (4608, 98) (4608,)\n",
      "ROUND:  30 (4608, 98) (4608,)\n",
      "ROUND:  31 (4608, 98) (4608,)\n",
      "ROUND:  32 (4608, 98) (4608,)\n",
      "ROUND:  33 (4608, 98) (4608,)\n",
      "ROUND:  34 (4608, 98) (4608,)\n",
      "ROUND:  35 (4608, 98) (4608,)\n",
      "ROUND:  36 (4608, 98) (4608,)\n",
      "ROUND:  37 (4608, 98) (4608,)\n",
      "ROUND:  38 (4608, 98) (4608,)\n",
      "ROUND:  39 (4608, 98) (4608,)\n",
      "ROUND:  40 (4608, 98) (4608,)\n",
      "ROUND:  41 (4608, 98) (4608,)\n",
      "ROUND:  42 (4608, 98) (4608,)\n",
      "ROUND:  43 (4608, 98) (4608,)\n",
      "ROUND:  44 (4608, 98) (4608,)\n",
      "ROUND:  45 (4608, 98) (4608,)\n",
      "ROUND:  46 (4608, 98) (4608,)\n",
      "ROUND:  47 (4608, 98) (4608,)\n",
      "ROUND:  48 (4608, 98) (4608,)\n",
      "ROUND:  49 (4608, 98) (4608,)\n",
      "ROUND:  50 (4608, 98) (4608,)\n",
      "ROUND:  51 (4608, 98) (4608,)\n",
      "ROUND:  52 (4608, 98) (4608,)\n",
      "ROUND:  53 (4608, 98) (4608,)\n",
      "ROUND:  54 (4608, 98) (4608,)\n",
      "ROUND:  55 (4608, 98) (4608,)\n",
      "ROUND:  56 (4608, 98) (4608,)\n",
      "ROUND:  57 (4608, 98) (4608,)\n",
      "ROUND:  58 (4608, 98) (4608,)\n",
      "ROUND:  59 (4608, 98) (4608,)\n",
      "ROUND:  60 (4608, 98) (4608,)\n",
      "ROUND:  61 (4608, 98) (4608,)\n",
      "ROUND:  62 (4608, 98) (4608,)\n",
      "ROUND:  63 (4608, 98) (4608,)\n",
      "ROUND:  64 (4608, 98) (4608,)\n",
      "ROUND:  65 (4608, 98) (4608,)\n",
      "ROUND:  66 (4608, 98) (4608,)\n",
      "ROUND:  67 (4608, 98) (4608,)\n",
      "ROUND:  68 (4608, 98) (4608,)\n",
      "ROUND:  69 (4608, 98) (4608,)\n",
      "ROUND:  70 (4608, 98) (4608,)\n",
      "ROUND:  71 (4608, 98) (4608,)\n",
      "ROUND:  72 (4608, 98) (4608,)\n",
      "ROUND:  73 (4608, 98) (4608,)\n",
      "ROUND:  74 (4608, 98) (4608,)\n",
      "ROUND:  75 (4608, 98) (4608,)\n",
      "ROUND:  76 (4608, 98) (4608,)\n",
      "ROUND:  77 (4608, 98) (4608,)\n",
      "ROUND:  78 (4608, 98) (4608,)\n",
      "ROUND:  79 (4608, 98) (4608,)\n",
      "ROUND:  80 (4608, 98) (4608,)\n",
      "ROUND:  81 (4608, 98) (4608,)\n",
      "ROUND:  82 (4608, 98) (4608,)\n",
      "ROUND:  83 (4608, 98) (4608,)\n",
      "ROUND:  84 (4608, 98) (4608,)\n",
      "ROUND:  85 (4608, 98) (4608,)\n",
      "ROUND:  86 (4608, 98) (4608,)\n",
      "ROUND:  87 (4608, 98) (4608,)\n",
      "ROUND:  88 (4608, 98) (4608,)\n",
      "ROUND:  89 (4608, 98) (4608,)\n",
      "ROUND:  90 (4608, 98) (4608,)\n",
      "ROUND:  91 (4608, 98) (4608,)\n",
      "ROUND:  92 (4608, 98) (4608,)\n",
      "ROUND:  93 (4608, 98) (4608,)\n",
      "ROUND:  94 (4608, 98) (4608,)\n",
      "ROUND:  95 (4608, 98) (4608,)\n",
      "ROUND:  96 (4608, 98) (4608,)\n",
      "ROUND:  97 (4608, 98) (4608,)\n",
      "ROUND:  98 (4608, 98) (4608,)\n",
      "ROUND:  99 (4608, 98) (4608,)\n",
      "ROUND:  100 (4608, 98) (4608,)\n",
      "ROUND:  101 (4608, 98) (4608,)\n",
      "ROUND:  102 (4608, 98) (4608,)\n",
      "ROUND:  103 (4608, 98) (4608,)\n",
      "ROUND:  104 (4608, 98) (4608,)\n",
      "ROUND:  105 (4608, 98) (4608,)\n",
      "ROUND:  106 (4608, 98) (4608,)\n",
      "ROUND:  107 (4608, 98) (4608,)\n",
      "ROUND:  108 (4608, 98) (4608,)\n",
      "ROUND:  109 (4608, 98) (4608,)\n",
      "ROUND:  110 (4608, 98) (4608,)\n",
      "ROUND:  111 (4608, 98) (4608,)\n",
      "ROUND:  112 (4608, 98) (4608,)\n",
      "ROUND:  113 (4608, 98) (4608,)\n",
      "ROUND:  114 (4608, 98) (4608,)\n",
      "ROUND:  115 (4608, 98) (4608,)\n",
      "ROUND:  116 (4608, 98) (4608,)\n",
      "ROUND:  117 (4608, 98) (4608,)\n",
      "ROUND:  118 (4608, 98) (4608,)\n",
      "ROUND:  119 (4608, 98) (4608,)\n",
      "ROUND:  120 (4608, 98) (4608,)\n",
      "ROUND:  121 (4608, 98) (4608,)\n",
      "ROUND:  122 (4608, 98) (4608,)\n",
      "ROUND:  123 (4608, 98) (4608,)\n",
      "ROUND:  124 (4608, 98) (4608,)\n",
      "ROUND:  125 (4608, 98) (4608,)\n",
      "ROUND:  126 (4608, 98) (4608,)\n",
      "ROUND:  127 (4608, 98) (4608,)\n",
      "ROUND:  128 (4608, 98) (4608,)\n",
      "ROUND:  129 (4608, 98) (4608,)\n",
      "ROUND:  130 (4608, 98) (4608,)\n",
      "ROUND:  131 (4608, 98) (4608,)\n",
      "ROUND:  132 (4608, 98) (4608,)\n",
      "ROUND:  133 (4608, 98) (4608,)\n",
      "ROUND:  134 (4608, 98) (4608,)\n",
      "ROUND:  135 (4608, 98) (4608,)\n",
      "ROUND:  136 (4608, 98) (4608,)\n",
      "ROUND:  137 (4608, 98) (4608,)\n",
      "ROUND:  138 (4608, 98) (4608,)\n",
      "ROUND:  139 (4608, 98) (4608,)\n",
      "ROUND:  140 (4608, 98) (4608,)\n",
      "ROUND:  141 (4608, 98) (4608,)\n",
      "ROUND:  142 (4608, 98) (4608,)\n",
      "ROUND:  143 (4608, 98) (4608,)\n",
      "ROUND:  144 (4608, 98) (4608,)\n",
      "ROUND:  145 (4608, 98) (4608,)\n",
      "ROUND:  146 (4608, 98) (4608,)\n",
      "ROUND:  147 (4608, 98) (4608,)\n",
      "ROUND:  148 (4608, 98) (4608,)\n",
      "ROUND:  149 (4608, 98) (4608,)\n",
      "ROUND:  150 (4608, 98) (4608,)\n",
      "ROUND:  151 (4608, 98) (4608,)\n",
      "ROUND:  152 (4608, 98) (4608,)\n",
      "ROUND:  153 (4608, 98) (4608,)\n",
      "ROUND:  154 (4608, 98) (4608,)\n",
      "ROUND:  155 (4608, 98) (4608,)\n",
      "ROUND:  156 (4608, 98) (4608,)\n",
      "ROUND:  157 (4608, 98) (4608,)\n",
      "ROUND:  158 (4608, 98) (4608,)\n",
      "ROUND:  159 (4608, 98) (4608,)\n",
      "ROUND:  160 (4608, 98) (4608,)\n",
      "741888 4608\n"
     ]
    }
   ],
   "source": [
    "TrafficData['ROUNDS']={}\n",
    "for ROUND in range(1, ROUNDS+1):\n",
    "    TrafficData['ROUNDS'][ROUND]={}\n",
    "\n",
    "SIZE_Demo = SIZE_ROUND\n",
    "for ROUND in range(1,ROUNDS+1):\n",
    "    if ROUND == 1:\n",
    "        TrafficData['ROUNDS'][ROUND]['X']= TrafficData['Train']['X'][:SIZE_Demo]\n",
    "        TrafficData['ROUNDS'][ROUND]['y']= TrafficData['Train']['y'][:SIZE_Demo]\n",
    "    else:\n",
    "        print((SIZE_Demo - SIZE_ROUND),SIZE_Demo)\n",
    "        TrafficData['ROUNDS'][ROUND]['X']= TrafficData['Train']['X'][(SIZE_Demo - SIZE_ROUND):SIZE_Demo]\n",
    "        TrafficData['ROUNDS'][ROUND]['y']= TrafficData['Train']['y'][(SIZE_Demo - SIZE_ROUND):SIZE_Demo]\n",
    "    SIZE_Demo = SIZE_Demo + SIZE_ROUND\n",
    "for ROUND in TrafficData['ROUNDS']:\n",
    "    print(\"ROUND: \", ROUND, TrafficData['ROUNDS'][ROUND]['X'].shape, TrafficData['ROUNDS'][ROUND]['y'].shape)\n",
    "print(SIZE_Demo, SIZE_ROUND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f73dc-662e-441e-9361-32fbca07af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ade83a-b7fd-450d-82cb-0649af87fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrafficData['trainsets']={}\n",
    "for ROUND in range(1, ROUNDS+1):\n",
    "    TrafficData['trainsets'][ROUND]= ClassifierDataset(torch.from_numpy(TrafficData['ROUNDS'][ROUND]['X']).float(), torch.from_numpy(TrafficData['ROUNDS'][ROUND]['y']).long())\n",
    "TrafficData['testset'] = ClassifierDataset(torch.from_numpy(TrafficData['Test']['X']).float(), torch.from_numpy(TrafficData['Test']['y']).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d6a1f-95bc-4173-a8b4-8961e3d2875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train(numberofclients, ROUND):    \n",
    "    portion_size = int(BATCH_ROUND*BATCH_SIZE)\n",
    "    num_portions = int(NUM_CLIENTS)\n",
    "    portion_indices = []\n",
    "    for i in range(num_portions):\n",
    "        start_idx = i * portion_size\n",
    "        end_idx = (i + 1) * portion_size\n",
    "        portion_indices.append(list(range(start_idx, min(end_idx, SIZE_ROUND))))\n",
    "    portion_datasets = [Subset(TrafficData['trainsets'][ROUND], indices) for indices in portion_indices]\n",
    "    portion_loaders = [DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False) for dataset in portion_datasets]             \n",
    "    return portion_loaders\n",
    "def load_test(numberofclients):    \n",
    "    testloader = DataLoader(TrafficData['testset'], batch_size=BATCH_SIZE, shuffle=False)\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e2e5d8-60da-4260-80bc-4850460279c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataloaders = {}\n",
    "for ROUND in range(1, ROUNDS+1):\n",
    "    Dataloaders[ROUND] = load_train(NUM_CLIENTS, ROUND)\n",
    "Dataloaders['Test'] = load_test(NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6455bfc-e8b5-4d01-84d6-3f484a2c8ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 size: 64\n",
      "Batch 2 size: 64\n",
      "Batch 3 size: 64\n",
      "Batch 4 size: 64\n",
      "Batch 5 size: 64\n",
      "Batch 6 size: 64\n",
      "Batch 7 size: 64\n",
      "Batch 8 size: 64\n",
      "Batch 9 size: 64\n",
      "Batch 10 size: 64\n",
      "Batch 11 size: 64\n",
      "Batch 12 size: 64\n",
      "Batch 13 size: 64\n",
      "Batch 14 size: 64\n",
      "Batch 15 size: 64\n",
      "Batch 16 size: 64\n",
      "Batch 17 size: 64\n",
      "Batch 18 size: 64\n",
      "Batch 19 size: 64\n",
      "Batch 20 size: 64\n",
      "Batch 21 size: 64\n",
      "Batch 22 size: 64\n",
      "Batch 23 size: 64\n",
      "Batch 24 size: 64\n",
      "Batch 25 size: 64\n",
      "Batch 26 size: 64\n",
      "Batch 27 size: 64\n",
      "Batch 28 size: 64\n",
      "Batch 29 size: 64\n",
      "Batch 30 size: 64\n",
      "Batch 31 size: 64\n",
      "Batch 32 size: 64\n",
      "Batch 33 size: 64\n",
      "Batch 34 size: 64\n",
      "Batch 35 size: 64\n",
      "Batch 36 size: 64\n",
      "Batch 37 size: 64\n",
      "Batch 38 size: 64\n",
      "Batch 39 size: 64\n",
      "Batch 40 size: 64\n",
      "Batch 41 size: 64\n",
      "Batch 42 size: 64\n",
      "Batch 43 size: 64\n",
      "Batch 44 size: 64\n",
      "Batch 45 size: 64\n",
      "Batch 46 size: 64\n",
      "Batch 47 size: 64\n",
      "Batch 48 size: 64\n",
      "Batch 49 size: 64\n",
      "Batch 50 size: 64\n",
      "Batch 51 size: 64\n",
      "Batch 52 size: 64\n",
      "Batch 53 size: 64\n",
      "Batch 54 size: 64\n",
      "Batch 55 size: 64\n",
      "Batch 56 size: 64\n",
      "Batch 57 size: 64\n",
      "Batch 58 size: 64\n",
      "Batch 59 size: 64\n",
      "Batch 60 size: 64\n",
      "Batch 61 size: 64\n",
      "Batch 62 size: 64\n",
      "Batch 63 size: 64\n",
      "Batch 64 size: 64\n",
      "Batch 65 size: 64\n",
      "Batch 66 size: 64\n",
      "Batch 67 size: 64\n",
      "Batch 68 size: 64\n",
      "Batch 69 size: 64\n",
      "Batch 70 size: 64\n",
      "Batch 71 size: 64\n",
      "Batch 72 size: 64\n",
      "Batch 73 size: 64\n",
      "Batch 74 size: 64\n",
      "Batch 75 size: 64\n",
      "Batch 76 size: 64\n",
      "Batch 77 size: 64\n",
      "Batch 78 size: 64\n",
      "Batch 79 size: 64\n",
      "Batch 80 size: 64\n",
      "Batch 81 size: 64\n",
      "Batch 82 size: 64\n",
      "Batch 83 size: 64\n",
      "Batch 84 size: 64\n",
      "Batch 85 size: 64\n",
      "Batch 86 size: 64\n",
      "Batch 87 size: 64\n",
      "Batch 88 size: 64\n",
      "Batch 89 size: 64\n",
      "Batch 90 size: 64\n",
      "Batch 91 size: 64\n",
      "Batch 92 size: 64\n",
      "Batch 93 size: 64\n",
      "Batch 94 size: 64\n",
      "Batch 95 size: 64\n",
      "Batch 96 size: 64\n",
      "Batch 97 size: 64\n",
      "Batch 98 size: 64\n",
      "Batch 99 size: 64\n",
      "Batch 100 size: 64\n",
      "Batch 101 size: 64\n",
      "Batch 102 size: 64\n",
      "Batch 103 size: 64\n",
      "Batch 104 size: 64\n",
      "Batch 105 size: 64\n",
      "Batch 106 size: 64\n",
      "Batch 107 size: 64\n",
      "Batch 108 size: 64\n",
      "Batch 109 size: 64\n",
      "Batch 110 size: 64\n",
      "Batch 111 size: 64\n",
      "Batch 112 size: 64\n",
      "Batch 113 size: 64\n",
      "Batch 114 size: 64\n",
      "Batch 115 size: 64\n",
      "Batch 116 size: 64\n",
      "Batch 117 size: 64\n",
      "Batch 118 size: 64\n",
      "Batch 119 size: 64\n",
      "Batch 120 size: 64\n",
      "Batch 121 size: 64\n",
      "Batch 122 size: 64\n",
      "Batch 123 size: 64\n",
      "Batch 124 size: 64\n",
      "Batch 125 size: 64\n",
      "Batch 126 size: 64\n",
      "Batch 127 size: 64\n",
      "Batch 128 size: 64\n",
      "Batch 129 size: 64\n",
      "Batch 130 size: 64\n",
      "Batch 131 size: 64\n",
      "Batch 132 size: 64\n",
      "Batch 133 size: 64\n",
      "Batch 134 size: 64\n",
      "Batch 135 size: 64\n",
      "Batch 136 size: 64\n",
      "Batch 137 size: 64\n",
      "Batch 138 size: 64\n",
      "Batch 139 size: 64\n",
      "Batch 140 size: 64\n",
      "Batch 141 size: 64\n",
      "Batch 142 size: 64\n",
      "Batch 143 size: 64\n",
      "Batch 144 size: 64\n",
      "Batch 145 size: 64\n",
      "Batch 146 size: 64\n",
      "Batch 147 size: 64\n",
      "Batch 148 size: 64\n",
      "Batch 149 size: 64\n",
      "Batch 150 size: 64\n",
      "Batch 151 size: 64\n",
      "Batch 152 size: 64\n",
      "Batch 153 size: 64\n",
      "Batch 154 size: 64\n",
      "Batch 155 size: 64\n",
      "Batch 156 size: 64\n",
      "Batch 157 size: 64\n",
      "Batch 158 size: 64\n",
      "Batch 159 size: 64\n",
      "Batch 160 size: 64\n",
      "Batch 161 size: 64\n",
      "Batch 162 size: 64\n",
      "Batch 163 size: 64\n",
      "Batch 164 size: 64\n",
      "Batch 165 size: 64\n",
      "Batch 166 size: 64\n",
      "Batch 167 size: 64\n",
      "Batch 168 size: 64\n",
      "Batch 169 size: 64\n",
      "Batch 170 size: 64\n",
      "Batch 171 size: 64\n",
      "Batch 172 size: 64\n",
      "Batch 173 size: 64\n",
      "Batch 174 size: 64\n",
      "Batch 175 size: 64\n",
      "Batch 176 size: 64\n",
      "Batch 177 size: 64\n",
      "Batch 178 size: 64\n",
      "Batch 179 size: 64\n",
      "Batch 180 size: 64\n",
      "Batch 181 size: 64\n",
      "Batch 182 size: 64\n",
      "Batch 183 size: 64\n",
      "Batch 184 size: 64\n",
      "Batch 185 size: 64\n",
      "Batch 186 size: 64\n",
      "Batch 187 size: 64\n",
      "Batch 188 size: 64\n",
      "Batch 189 size: 64\n",
      "Batch 190 size: 64\n",
      "Batch 191 size: 64\n",
      "Batch 192 size: 64\n",
      "Batch 193 size: 64\n",
      "Batch 194 size: 64\n",
      "Batch 195 size: 64\n",
      "Batch 196 size: 64\n",
      "Batch 197 size: 64\n",
      "Batch 198 size: 64\n",
      "Batch 199 size: 64\n",
      "Batch 200 size: 64\n",
      "Batch 201 size: 64\n",
      "Batch 202 size: 64\n",
      "Batch 203 size: 64\n",
      "Batch 204 size: 64\n",
      "Batch 205 size: 64\n",
      "Batch 206 size: 64\n",
      "Batch 207 size: 64\n",
      "Batch 208 size: 64\n",
      "Batch 209 size: 64\n",
      "Batch 210 size: 64\n",
      "Batch 211 size: 64\n",
      "Batch 212 size: 64\n",
      "Batch 213 size: 64\n",
      "Batch 214 size: 64\n",
      "Batch 215 size: 64\n",
      "Batch 216 size: 64\n",
      "Batch 217 size: 64\n",
      "Batch 218 size: 64\n",
      "Batch 219 size: 64\n",
      "Batch 220 size: 64\n",
      "Batch 221 size: 64\n",
      "Batch 222 size: 64\n",
      "Batch 223 size: 64\n",
      "Batch 224 size: 64\n",
      "Batch 225 size: 64\n",
      "Batch 226 size: 64\n",
      "Batch 227 size: 64\n",
      "Batch 228 size: 64\n",
      "Batch 229 size: 64\n",
      "Batch 230 size: 64\n",
      "Batch 231 size: 64\n",
      "Batch 232 size: 64\n",
      "Batch 233 size: 64\n",
      "Batch 234 size: 64\n",
      "Batch 235 size: 64\n",
      "Batch 236 size: 64\n",
      "Batch 237 size: 64\n",
      "Batch 238 size: 64\n",
      "Batch 239 size: 64\n",
      "Batch 240 size: 64\n",
      "Batch 241 size: 64\n",
      "Batch 242 size: 64\n",
      "Batch 243 size: 64\n",
      "Batch 244 size: 64\n",
      "Batch 245 size: 64\n",
      "Batch 246 size: 64\n",
      "Batch 247 size: 64\n",
      "Batch 248 size: 64\n",
      "Batch 249 size: 64\n",
      "Batch 250 size: 64\n",
      "Batch 251 size: 64\n",
      "Batch 252 size: 64\n",
      "Batch 253 size: 64\n",
      "Batch 254 size: 64\n",
      "Batch 255 size: 64\n",
      "Batch 256 size: 64\n",
      "Batch 257 size: 64\n",
      "Batch 258 size: 64\n",
      "Batch 259 size: 64\n",
      "Batch 260 size: 64\n",
      "Batch 261 size: 64\n",
      "Batch 262 size: 64\n",
      "Batch 263 size: 64\n",
      "Batch 264 size: 64\n",
      "Batch 265 size: 64\n",
      "Batch 266 size: 64\n",
      "Batch 267 size: 64\n",
      "Batch 268 size: 64\n",
      "Batch 269 size: 64\n",
      "Batch 270 size: 64\n",
      "Batch 271 size: 64\n",
      "Batch 272 size: 64\n",
      "Batch 273 size: 64\n",
      "Batch 274 size: 64\n",
      "Batch 275 size: 64\n",
      "Batch 276 size: 64\n",
      "Batch 277 size: 64\n",
      "Batch 278 size: 64\n",
      "Batch 279 size: 64\n",
      "Batch 280 size: 64\n",
      "Batch 281 size: 64\n",
      "Batch 282 size: 64\n",
      "Batch 283 size: 64\n",
      "Batch 284 size: 64\n",
      "Batch 285 size: 64\n",
      "Batch 286 size: 64\n",
      "Batch 287 size: 64\n",
      "Batch 288 size: 64\n",
      "Batch 289 size: 64\n",
      "Batch 290 size: 64\n",
      "Batch 291 size: 64\n",
      "Batch 292 size: 64\n",
      "Batch 293 size: 64\n",
      "Batch 294 size: 64\n",
      "Batch 295 size: 64\n",
      "Batch 296 size: 64\n",
      "Batch 297 size: 64\n",
      "Batch 298 size: 64\n",
      "Batch 299 size: 64\n",
      "Batch 300 size: 64\n",
      "Batch 301 size: 64\n",
      "Batch 302 size: 64\n",
      "Batch 303 size: 64\n",
      "Batch 304 size: 64\n",
      "Batch 305 size: 64\n",
      "Batch 306 size: 64\n",
      "Batch 307 size: 64\n",
      "Batch 308 size: 64\n",
      "Batch 309 size: 64\n",
      "Batch 310 size: 64\n",
      "Batch 311 size: 64\n",
      "Batch 312 size: 64\n",
      "Batch 313 size: 64\n",
      "Batch 314 size: 64\n",
      "Batch 315 size: 64\n",
      "Batch 316 size: 64\n",
      "Batch 317 size: 64\n",
      "Batch 318 size: 64\n",
      "Batch 319 size: 64\n",
      "Batch 320 size: 64\n",
      "Batch 321 size: 64\n",
      "Batch 322 size: 64\n",
      "Batch 323 size: 64\n",
      "Batch 324 size: 64\n",
      "Batch 325 size: 64\n",
      "Batch 326 size: 64\n",
      "Batch 327 size: 64\n",
      "Batch 328 size: 64\n",
      "Batch 329 size: 64\n",
      "Batch 330 size: 64\n",
      "Batch 331 size: 64\n",
      "Batch 332 size: 64\n",
      "Batch 333 size: 64\n",
      "Batch 334 size: 64\n",
      "Batch 335 size: 64\n",
      "Batch 336 size: 64\n",
      "Batch 337 size: 64\n",
      "Batch 338 size: 64\n",
      "Batch 339 size: 64\n",
      "Batch 340 size: 64\n",
      "Batch 341 size: 64\n",
      "Batch 342 size: 64\n",
      "Batch 343 size: 64\n",
      "Batch 344 size: 64\n",
      "Batch 345 size: 64\n",
      "Batch 346 size: 64\n",
      "Batch 347 size: 64\n",
      "Batch 348 size: 64\n",
      "Batch 349 size: 64\n",
      "Batch 350 size: 64\n",
      "Batch 351 size: 64\n",
      "Batch 352 size: 64\n",
      "Batch 353 size: 64\n",
      "Batch 354 size: 64\n",
      "Batch 355 size: 64\n",
      "Batch 356 size: 64\n",
      "Batch 357 size: 64\n",
      "Batch 358 size: 64\n",
      "Batch 359 size: 64\n",
      "Batch 360 size: 64\n",
      "Batch 361 size: 64\n",
      "Batch 362 size: 64\n",
      "Batch 363 size: 64\n",
      "Batch 364 size: 64\n",
      "Batch 365 size: 64\n",
      "Batch 366 size: 64\n",
      "Batch 367 size: 64\n",
      "Batch 368 size: 64\n",
      "Batch 369 size: 64\n",
      "Batch 370 size: 64\n",
      "Batch 371 size: 64\n",
      "Batch 372 size: 64\n",
      "Batch 373 size: 64\n",
      "Batch 374 size: 64\n",
      "Batch 375 size: 64\n",
      "Batch 376 size: 64\n",
      "Batch 377 size: 64\n",
      "Batch 378 size: 64\n",
      "Batch 379 size: 64\n",
      "Batch 380 size: 64\n",
      "Batch 381 size: 64\n",
      "Batch 382 size: 64\n",
      "Batch 383 size: 64\n",
      "Batch 384 size: 64\n",
      "Batch 385 size: 64\n",
      "Batch 386 size: 64\n",
      "Batch 387 size: 64\n",
      "Batch 388 size: 64\n",
      "Batch 389 size: 64\n",
      "Batch 390 size: 64\n",
      "Batch 391 size: 64\n",
      "Batch 392 size: 64\n",
      "Batch 393 size: 64\n",
      "Batch 394 size: 64\n",
      "Batch 395 size: 64\n",
      "Batch 396 size: 64\n",
      "Batch 397 size: 64\n",
      "Batch 398 size: 64\n",
      "Batch 399 size: 64\n",
      "Batch 400 size: 64\n",
      "Batch 401 size: 64\n",
      "Batch 402 size: 64\n",
      "Batch 403 size: 64\n",
      "Batch 404 size: 64\n",
      "Batch 405 size: 64\n",
      "Batch 406 size: 64\n",
      "Batch 407 size: 64\n",
      "Batch 408 size: 64\n",
      "Batch 409 size: 64\n",
      "Batch 410 size: 64\n",
      "Batch 411 size: 64\n",
      "Batch 412 size: 64\n",
      "Batch 413 size: 64\n",
      "Batch 414 size: 64\n",
      "Batch 415 size: 64\n",
      "Batch 416 size: 64\n",
      "Batch 417 size: 64\n",
      "Batch 418 size: 64\n",
      "Batch 419 size: 64\n",
      "Batch 420 size: 64\n",
      "Batch 421 size: 64\n",
      "Batch 422 size: 64\n",
      "Batch 423 size: 64\n",
      "Batch 424 size: 64\n",
      "Batch 425 size: 64\n",
      "Batch 426 size: 64\n",
      "Batch 427 size: 64\n",
      "Batch 428 size: 64\n",
      "Batch 429 size: 64\n",
      "Batch 430 size: 64\n",
      "Batch 431 size: 64\n",
      "Batch 432 size: 64\n",
      "Batch 433 size: 64\n",
      "Batch 434 size: 64\n",
      "Batch 435 size: 64\n",
      "Batch 436 size: 64\n",
      "Batch 437 size: 64\n",
      "Batch 438 size: 64\n",
      "Batch 439 size: 64\n",
      "Batch 440 size: 64\n",
      "Batch 441 size: 64\n",
      "Batch 442 size: 64\n",
      "Batch 443 size: 64\n",
      "Batch 444 size: 64\n",
      "Batch 445 size: 64\n",
      "Batch 446 size: 64\n",
      "Batch 447 size: 64\n",
      "Batch 448 size: 64\n",
      "Batch 449 size: 64\n",
      "Batch 450 size: 64\n",
      "Batch 451 size: 64\n",
      "Batch 452 size: 64\n",
      "Batch 453 size: 64\n",
      "Batch 454 size: 64\n",
      "Batch 455 size: 64\n",
      "Batch 456 size: 64\n",
      "Batch 457 size: 64\n",
      "Batch 458 size: 64\n",
      "Batch 459 size: 64\n",
      "Batch 460 size: 64\n",
      "Batch 461 size: 64\n",
      "Batch 462 size: 64\n",
      "Batch 463 size: 64\n",
      "Batch 464 size: 64\n",
      "Batch 465 size: 64\n",
      "Batch 466 size: 64\n",
      "Batch 467 size: 64\n",
      "Batch 468 size: 64\n",
      "Batch 469 size: 64\n",
      "Batch 470 size: 64\n",
      "Batch 471 size: 64\n",
      "Batch 472 size: 64\n",
      "Batch 473 size: 64\n",
      "Batch 474 size: 64\n",
      "Batch 475 size: 64\n",
      "Batch 476 size: 64\n",
      "Batch 477 size: 64\n",
      "Batch 478 size: 64\n",
      "Batch 479 size: 64\n",
      "Batch 480 size: 64\n",
      "Batch 481 size: 64\n",
      "Batch 482 size: 64\n",
      "Batch 483 size: 64\n",
      "Batch 484 size: 64\n",
      "Batch 485 size: 64\n",
      "Batch 486 size: 64\n",
      "Batch 487 size: 64\n",
      "Batch 488 size: 64\n",
      "Batch 489 size: 64\n",
      "Batch 490 size: 64\n",
      "Batch 491 size: 64\n",
      "Batch 492 size: 64\n",
      "Batch 493 size: 64\n",
      "Batch 494 size: 64\n",
      "Batch 495 size: 64\n",
      "Batch 496 size: 64\n",
      "Batch 497 size: 64\n",
      "Batch 498 size: 64\n",
      "Batch 499 size: 64\n",
      "Batch 500 size: 64\n",
      "Batch 501 size: 64\n",
      "Batch 502 size: 64\n",
      "Batch 503 size: 64\n",
      "Batch 504 size: 64\n",
      "Batch 505 size: 64\n",
      "Batch 506 size: 64\n",
      "Batch 507 size: 64\n",
      "Batch 508 size: 64\n",
      "Batch 509 size: 64\n",
      "Batch 510 size: 64\n",
      "Batch 511 size: 64\n",
      "Batch 512 size: 64\n",
      "Batch 513 size: 64\n",
      "Batch 514 size: 64\n",
      "Batch 515 size: 64\n",
      "Batch 516 size: 64\n",
      "Batch 517 size: 64\n",
      "Batch 518 size: 64\n",
      "Batch 519 size: 64\n",
      "Batch 520 size: 64\n",
      "Batch 521 size: 64\n",
      "Batch 522 size: 64\n",
      "Batch 523 size: 64\n",
      "Batch 524 size: 64\n",
      "Batch 525 size: 64\n",
      "Batch 526 size: 64\n",
      "Batch 527 size: 64\n",
      "Batch 528 size: 64\n",
      "Batch 529 size: 64\n",
      "Batch 530 size: 64\n",
      "Batch 531 size: 64\n",
      "Batch 532 size: 64\n",
      "Batch 533 size: 64\n",
      "Batch 534 size: 64\n",
      "Batch 535 size: 64\n",
      "Batch 536 size: 64\n",
      "Batch 537 size: 64\n",
      "Batch 538 size: 64\n",
      "Batch 539 size: 64\n",
      "Batch 540 size: 64\n",
      "Batch 541 size: 64\n",
      "Batch 542 size: 64\n",
      "Batch 543 size: 64\n",
      "Batch 544 size: 64\n",
      "Batch 545 size: 64\n",
      "Batch 546 size: 64\n",
      "Batch 547 size: 64\n",
      "Batch 548 size: 64\n",
      "Batch 549 size: 64\n",
      "Batch 550 size: 64\n",
      "Batch 551 size: 64\n",
      "Batch 552 size: 64\n",
      "Batch 553 size: 64\n",
      "Batch 554 size: 64\n",
      "Batch 555 size: 64\n",
      "Batch 556 size: 64\n",
      "Batch 557 size: 64\n",
      "Batch 558 size: 64\n",
      "Batch 559 size: 64\n",
      "Batch 560 size: 64\n",
      "Batch 561 size: 64\n",
      "Batch 562 size: 64\n",
      "Batch 563 size: 64\n",
      "Batch 564 size: 64\n",
      "Batch 565 size: 64\n",
      "Batch 566 size: 64\n",
      "Batch 567 size: 64\n",
      "Batch 568 size: 64\n",
      "Batch 569 size: 64\n",
      "Batch 570 size: 64\n",
      "Batch 571 size: 64\n",
      "Batch 572 size: 64\n",
      "Batch 573 size: 64\n",
      "Batch 574 size: 64\n",
      "Batch 575 size: 64\n",
      "Batch 576 size: 64\n",
      "Batch 577 size: 64\n",
      "Batch 578 size: 64\n",
      "Batch 579 size: 64\n",
      "Batch 580 size: 64\n",
      "Batch 581 size: 64\n",
      "Batch 582 size: 64\n",
      "Batch 583 size: 64\n",
      "Batch 584 size: 64\n",
      "Batch 585 size: 64\n",
      "Batch 586 size: 64\n",
      "Batch 587 size: 64\n",
      "Batch 588 size: 64\n",
      "Batch 589 size: 64\n",
      "Batch 590 size: 64\n",
      "Batch 591 size: 64\n",
      "Batch 592 size: 64\n",
      "Batch 593 size: 64\n",
      "Batch 594 size: 64\n",
      "Batch 595 size: 64\n",
      "Batch 596 size: 64\n",
      "Batch 597 size: 64\n",
      "Batch 598 size: 64\n",
      "Batch 599 size: 64\n",
      "Batch 600 size: 64\n",
      "Batch 601 size: 64\n",
      "Batch 602 size: 64\n",
      "Batch 603 size: 64\n",
      "Batch 604 size: 64\n",
      "Batch 605 size: 64\n",
      "Batch 606 size: 64\n",
      "Batch 607 size: 64\n",
      "Batch 608 size: 64\n",
      "Batch 609 size: 64\n",
      "Batch 610 size: 64\n",
      "Batch 611 size: 64\n",
      "Batch 612 size: 64\n",
      "Batch 613 size: 64\n",
      "Batch 614 size: 64\n",
      "Batch 615 size: 64\n",
      "Batch 616 size: 64\n",
      "Batch 617 size: 64\n",
      "Batch 618 size: 64\n",
      "Batch 619 size: 64\n",
      "Batch 620 size: 64\n",
      "Batch 621 size: 64\n",
      "Batch 622 size: 64\n",
      "Batch 623 size: 64\n",
      "Batch 624 size: 64\n",
      "Batch 625 size: 64\n",
      "Batch 626 size: 64\n",
      "Batch 627 size: 64\n",
      "Batch 628 size: 64\n",
      "Batch 629 size: 64\n",
      "Batch 630 size: 64\n",
      "Batch 631 size: 64\n",
      "Batch 632 size: 64\n",
      "Batch 633 size: 64\n",
      "Batch 634 size: 64\n",
      "Batch 635 size: 64\n",
      "Batch 636 size: 64\n",
      "Batch 637 size: 64\n",
      "Batch 638 size: 64\n",
      "Batch 639 size: 64\n",
      "Batch 640 size: 64\n",
      "Batch 641 size: 64\n",
      "Batch 642 size: 64\n",
      "Batch 643 size: 64\n",
      "Batch 644 size: 64\n",
      "Batch 645 size: 64\n",
      "Batch 646 size: 64\n",
      "Batch 647 size: 64\n",
      "Batch 648 size: 64\n",
      "Batch 649 size: 64\n",
      "Batch 650 size: 64\n",
      "Batch 651 size: 64\n",
      "Batch 652 size: 64\n",
      "Batch 653 size: 64\n",
      "Batch 654 size: 64\n",
      "Batch 655 size: 64\n",
      "Batch 656 size: 64\n",
      "Batch 657 size: 64\n",
      "Batch 658 size: 64\n",
      "Batch 659 size: 64\n",
      "Batch 660 size: 64\n",
      "Batch 661 size: 64\n",
      "Batch 662 size: 64\n",
      "Batch 663 size: 64\n",
      "Batch 664 size: 64\n",
      "Batch 665 size: 64\n",
      "Batch 666 size: 64\n",
      "Batch 667 size: 64\n",
      "Batch 668 size: 64\n",
      "Batch 669 size: 64\n",
      "Batch 670 size: 64\n",
      "Batch 671 size: 64\n",
      "Batch 672 size: 64\n",
      "Batch 673 size: 64\n",
      "Batch 674 size: 64\n",
      "Batch 675 size: 64\n",
      "Batch 676 size: 64\n",
      "Batch 677 size: 64\n",
      "Batch 678 size: 64\n",
      "Batch 679 size: 64\n",
      "Batch 680 size: 64\n",
      "Batch 681 size: 64\n",
      "Batch 682 size: 64\n",
      "Batch 683 size: 64\n",
      "Batch 684 size: 64\n",
      "Batch 685 size: 64\n",
      "Batch 686 size: 64\n",
      "Batch 687 size: 64\n",
      "Batch 688 size: 64\n",
      "Batch 689 size: 64\n",
      "Batch 690 size: 64\n",
      "Batch 691 size: 64\n",
      "Batch 692 size: 64\n",
      "Batch 693 size: 64\n",
      "Batch 694 size: 64\n",
      "Batch 695 size: 64\n",
      "Batch 696 size: 64\n",
      "Batch 697 size: 64\n",
      "Batch 698 size: 64\n",
      "Batch 699 size: 64\n",
      "Batch 700 size: 64\n",
      "Batch 701 size: 64\n",
      "Batch 702 size: 64\n",
      "Batch 703 size: 64\n",
      "Batch 704 size: 64\n",
      "Batch 705 size: 64\n",
      "Batch 706 size: 64\n",
      "Batch 707 size: 64\n",
      "Batch 708 size: 64\n",
      "Batch 709 size: 64\n",
      "Batch 710 size: 64\n",
      "Batch 711 size: 64\n",
      "Batch 712 size: 64\n",
      "Batch 713 size: 64\n",
      "Batch 714 size: 64\n",
      "Batch 715 size: 64\n",
      "Batch 716 size: 64\n",
      "Batch 717 size: 64\n",
      "Batch 718 size: 64\n",
      "Batch 719 size: 64\n",
      "Batch 720 size: 64\n",
      "Batch 721 size: 64\n",
      "Batch 722 size: 64\n",
      "Batch 723 size: 64\n",
      "Batch 724 size: 64\n",
      "Batch 725 size: 64\n",
      "Batch 726 size: 64\n",
      "Batch 727 size: 64\n",
      "Batch 728 size: 64\n",
      "Batch 729 size: 64\n",
      "Batch 730 size: 64\n",
      "Batch 731 size: 64\n",
      "Batch 732 size: 64\n",
      "Batch 733 size: 64\n",
      "Batch 734 size: 64\n",
      "Batch 735 size: 64\n",
      "Batch 736 size: 64\n",
      "Batch 737 size: 64\n",
      "Batch 738 size: 64\n",
      "Batch 739 size: 64\n",
      "Batch 740 size: 64\n",
      "Batch 741 size: 64\n",
      "Batch 742 size: 64\n",
      "Batch 743 size: 64\n",
      "Batch 744 size: 64\n",
      "Batch 745 size: 64\n",
      "Batch 746 size: 64\n",
      "Batch 747 size: 64\n",
      "Batch 748 size: 64\n",
      "Batch 749 size: 64\n",
      "Batch 750 size: 64\n",
      "Batch 751 size: 64\n",
      "Batch 752 size: 64\n",
      "Batch 753 size: 64\n",
      "Batch 754 size: 64\n",
      "Batch 755 size: 64\n",
      "Batch 756 size: 64\n",
      "Batch 757 size: 64\n",
      "Batch 758 size: 64\n",
      "Batch 759 size: 64\n",
      "Batch 760 size: 64\n",
      "Batch 761 size: 64\n",
      "Batch 762 size: 64\n",
      "Batch 763 size: 64\n",
      "Batch 764 size: 64\n",
      "Batch 765 size: 64\n",
      "Batch 766 size: 64\n",
      "Batch 767 size: 64\n",
      "Batch 768 size: 64\n",
      "Batch 769 size: 64\n",
      "Batch 770 size: 64\n",
      "Batch 771 size: 64\n",
      "Batch 772 size: 64\n",
      "Batch 773 size: 64\n",
      "Batch 774 size: 64\n",
      "Batch 775 size: 64\n",
      "Batch 776 size: 64\n",
      "Batch 777 size: 64\n",
      "Batch 778 size: 64\n",
      "Batch 779 size: 64\n",
      "Batch 780 size: 64\n",
      "Batch 781 size: 64\n",
      "Batch 782 size: 64\n",
      "Batch 783 size: 64\n",
      "Batch 784 size: 64\n",
      "Batch 785 size: 64\n",
      "Batch 786 size: 64\n",
      "Batch 787 size: 64\n",
      "Batch 788 size: 64\n",
      "Batch 789 size: 64\n",
      "Batch 790 size: 64\n",
      "Batch 791 size: 64\n",
      "Batch 792 size: 64\n",
      "Batch 793 size: 64\n",
      "Batch 794 size: 64\n",
      "Batch 795 size: 64\n",
      "Batch 796 size: 64\n",
      "Batch 797 size: 64\n",
      "Batch 798 size: 64\n",
      "Batch 799 size: 64\n",
      "Batch 800 size: 64\n",
      "Batch 801 size: 64\n",
      "Batch 802 size: 64\n",
      "Batch 803 size: 64\n",
      "Batch 804 size: 64\n",
      "Batch 805 size: 64\n",
      "Batch 806 size: 64\n",
      "Batch 807 size: 64\n",
      "Batch 808 size: 64\n",
      "Batch 809 size: 64\n",
      "Batch 810 size: 64\n",
      "Batch 811 size: 64\n",
      "Batch 812 size: 64\n",
      "Batch 813 size: 64\n",
      "Batch 814 size: 64\n",
      "Batch 815 size: 64\n",
      "Batch 816 size: 64\n",
      "Batch 817 size: 64\n",
      "Batch 818 size: 64\n",
      "Batch 819 size: 64\n",
      "Batch 820 size: 64\n",
      "Batch 821 size: 64\n",
      "Batch 822 size: 64\n",
      "Batch 823 size: 64\n",
      "Batch 824 size: 64\n",
      "Batch 825 size: 64\n",
      "Batch 826 size: 64\n",
      "Batch 827 size: 64\n",
      "Batch 828 size: 64\n",
      "Batch 829 size: 64\n",
      "Batch 830 size: 64\n",
      "Batch 831 size: 64\n",
      "Batch 832 size: 64\n",
      "Batch 833 size: 64\n",
      "Batch 834 size: 64\n",
      "Batch 835 size: 64\n",
      "Batch 836 size: 64\n",
      "Batch 837 size: 64\n",
      "Batch 838 size: 64\n",
      "Batch 839 size: 64\n",
      "Batch 840 size: 64\n",
      "Batch 841 size: 64\n",
      "Batch 842 size: 64\n",
      "Batch 843 size: 64\n",
      "Batch 844 size: 64\n",
      "Batch 845 size: 64\n",
      "Batch 846 size: 64\n",
      "Batch 847 size: 64\n",
      "Batch 848 size: 64\n",
      "Batch 849 size: 64\n",
      "Batch 850 size: 64\n",
      "Batch 851 size: 64\n",
      "Batch 852 size: 64\n",
      "Batch 853 size: 64\n",
      "Batch 854 size: 64\n",
      "Batch 855 size: 64\n",
      "Batch 856 size: 64\n",
      "Batch 857 size: 64\n",
      "Batch 858 size: 64\n",
      "Batch 859 size: 64\n",
      "Batch 860 size: 64\n",
      "Batch 861 size: 64\n",
      "Batch 862 size: 64\n",
      "Batch 863 size: 64\n",
      "Batch 864 size: 64\n",
      "Batch 865 size: 64\n",
      "Batch 866 size: 64\n",
      "Batch 867 size: 64\n",
      "Batch 868 size: 64\n",
      "Batch 869 size: 64\n",
      "Batch 870 size: 64\n",
      "Batch 871 size: 64\n",
      "Batch 872 size: 64\n",
      "Batch 873 size: 64\n",
      "Batch 874 size: 64\n",
      "Batch 875 size: 64\n",
      "Batch 876 size: 64\n",
      "Batch 877 size: 64\n",
      "Batch 878 size: 64\n",
      "Batch 879 size: 64\n",
      "Batch 880 size: 64\n",
      "Batch 881 size: 64\n",
      "Batch 882 size: 64\n",
      "Batch 883 size: 64\n",
      "Batch 884 size: 64\n",
      "Batch 885 size: 64\n",
      "Batch 886 size: 64\n",
      "Batch 887 size: 64\n",
      "Batch 888 size: 64\n",
      "Batch 889 size: 64\n",
      "Batch 890 size: 64\n",
      "Batch 891 size: 64\n",
      "Batch 892 size: 64\n",
      "Batch 893 size: 64\n",
      "Batch 894 size: 64\n",
      "Batch 895 size: 64\n",
      "Batch 896 size: 64\n",
      "Batch 897 size: 64\n",
      "Batch 898 size: 64\n",
      "Batch 899 size: 64\n",
      "Batch 900 size: 64\n",
      "Batch 901 size: 64\n",
      "Batch 902 size: 64\n",
      "Batch 903 size: 64\n",
      "Batch 904 size: 64\n",
      "Batch 905 size: 64\n",
      "Batch 906 size: 64\n",
      "Batch 907 size: 64\n",
      "Batch 908 size: 64\n",
      "Batch 909 size: 64\n",
      "Batch 910 size: 64\n",
      "Batch 911 size: 64\n",
      "Batch 912 size: 64\n",
      "Batch 913 size: 64\n",
      "Batch 914 size: 64\n",
      "Batch 915 size: 64\n",
      "Batch 916 size: 64\n",
      "Batch 917 size: 64\n",
      "Batch 918 size: 64\n",
      "Batch 919 size: 64\n",
      "Batch 920 size: 64\n",
      "Batch 921 size: 64\n",
      "Batch 922 size: 64\n",
      "Batch 923 size: 64\n",
      "Batch 924 size: 64\n",
      "Batch 925 size: 64\n",
      "Batch 926 size: 64\n",
      "Batch 927 size: 64\n",
      "Batch 928 size: 64\n",
      "Batch 929 size: 64\n",
      "Batch 930 size: 64\n",
      "Batch 931 size: 64\n",
      "Batch 932 size: 64\n",
      "Batch 933 size: 64\n",
      "Batch 934 size: 64\n",
      "Batch 935 size: 64\n",
      "Batch 936 size: 64\n",
      "Batch 937 size: 64\n",
      "Batch 938 size: 64\n",
      "Batch 939 size: 64\n",
      "Batch 940 size: 64\n",
      "Batch 941 size: 64\n",
      "Batch 942 size: 64\n",
      "Batch 943 size: 64\n",
      "Batch 944 size: 64\n",
      "Batch 945 size: 64\n",
      "Batch 946 size: 64\n",
      "Batch 947 size: 64\n",
      "Batch 948 size: 64\n",
      "Batch 949 size: 64\n",
      "Batch 950 size: 64\n",
      "Batch 951 size: 64\n",
      "Batch 952 size: 64\n",
      "Batch 953 size: 64\n",
      "Batch 954 size: 64\n",
      "Batch 955 size: 64\n",
      "Batch 956 size: 64\n",
      "Batch 957 size: 64\n",
      "Batch 958 size: 64\n",
      "Batch 959 size: 64\n",
      "Batch 960 size: 64\n",
      "Batch 961 size: 64\n",
      "Batch 962 size: 64\n",
      "Batch 963 size: 64\n",
      "Batch 964 size: 64\n",
      "Batch 965 size: 64\n",
      "Batch 966 size: 64\n",
      "Batch 967 size: 64\n",
      "Batch 968 size: 64\n",
      "Batch 969 size: 64\n",
      "Batch 970 size: 64\n",
      "Batch 971 size: 64\n",
      "Batch 972 size: 64\n",
      "Batch 973 size: 64\n",
      "Batch 974 size: 64\n",
      "Batch 975 size: 64\n",
      "Batch 976 size: 64\n",
      "Batch 977 size: 64\n",
      "Batch 978 size: 64\n",
      "Batch 979 size: 64\n",
      "Batch 980 size: 64\n",
      "Batch 981 size: 64\n",
      "Batch 982 size: 64\n",
      "Batch 983 size: 64\n",
      "Batch 984 size: 64\n",
      "Batch 985 size: 64\n",
      "Batch 986 size: 64\n",
      "Batch 987 size: 64\n",
      "Batch 988 size: 64\n",
      "Batch 989 size: 64\n",
      "Batch 990 size: 64\n",
      "Batch 991 size: 64\n",
      "Batch 992 size: 64\n",
      "Batch 993 size: 64\n",
      "Batch 994 size: 64\n",
      "Batch 995 size: 64\n",
      "Batch 996 size: 64\n",
      "Batch 997 size: 64\n",
      "Batch 998 size: 64\n",
      "Batch 999 size: 64\n",
      "Batch 1000 size: 64\n",
      "Batch 1001 size: 64\n",
      "Batch 1002 size: 64\n",
      "Batch 1003 size: 64\n",
      "Batch 1004 size: 64\n",
      "Batch 1005 size: 64\n",
      "Batch 1006 size: 64\n",
      "Batch 1007 size: 64\n",
      "Batch 1008 size: 64\n",
      "Batch 1009 size: 64\n",
      "Batch 1010 size: 64\n",
      "Batch 1011 size: 64\n",
      "Batch 1012 size: 64\n",
      "Batch 1013 size: 64\n",
      "Batch 1014 size: 64\n",
      "Batch 1015 size: 64\n",
      "Batch 1016 size: 64\n",
      "Batch 1017 size: 64\n",
      "Batch 1018 size: 64\n",
      "Batch 1019 size: 64\n",
      "Batch 1020 size: 64\n",
      "Batch 1021 size: 64\n",
      "Batch 1022 size: 64\n",
      "Batch 1023 size: 64\n",
      "Batch 1024 size: 64\n",
      "Batch 1025 size: 64\n",
      "Batch 1026 size: 64\n",
      "Batch 1027 size: 64\n",
      "Batch 1028 size: 64\n",
      "Batch 1029 size: 64\n",
      "Batch 1030 size: 64\n",
      "Batch 1031 size: 64\n",
      "Batch 1032 size: 64\n",
      "Batch 1033 size: 64\n",
      "Batch 1034 size: 64\n",
      "Batch 1035 size: 64\n",
      "Batch 1036 size: 64\n",
      "Batch 1037 size: 64\n",
      "Batch 1038 size: 64\n",
      "Batch 1039 size: 64\n",
      "Batch 1040 size: 64\n",
      "Batch 1041 size: 64\n",
      "Batch 1042 size: 64\n",
      "Batch 1043 size: 64\n",
      "Batch 1044 size: 64\n",
      "Batch 1045 size: 64\n",
      "Batch 1046 size: 64\n",
      "Batch 1047 size: 64\n",
      "Batch 1048 size: 64\n",
      "Batch 1049 size: 64\n",
      "Batch 1050 size: 64\n",
      "Batch 1051 size: 64\n",
      "Batch 1052 size: 64\n",
      "Batch 1053 size: 64\n",
      "Batch 1054 size: 64\n",
      "Batch 1055 size: 64\n",
      "Batch 1056 size: 64\n",
      "Batch 1057 size: 64\n",
      "Batch 1058 size: 64\n",
      "Batch 1059 size: 64\n",
      "Batch 1060 size: 64\n",
      "Batch 1061 size: 64\n",
      "Batch 1062 size: 64\n",
      "Batch 1063 size: 64\n",
      "Batch 1064 size: 64\n",
      "Batch 1065 size: 64\n",
      "Batch 1066 size: 64\n",
      "Batch 1067 size: 64\n",
      "Batch 1068 size: 64\n",
      "Batch 1069 size: 64\n",
      "Batch 1070 size: 64\n",
      "Batch 1071 size: 64\n",
      "Batch 1072 size: 64\n",
      "Batch 1073 size: 64\n",
      "Batch 1074 size: 64\n",
      "Batch 1075 size: 64\n",
      "Batch 1076 size: 64\n",
      "Batch 1077 size: 64\n",
      "Batch 1078 size: 64\n",
      "Batch 1079 size: 64\n",
      "Batch 1080 size: 64\n",
      "Batch 1081 size: 64\n",
      "Batch 1082 size: 64\n",
      "Batch 1083 size: 64\n",
      "Batch 1084 size: 64\n",
      "Batch 1085 size: 64\n",
      "Batch 1086 size: 64\n",
      "Batch 1087 size: 64\n",
      "Batch 1088 size: 64\n",
      "Batch 1089 size: 64\n",
      "Batch 1090 size: 64\n",
      "Batch 1091 size: 64\n",
      "Batch 1092 size: 64\n",
      "Batch 1093 size: 64\n",
      "Batch 1094 size: 64\n",
      "Batch 1095 size: 64\n",
      "Batch 1096 size: 64\n",
      "Batch 1097 size: 64\n",
      "Batch 1098 size: 64\n",
      "Batch 1099 size: 64\n",
      "Batch 1100 size: 64\n",
      "Batch 1101 size: 64\n",
      "Batch 1102 size: 64\n",
      "Batch 1103 size: 64\n",
      "Batch 1104 size: 64\n",
      "Batch 1105 size: 64\n",
      "Batch 1106 size: 64\n",
      "Batch 1107 size: 64\n",
      "Batch 1108 size: 64\n",
      "Batch 1109 size: 64\n",
      "Batch 1110 size: 64\n",
      "Batch 1111 size: 64\n",
      "Batch 1112 size: 64\n",
      "Batch 1113 size: 64\n",
      "Batch 1114 size: 64\n",
      "Batch 1115 size: 64\n",
      "Batch 1116 size: 64\n",
      "Batch 1117 size: 64\n",
      "Batch 1118 size: 64\n",
      "Batch 1119 size: 64\n",
      "Batch 1120 size: 64\n",
      "Batch 1121 size: 64\n",
      "Batch 1122 size: 64\n",
      "Batch 1123 size: 64\n",
      "Batch 1124 size: 64\n",
      "Batch 1125 size: 64\n",
      "Batch 1126 size: 64\n",
      "Batch 1127 size: 64\n",
      "Batch 1128 size: 64\n",
      "Batch 1129 size: 64\n",
      "Batch 1130 size: 64\n",
      "Batch 1131 size: 64\n",
      "Batch 1132 size: 64\n",
      "Batch 1133 size: 64\n",
      "Batch 1134 size: 64\n",
      "Batch 1135 size: 64\n",
      "Batch 1136 size: 64\n",
      "Batch 1137 size: 64\n",
      "Batch 1138 size: 64\n",
      "Batch 1139 size: 64\n",
      "Batch 1140 size: 64\n",
      "Batch 1141 size: 64\n",
      "Batch 1142 size: 64\n",
      "Batch 1143 size: 64\n",
      "Batch 1144 size: 64\n",
      "Batch 1145 size: 64\n",
      "Batch 1146 size: 64\n",
      "Batch 1147 size: 64\n",
      "Batch 1148 size: 64\n",
      "Batch 1149 size: 64\n",
      "Batch 1150 size: 64\n",
      "Batch 1151 size: 64\n",
      "Batch 1152 size: 64\n",
      "Batch 1153 size: 64\n",
      "Batch 1154 size: 64\n",
      "Batch 1155 size: 64\n",
      "Batch 1156 size: 64\n",
      "Batch 1157 size: 64\n",
      "Batch 1158 size: 64\n",
      "Batch 1159 size: 64\n",
      "Batch 1160 size: 64\n",
      "Batch 1161 size: 64\n",
      "Batch 1162 size: 64\n",
      "Batch 1163 size: 64\n",
      "Batch 1164 size: 64\n",
      "Batch 1165 size: 64\n",
      "Batch 1166 size: 64\n",
      "Batch 1167 size: 64\n",
      "Batch 1168 size: 64\n",
      "Batch 1169 size: 64\n",
      "Batch 1170 size: 64\n",
      "Batch 1171 size: 64\n",
      "Batch 1172 size: 64\n",
      "Batch 1173 size: 64\n",
      "Batch 1174 size: 64\n",
      "Batch 1175 size: 64\n",
      "Batch 1176 size: 64\n",
      "Batch 1177 size: 64\n",
      "Batch 1178 size: 64\n",
      "Batch 1179 size: 64\n",
      "Batch 1180 size: 64\n",
      "Batch 1181 size: 64\n",
      "Batch 1182 size: 64\n",
      "Batch 1183 size: 64\n",
      "Batch 1184 size: 64\n",
      "Batch 1185 size: 64\n",
      "Batch 1186 size: 64\n",
      "Batch 1187 size: 64\n",
      "Batch 1188 size: 64\n",
      "Batch 1189 size: 64\n",
      "Batch 1190 size: 64\n",
      "Batch 1191 size: 64\n",
      "Batch 1192 size: 64\n",
      "Batch 1193 size: 64\n",
      "Batch 1194 size: 64\n",
      "Batch 1195 size: 64\n",
      "Batch 1196 size: 64\n",
      "Batch 1197 size: 64\n",
      "Batch 1198 size: 64\n",
      "Batch 1199 size: 64\n",
      "Batch 1200 size: 64\n",
      "Batch 1201 size: 64\n",
      "Batch 1202 size: 64\n",
      "Batch 1203 size: 64\n",
      "Batch 1204 size: 64\n",
      "Batch 1205 size: 64\n",
      "Batch 1206 size: 64\n",
      "Batch 1207 size: 64\n",
      "Batch 1208 size: 64\n",
      "Batch 1209 size: 64\n",
      "Batch 1210 size: 64\n",
      "Batch 1211 size: 64\n",
      "Batch 1212 size: 64\n",
      "Batch 1213 size: 64\n",
      "Batch 1214 size: 64\n",
      "Batch 1215 size: 64\n",
      "Batch 1216 size: 64\n",
      "Batch 1217 size: 64\n",
      "Batch 1218 size: 64\n",
      "Batch 1219 size: 64\n",
      "Batch 1220 size: 64\n",
      "Batch 1221 size: 64\n",
      "Batch 1222 size: 64\n",
      "Batch 1223 size: 64\n",
      "Batch 1224 size: 64\n",
      "Batch 1225 size: 64\n",
      "Batch 1226 size: 64\n",
      "Batch 1227 size: 64\n",
      "Batch 1228 size: 64\n",
      "Batch 1229 size: 64\n",
      "Batch 1230 size: 64\n",
      "Batch 1231 size: 64\n",
      "Batch 1232 size: 64\n",
      "Batch 1233 size: 64\n",
      "Batch 1234 size: 64\n",
      "Batch 1235 size: 64\n",
      "Batch 1236 size: 64\n",
      "Batch 1237 size: 64\n",
      "Batch 1238 size: 64\n",
      "Batch 1239 size: 64\n",
      "Batch 1240 size: 64\n",
      "Batch 1241 size: 64\n",
      "Batch 1242 size: 64\n",
      "Batch 1243 size: 64\n",
      "Batch 1244 size: 64\n",
      "Batch 1245 size: 64\n",
      "Batch 1246 size: 64\n",
      "Batch 1247 size: 64\n",
      "Batch 1248 size: 64\n",
      "Batch 1249 size: 64\n",
      "Batch 1250 size: 64\n",
      "Batch 1251 size: 64\n",
      "Batch 1252 size: 64\n",
      "Batch 1253 size: 64\n",
      "Batch 1254 size: 64\n",
      "Batch 1255 size: 64\n",
      "Batch 1256 size: 64\n",
      "Batch 1257 size: 64\n",
      "Batch 1258 size: 64\n",
      "Batch 1259 size: 64\n",
      "Batch 1260 size: 64\n",
      "Batch 1261 size: 64\n",
      "Batch 1262 size: 64\n",
      "Batch 1263 size: 64\n",
      "Batch 1264 size: 64\n",
      "Batch 1265 size: 64\n",
      "Batch 1266 size: 64\n",
      "Batch 1267 size: 64\n",
      "Batch 1268 size: 64\n",
      "Batch 1269 size: 64\n",
      "Batch 1270 size: 64\n",
      "Batch 1271 size: 64\n",
      "Batch 1272 size: 64\n",
      "Batch 1273 size: 64\n",
      "Batch 1274 size: 64\n",
      "Batch 1275 size: 64\n",
      "Batch 1276 size: 64\n",
      "Batch 1277 size: 64\n",
      "Batch 1278 size: 64\n",
      "Batch 1279 size: 64\n",
      "Batch 1280 size: 64\n",
      "Batch 1281 size: 64\n",
      "Batch 1282 size: 64\n",
      "Batch 1283 size: 64\n",
      "Batch 1284 size: 64\n",
      "Batch 1285 size: 64\n",
      "Batch 1286 size: 64\n",
      "Batch 1287 size: 64\n",
      "Batch 1288 size: 64\n",
      "Batch 1289 size: 64\n",
      "Batch 1290 size: 64\n",
      "Batch 1291 size: 64\n",
      "Batch 1292 size: 64\n",
      "Batch 1293 size: 64\n",
      "Batch 1294 size: 64\n",
      "Batch 1295 size: 64\n",
      "Batch 1296 size: 64\n",
      "Batch 1297 size: 64\n",
      "Batch 1298 size: 64\n",
      "Batch 1299 size: 64\n",
      "Batch 1300 size: 64\n",
      "Batch 1301 size: 64\n",
      "Batch 1302 size: 64\n",
      "Batch 1303 size: 64\n",
      "Batch 1304 size: 64\n",
      "Batch 1305 size: 64\n",
      "Batch 1306 size: 64\n",
      "Batch 1307 size: 64\n",
      "Batch 1308 size: 64\n",
      "Batch 1309 size: 64\n",
      "Batch 1310 size: 64\n",
      "Batch 1311 size: 64\n",
      "Batch 1312 size: 64\n",
      "Batch 1313 size: 64\n",
      "Batch 1314 size: 64\n",
      "Batch 1315 size: 64\n",
      "Batch 1316 size: 64\n",
      "Batch 1317 size: 64\n",
      "Batch 1318 size: 64\n",
      "Batch 1319 size: 64\n",
      "Batch 1320 size: 64\n",
      "Batch 1321 size: 64\n",
      "Batch 1322 size: 64\n",
      "Batch 1323 size: 64\n",
      "Batch 1324 size: 64\n",
      "Batch 1325 size: 64\n",
      "Batch 1326 size: 64\n",
      "Batch 1327 size: 64\n",
      "Batch 1328 size: 64\n",
      "Batch 1329 size: 64\n",
      "Batch 1330 size: 64\n",
      "Batch 1331 size: 64\n",
      "Batch 1332 size: 64\n",
      "Batch 1333 size: 64\n",
      "Batch 1334 size: 64\n",
      "Batch 1335 size: 64\n",
      "Batch 1336 size: 64\n",
      "Batch 1337 size: 64\n",
      "Batch 1338 size: 64\n",
      "Batch 1339 size: 64\n",
      "Batch 1340 size: 64\n",
      "Batch 1341 size: 64\n",
      "Batch 1342 size: 64\n",
      "Batch 1343 size: 64\n",
      "Batch 1344 size: 64\n",
      "Batch 1345 size: 64\n",
      "Batch 1346 size: 64\n",
      "Batch 1347 size: 64\n",
      "Batch 1348 size: 64\n",
      "Batch 1349 size: 64\n",
      "Batch 1350 size: 64\n",
      "Batch 1351 size: 64\n",
      "Batch 1352 size: 64\n",
      "Batch 1353 size: 64\n",
      "Batch 1354 size: 64\n",
      "Batch 1355 size: 64\n",
      "Batch 1356 size: 64\n",
      "Batch 1357 size: 64\n",
      "Batch 1358 size: 64\n",
      "Batch 1359 size: 64\n",
      "Batch 1360 size: 64\n",
      "Batch 1361 size: 64\n",
      "Batch 1362 size: 64\n",
      "Batch 1363 size: 64\n",
      "Batch 1364 size: 64\n",
      "Batch 1365 size: 64\n",
      "Batch 1366 size: 64\n",
      "Batch 1367 size: 64\n",
      "Batch 1368 size: 64\n",
      "Batch 1369 size: 64\n",
      "Batch 1370 size: 64\n",
      "Batch 1371 size: 64\n",
      "Batch 1372 size: 64\n",
      "Batch 1373 size: 64\n",
      "Batch 1374 size: 64\n",
      "Batch 1375 size: 64\n",
      "Batch 1376 size: 64\n",
      "Batch 1377 size: 64\n",
      "Batch 1378 size: 64\n",
      "Batch 1379 size: 64\n",
      "Batch 1380 size: 64\n",
      "Batch 1381 size: 64\n",
      "Batch 1382 size: 64\n",
      "Batch 1383 size: 64\n",
      "Batch 1384 size: 64\n",
      "Batch 1385 size: 64\n",
      "Batch 1386 size: 64\n",
      "Batch 1387 size: 64\n",
      "Batch 1388 size: 64\n",
      "Batch 1389 size: 64\n",
      "Batch 1390 size: 64\n",
      "Batch 1391 size: 64\n",
      "Batch 1392 size: 64\n",
      "Batch 1393 size: 64\n",
      "Batch 1394 size: 64\n",
      "Batch 1395 size: 64\n",
      "Batch 1396 size: 64\n",
      "Batch 1397 size: 64\n",
      "Batch 1398 size: 64\n",
      "Batch 1399 size: 64\n",
      "Batch 1400 size: 64\n",
      "Batch 1401 size: 64\n",
      "Batch 1402 size: 64\n",
      "Batch 1403 size: 64\n",
      "Batch 1404 size: 64\n",
      "Batch 1405 size: 64\n",
      "Batch 1406 size: 64\n",
      "Batch 1407 size: 64\n",
      "Batch 1408 size: 64\n",
      "Batch 1409 size: 64\n",
      "Batch 1410 size: 64\n",
      "Batch 1411 size: 64\n",
      "Batch 1412 size: 64\n",
      "Batch 1413 size: 64\n",
      "Batch 1414 size: 64\n",
      "Batch 1415 size: 64\n",
      "Batch 1416 size: 64\n",
      "Batch 1417 size: 64\n",
      "Batch 1418 size: 64\n",
      "Batch 1419 size: 64\n",
      "Batch 1420 size: 64\n",
      "Batch 1421 size: 64\n",
      "Batch 1422 size: 64\n",
      "Batch 1423 size: 64\n",
      "Batch 1424 size: 64\n",
      "Batch 1425 size: 64\n",
      "Batch 1426 size: 64\n",
      "Batch 1427 size: 64\n",
      "Batch 1428 size: 64\n",
      "Batch 1429 size: 64\n",
      "Batch 1430 size: 64\n",
      "Batch 1431 size: 64\n",
      "Batch 1432 size: 64\n",
      "Batch 1433 size: 64\n",
      "Batch 1434 size: 64\n",
      "Batch 1435 size: 64\n",
      "Batch 1436 size: 64\n",
      "Batch 1437 size: 64\n",
      "Batch 1438 size: 64\n",
      "Batch 1439 size: 64\n",
      "Batch 1440 size: 64\n",
      "Batch 1441 size: 64\n",
      "Batch 1442 size: 64\n",
      "Batch 1443 size: 64\n",
      "Batch 1444 size: 64\n",
      "Batch 1445 size: 64\n",
      "Batch 1446 size: 64\n",
      "Batch 1447 size: 64\n",
      "Batch 1448 size: 64\n",
      "Batch 1449 size: 64\n",
      "Batch 1450 size: 64\n",
      "Batch 1451 size: 64\n",
      "Batch 1452 size: 64\n",
      "Batch 1453 size: 64\n",
      "Batch 1454 size: 64\n",
      "Batch 1455 size: 64\n",
      "Batch 1456 size: 64\n",
      "Batch 1457 size: 64\n",
      "Batch 1458 size: 64\n",
      "Batch 1459 size: 64\n",
      "Batch 1460 size: 64\n",
      "Batch 1461 size: 64\n",
      "Batch 1462 size: 64\n",
      "Batch 1463 size: 64\n",
      "Batch 1464 size: 64\n",
      "Batch 1465 size: 64\n",
      "Batch 1466 size: 64\n",
      "Batch 1467 size: 64\n",
      "Batch 1468 size: 64\n",
      "Batch 1469 size: 64\n",
      "Batch 1470 size: 64\n",
      "Batch 1471 size: 64\n",
      "Batch 1472 size: 64\n",
      "Batch 1473 size: 64\n",
      "Batch 1474 size: 64\n",
      "Batch 1475 size: 64\n",
      "Batch 1476 size: 64\n",
      "Batch 1477 size: 64\n",
      "Batch 1478 size: 64\n",
      "Batch 1479 size: 64\n",
      "Batch 1480 size: 64\n",
      "Batch 1481 size: 64\n",
      "Batch 1482 size: 64\n",
      "Batch 1483 size: 64\n",
      "Batch 1484 size: 64\n",
      "Batch 1485 size: 64\n",
      "Batch 1486 size: 64\n",
      "Batch 1487 size: 64\n",
      "Batch 1488 size: 64\n",
      "Batch 1489 size: 64\n",
      "Batch 1490 size: 64\n",
      "Batch 1491 size: 64\n",
      "Batch 1492 size: 64\n",
      "Batch 1493 size: 64\n",
      "Batch 1494 size: 64\n",
      "Batch 1495 size: 64\n",
      "Batch 1496 size: 64\n",
      "Batch 1497 size: 64\n",
      "Batch 1498 size: 64\n",
      "Batch 1499 size: 64\n",
      "Batch 1500 size: 64\n",
      "Batch 1501 size: 64\n",
      "Batch 1502 size: 64\n",
      "Batch 1503 size: 64\n",
      "Batch 1504 size: 64\n",
      "Batch 1505 size: 64\n",
      "Batch 1506 size: 64\n",
      "Batch 1507 size: 64\n",
      "Batch 1508 size: 64\n",
      "Batch 1509 size: 64\n",
      "Batch 1510 size: 64\n",
      "Batch 1511 size: 64\n",
      "Batch 1512 size: 64\n",
      "Batch 1513 size: 64\n",
      "Batch 1514 size: 64\n",
      "Batch 1515 size: 64\n",
      "Batch 1516 size: 64\n",
      "Batch 1517 size: 64\n",
      "Batch 1518 size: 64\n",
      "Batch 1519 size: 64\n",
      "Batch 1520 size: 64\n",
      "Batch 1521 size: 64\n",
      "Batch 1522 size: 64\n",
      "Batch 1523 size: 64\n",
      "Batch 1524 size: 64\n",
      "Batch 1525 size: 64\n",
      "Batch 1526 size: 64\n",
      "Batch 1527 size: 64\n",
      "Batch 1528 size: 64\n",
      "Batch 1529 size: 64\n",
      "Batch 1530 size: 64\n",
      "Batch 1531 size: 64\n",
      "Batch 1532 size: 64\n",
      "Batch 1533 size: 64\n",
      "Batch 1534 size: 64\n",
      "Batch 1535 size: 64\n",
      "Batch 1536 size: 64\n",
      "Batch 1537 size: 64\n",
      "Batch 1538 size: 64\n",
      "Batch 1539 size: 64\n",
      "Batch 1540 size: 64\n",
      "Batch 1541 size: 64\n",
      "Batch 1542 size: 64\n",
      "Batch 1543 size: 64\n",
      "Batch 1544 size: 64\n",
      "Batch 1545 size: 64\n",
      "Batch 1546 size: 64\n",
      "Batch 1547 size: 64\n",
      "Batch 1548 size: 64\n",
      "Batch 1549 size: 64\n",
      "Batch 1550 size: 64\n",
      "Batch 1551 size: 64\n",
      "Batch 1552 size: 64\n",
      "Batch 1553 size: 64\n",
      "Batch 1554 size: 64\n",
      "Batch 1555 size: 64\n",
      "Batch 1556 size: 64\n",
      "Batch 1557 size: 64\n",
      "Batch 1558 size: 64\n",
      "Batch 1559 size: 64\n",
      "Batch 1560 size: 64\n",
      "Batch 1561 size: 64\n",
      "Batch 1562 size: 64\n",
      "Batch 1563 size: 64\n",
      "Batch 1564 size: 64\n",
      "Batch 1565 size: 64\n",
      "Batch 1566 size: 64\n",
      "Batch 1567 size: 64\n",
      "Batch 1568 size: 64\n",
      "Batch 1569 size: 64\n",
      "Batch 1570 size: 64\n",
      "Batch 1571 size: 64\n",
      "Batch 1572 size: 64\n",
      "Batch 1573 size: 64\n",
      "Batch 1574 size: 64\n",
      "Batch 1575 size: 64\n",
      "Batch 1576 size: 64\n",
      "Batch 1577 size: 64\n",
      "Batch 1578 size: 64\n",
      "Batch 1579 size: 64\n",
      "Batch 1580 size: 64\n",
      "Batch 1581 size: 64\n",
      "Batch 1582 size: 64\n",
      "Batch 1583 size: 64\n",
      "Batch 1584 size: 64\n",
      "Batch 1585 size: 64\n",
      "Batch 1586 size: 64\n",
      "Batch 1587 size: 64\n",
      "Batch 1588 size: 64\n",
      "Batch 1589 size: 64\n",
      "Batch 1590 size: 64\n",
      "Batch 1591 size: 64\n",
      "Batch 1592 size: 64\n",
      "Batch 1593 size: 64\n",
      "Batch 1594 size: 64\n",
      "Batch 1595 size: 64\n",
      "Batch 1596 size: 64\n",
      "Batch 1597 size: 64\n",
      "Batch 1598 size: 64\n",
      "Batch 1599 size: 64\n",
      "Batch 1600 size: 64\n",
      "Batch 1601 size: 64\n",
      "Batch 1602 size: 64\n",
      "Batch 1603 size: 64\n",
      "Batch 1604 size: 64\n",
      "Batch 1605 size: 64\n",
      "Batch 1606 size: 64\n",
      "Batch 1607 size: 64\n",
      "Batch 1608 size: 64\n",
      "Batch 1609 size: 64\n",
      "Batch 1610 size: 64\n",
      "Batch 1611 size: 64\n",
      "Batch 1612 size: 64\n",
      "Batch 1613 size: 64\n",
      "Batch 1614 size: 64\n",
      "Batch 1615 size: 64\n",
      "Batch 1616 size: 64\n",
      "Batch 1617 size: 64\n",
      "Batch 1618 size: 64\n",
      "Batch 1619 size: 64\n",
      "Batch 1620 size: 64\n",
      "Batch 1621 size: 64\n",
      "Batch 1622 size: 64\n",
      "Batch 1623 size: 64\n",
      "Batch 1624 size: 64\n",
      "Batch 1625 size: 64\n",
      "Batch 1626 size: 64\n",
      "Batch 1627 size: 64\n",
      "Batch 1628 size: 64\n",
      "Batch 1629 size: 64\n",
      "Batch 1630 size: 64\n",
      "Batch 1631 size: 64\n",
      "Batch 1632 size: 64\n",
      "Batch 1633 size: 64\n",
      "Batch 1634 size: 64\n",
      "Batch 1635 size: 64\n",
      "Batch 1636 size: 64\n",
      "Batch 1637 size: 64\n",
      "Batch 1638 size: 64\n",
      "Batch 1639 size: 64\n",
      "Batch 1640 size: 64\n",
      "Batch 1641 size: 64\n",
      "Batch 1642 size: 64\n",
      "Batch 1643 size: 64\n",
      "Batch 1644 size: 64\n",
      "Batch 1645 size: 64\n",
      "Batch 1646 size: 64\n",
      "Batch 1647 size: 64\n",
      "Batch 1648 size: 64\n",
      "Batch 1649 size: 64\n",
      "Batch 1650 size: 64\n",
      "Batch 1651 size: 64\n",
      "Batch 1652 size: 64\n",
      "Batch 1653 size: 64\n",
      "Batch 1654 size: 64\n",
      "Batch 1655 size: 64\n",
      "Batch 1656 size: 64\n",
      "Batch 1657 size: 64\n",
      "Batch 1658 size: 64\n",
      "Batch 1659 size: 64\n",
      "Batch 1660 size: 64\n",
      "Batch 1661 size: 64\n",
      "Batch 1662 size: 64\n",
      "Batch 1663 size: 64\n",
      "Batch 1664 size: 64\n",
      "Batch 1665 size: 64\n",
      "Batch 1666 size: 64\n",
      "Batch 1667 size: 64\n",
      "Batch 1668 size: 64\n",
      "Batch 1669 size: 64\n",
      "Batch 1670 size: 64\n",
      "Batch 1671 size: 64\n",
      "Batch 1672 size: 64\n",
      "Batch 1673 size: 64\n",
      "Batch 1674 size: 64\n",
      "Batch 1675 size: 64\n",
      "Batch 1676 size: 64\n",
      "Batch 1677 size: 64\n",
      "Batch 1678 size: 64\n",
      "Batch 1679 size: 64\n",
      "Batch 1680 size: 64\n",
      "Batch 1681 size: 64\n",
      "Batch 1682 size: 64\n",
      "Batch 1683 size: 64\n",
      "Batch 1684 size: 64\n",
      "Batch 1685 size: 64\n",
      "Batch 1686 size: 64\n",
      "Batch 1687 size: 64\n",
      "Batch 1688 size: 64\n",
      "Batch 1689 size: 64\n",
      "Batch 1690 size: 64\n",
      "Batch 1691 size: 64\n",
      "Batch 1692 size: 64\n",
      "Batch 1693 size: 64\n",
      "Batch 1694 size: 64\n",
      "Batch 1695 size: 64\n",
      "Batch 1696 size: 64\n",
      "Batch 1697 size: 64\n",
      "Batch 1698 size: 64\n",
      "Batch 1699 size: 64\n",
      "Batch 1700 size: 64\n",
      "Batch 1701 size: 64\n",
      "Batch 1702 size: 64\n",
      "Batch 1703 size: 64\n",
      "Batch 1704 size: 64\n",
      "Batch 1705 size: 64\n",
      "Batch 1706 size: 64\n",
      "Batch 1707 size: 64\n",
      "Batch 1708 size: 64\n",
      "Batch 1709 size: 64\n",
      "Batch 1710 size: 64\n",
      "Batch 1711 size: 64\n",
      "Batch 1712 size: 64\n",
      "Batch 1713 size: 64\n",
      "Batch 1714 size: 64\n",
      "Batch 1715 size: 64\n",
      "Batch 1716 size: 64\n",
      "Batch 1717 size: 64\n",
      "Batch 1718 size: 64\n",
      "Batch 1719 size: 64\n",
      "Batch 1720 size: 64\n",
      "Batch 1721 size: 64\n",
      "Batch 1722 size: 64\n",
      "Batch 1723 size: 64\n",
      "Batch 1724 size: 64\n",
      "Batch 1725 size: 64\n",
      "Batch 1726 size: 64\n",
      "Batch 1727 size: 64\n",
      "Batch 1728 size: 64\n",
      "Batch 1729 size: 64\n",
      "Batch 1730 size: 64\n",
      "Batch 1731 size: 64\n",
      "Batch 1732 size: 64\n",
      "Batch 1733 size: 64\n",
      "Batch 1734 size: 64\n",
      "Batch 1735 size: 64\n",
      "Batch 1736 size: 64\n",
      "Batch 1737 size: 64\n",
      "Batch 1738 size: 64\n",
      "Batch 1739 size: 64\n",
      "Batch 1740 size: 64\n",
      "Batch 1741 size: 64\n",
      "Batch 1742 size: 64\n",
      "Batch 1743 size: 64\n",
      "Batch 1744 size: 64\n",
      "Batch 1745 size: 64\n",
      "Batch 1746 size: 64\n",
      "Batch 1747 size: 64\n",
      "Batch 1748 size: 64\n",
      "Batch 1749 size: 64\n",
      "Batch 1750 size: 64\n",
      "Batch 1751 size: 64\n",
      "Batch 1752 size: 64\n",
      "Batch 1753 size: 64\n",
      "Batch 1754 size: 64\n",
      "Batch 1755 size: 64\n",
      "Batch 1756 size: 64\n",
      "Batch 1757 size: 64\n",
      "Batch 1758 size: 64\n",
      "Batch 1759 size: 64\n",
      "Batch 1760 size: 64\n",
      "Batch 1761 size: 64\n",
      "Batch 1762 size: 64\n",
      "Batch 1763 size: 64\n",
      "Batch 1764 size: 64\n",
      "Batch 1765 size: 64\n",
      "Batch 1766 size: 64\n",
      "Batch 1767 size: 64\n",
      "Batch 1768 size: 64\n",
      "Batch 1769 size: 64\n",
      "Batch 1770 size: 64\n",
      "Batch 1771 size: 64\n",
      "Batch 1772 size: 64\n",
      "Batch 1773 size: 64\n",
      "Batch 1774 size: 64\n",
      "Batch 1775 size: 64\n",
      "Batch 1776 size: 64\n",
      "Batch 1777 size: 64\n",
      "Batch 1778 size: 64\n",
      "Batch 1779 size: 64\n",
      "Batch 1780 size: 64\n",
      "Batch 1781 size: 64\n",
      "Batch 1782 size: 64\n",
      "Batch 1783 size: 64\n",
      "Batch 1784 size: 64\n",
      "Batch 1785 size: 64\n",
      "Batch 1786 size: 64\n",
      "Batch 1787 size: 64\n",
      "Batch 1788 size: 64\n",
      "Batch 1789 size: 64\n",
      "Batch 1790 size: 64\n",
      "Batch 1791 size: 64\n",
      "Batch 1792 size: 64\n",
      "Batch 1793 size: 64\n",
      "Batch 1794 size: 64\n",
      "Batch 1795 size: 64\n",
      "Batch 1796 size: 64\n",
      "Batch 1797 size: 64\n",
      "Batch 1798 size: 64\n",
      "Batch 1799 size: 64\n",
      "Batch 1800 size: 64\n",
      "Batch 1801 size: 64\n",
      "Batch 1802 size: 64\n",
      "Batch 1803 size: 64\n",
      "Batch 1804 size: 64\n",
      "Batch 1805 size: 64\n",
      "Batch 1806 size: 64\n",
      "Batch 1807 size: 64\n",
      "Batch 1808 size: 64\n",
      "Batch 1809 size: 64\n",
      "Batch 1810 size: 64\n",
      "Batch 1811 size: 64\n",
      "Batch 1812 size: 64\n",
      "Batch 1813 size: 64\n",
      "Batch 1814 size: 64\n",
      "Batch 1815 size: 64\n",
      "Batch 1816 size: 64\n",
      "Batch 1817 size: 64\n",
      "Batch 1818 size: 64\n",
      "Batch 1819 size: 64\n",
      "Batch 1820 size: 64\n",
      "Batch 1821 size: 64\n",
      "Batch 1822 size: 64\n",
      "Batch 1823 size: 64\n",
      "Batch 1824 size: 64\n",
      "Batch 1825 size: 64\n",
      "Batch 1826 size: 64\n",
      "Batch 1827 size: 64\n",
      "Batch 1828 size: 64\n",
      "Batch 1829 size: 64\n",
      "Batch 1830 size: 64\n",
      "Batch 1831 size: 64\n",
      "Batch 1832 size: 64\n",
      "Batch 1833 size: 64\n",
      "Batch 1834 size: 64\n",
      "Batch 1835 size: 64\n",
      "Batch 1836 size: 64\n",
      "Batch 1837 size: 64\n",
      "Batch 1838 size: 64\n",
      "Batch 1839 size: 64\n",
      "Batch 1840 size: 64\n",
      "Batch 1841 size: 64\n",
      "Batch 1842 size: 64\n",
      "Batch 1843 size: 64\n",
      "Batch 1844 size: 64\n",
      "Batch 1845 size: 64\n",
      "Batch 1846 size: 64\n",
      "Batch 1847 size: 64\n",
      "Batch 1848 size: 64\n",
      "Batch 1849 size: 64\n",
      "Batch 1850 size: 64\n",
      "Batch 1851 size: 64\n",
      "Batch 1852 size: 64\n",
      "Batch 1853 size: 64\n",
      "Batch 1854 size: 64\n",
      "Batch 1855 size: 64\n",
      "Batch 1856 size: 64\n",
      "Batch 1857 size: 64\n",
      "Batch 1858 size: 64\n",
      "Batch 1859 size: 64\n",
      "Batch 1860 size: 64\n",
      "Batch 1861 size: 64\n",
      "Batch 1862 size: 64\n",
      "Batch 1863 size: 64\n",
      "Batch 1864 size: 64\n",
      "Batch 1865 size: 64\n",
      "Batch 1866 size: 64\n",
      "Batch 1867 size: 64\n",
      "Batch 1868 size: 64\n",
      "Batch 1869 size: 64\n",
      "Batch 1870 size: 64\n",
      "Batch 1871 size: 64\n",
      "Batch 1872 size: 64\n",
      "Batch 1873 size: 64\n",
      "Batch 1874 size: 64\n",
      "Batch 1875 size: 64\n",
      "Batch 1876 size: 64\n",
      "Batch 1877 size: 64\n",
      "Batch 1878 size: 64\n",
      "Batch 1879 size: 64\n",
      "Batch 1880 size: 64\n",
      "Batch 1881 size: 64\n",
      "Batch 1882 size: 64\n",
      "Batch 1883 size: 64\n",
      "Batch 1884 size: 64\n",
      "Batch 1885 size: 64\n",
      "Batch 1886 size: 64\n",
      "Batch 1887 size: 64\n",
      "Batch 1888 size: 64\n",
      "Batch 1889 size: 64\n",
      "Batch 1890 size: 64\n",
      "Batch 1891 size: 64\n",
      "Batch 1892 size: 64\n",
      "Batch 1893 size: 64\n",
      "Batch 1894 size: 64\n",
      "Batch 1895 size: 64\n",
      "Batch 1896 size: 64\n",
      "Batch 1897 size: 64\n",
      "Batch 1898 size: 64\n",
      "Batch 1899 size: 64\n",
      "Batch 1900 size: 64\n",
      "Batch 1901 size: 64\n",
      "Batch 1902 size: 64\n",
      "Batch 1903 size: 64\n",
      "Batch 1904 size: 64\n",
      "Batch 1905 size: 64\n",
      "Batch 1906 size: 64\n",
      "Batch 1907 size: 64\n",
      "Batch 1908 size: 64\n",
      "Batch 1909 size: 64\n",
      "Batch 1910 size: 64\n",
      "Batch 1911 size: 64\n",
      "Batch 1912 size: 64\n",
      "Batch 1913 size: 64\n",
      "Batch 1914 size: 64\n",
      "Batch 1915 size: 64\n",
      "Batch 1916 size: 64\n",
      "Batch 1917 size: 64\n",
      "Batch 1918 size: 64\n",
      "Batch 1919 size: 64\n",
      "Batch 1920 size: 64\n",
      "Batch 1921 size: 64\n",
      "Batch 1922 size: 64\n",
      "Batch 1923 size: 64\n",
      "Batch 1924 size: 64\n",
      "Batch 1925 size: 64\n",
      "Batch 1926 size: 64\n",
      "Batch 1927 size: 64\n",
      "Batch 1928 size: 64\n",
      "Batch 1929 size: 64\n",
      "Batch 1930 size: 64\n",
      "Batch 1931 size: 64\n",
      "Batch 1932 size: 64\n",
      "Batch 1933 size: 64\n",
      "Batch 1934 size: 64\n",
      "Batch 1935 size: 64\n",
      "Batch 1936 size: 64\n",
      "Batch 1937 size: 64\n",
      "Batch 1938 size: 64\n",
      "Batch 1939 size: 64\n",
      "Batch 1940 size: 64\n",
      "Batch 1941 size: 64\n",
      "Batch 1942 size: 64\n",
      "Batch 1943 size: 64\n",
      "Batch 1944 size: 64\n",
      "Batch 1945 size: 64\n",
      "Batch 1946 size: 64\n",
      "Batch 1947 size: 64\n",
      "Batch 1948 size: 64\n",
      "Batch 1949 size: 64\n",
      "Batch 1950 size: 64\n",
      "Batch 1951 size: 64\n",
      "Batch 1952 size: 64\n",
      "Batch 1953 size: 64\n",
      "Batch 1954 size: 64\n",
      "Batch 1955 size: 64\n",
      "Batch 1956 size: 64\n",
      "Batch 1957 size: 64\n",
      "Batch 1958 size: 64\n",
      "Batch 1959 size: 64\n",
      "Batch 1960 size: 64\n",
      "Batch 1961 size: 64\n",
      "Batch 1962 size: 64\n",
      "Batch 1963 size: 64\n",
      "Batch 1964 size: 64\n",
      "Batch 1965 size: 64\n",
      "Batch 1966 size: 64\n",
      "Batch 1967 size: 64\n",
      "Batch 1968 size: 64\n",
      "Batch 1969 size: 64\n",
      "Batch 1970 size: 64\n",
      "Batch 1971 size: 64\n",
      "Batch 1972 size: 64\n",
      "Batch 1973 size: 64\n",
      "Batch 1974 size: 64\n",
      "Batch 1975 size: 64\n",
      "Batch 1976 size: 64\n",
      "Batch 1977 size: 64\n",
      "Batch 1978 size: 64\n",
      "Batch 1979 size: 64\n",
      "Batch 1980 size: 64\n",
      "Batch 1981 size: 64\n",
      "Batch 1982 size: 64\n",
      "Batch 1983 size: 64\n",
      "Batch 1984 size: 64\n",
      "Batch 1985 size: 64\n",
      "Batch 1986 size: 64\n",
      "Batch 1987 size: 64\n",
      "Batch 1988 size: 64\n",
      "Batch 1989 size: 64\n",
      "Batch 1990 size: 64\n",
      "Batch 1991 size: 64\n",
      "Batch 1992 size: 64\n",
      "Batch 1993 size: 64\n",
      "Batch 1994 size: 64\n",
      "Batch 1995 size: 64\n",
      "Batch 1996 size: 64\n",
      "Batch 1997 size: 64\n",
      "Batch 1998 size: 64\n",
      "Batch 1999 size: 64\n",
      "Batch 2000 size: 64\n",
      "Batch 2001 size: 64\n",
      "Batch 2002 size: 64\n",
      "Batch 2003 size: 64\n",
      "Batch 2004 size: 64\n",
      "Batch 2005 size: 64\n",
      "Batch 2006 size: 64\n",
      "Batch 2007 size: 64\n",
      "Batch 2008 size: 64\n",
      "Batch 2009 size: 64\n",
      "Batch 2010 size: 64\n",
      "Batch 2011 size: 64\n",
      "Batch 2012 size: 64\n",
      "Batch 2013 size: 64\n",
      "Batch 2014 size: 64\n",
      "Batch 2015 size: 64\n",
      "Batch 2016 size: 64\n",
      "Batch 2017 size: 64\n",
      "Batch 2018 size: 64\n",
      "Batch 2019 size: 64\n",
      "Batch 2020 size: 64\n",
      "Batch 2021 size: 64\n",
      "Batch 2022 size: 64\n",
      "Batch 2023 size: 64\n",
      "Batch 2024 size: 64\n",
      "Batch 2025 size: 64\n",
      "Batch 2026 size: 64\n",
      "Batch 2027 size: 64\n",
      "Batch 2028 size: 64\n",
      "Batch 2029 size: 64\n",
      "Batch 2030 size: 64\n",
      "Batch 2031 size: 64\n",
      "Batch 2032 size: 64\n",
      "Batch 2033 size: 64\n",
      "Batch 2034 size: 64\n",
      "Batch 2035 size: 64\n",
      "Batch 2036 size: 64\n",
      "Batch 2037 size: 64\n",
      "Batch 2038 size: 64\n",
      "Batch 2039 size: 64\n",
      "Batch 2040 size: 64\n",
      "Batch 2041 size: 64\n",
      "Batch 2042 size: 64\n",
      "Batch 2043 size: 64\n",
      "Batch 2044 size: 64\n",
      "Batch 2045 size: 64\n",
      "Batch 2046 size: 64\n",
      "Batch 2047 size: 64\n",
      "Batch 2048 size: 64\n",
      "Batch 2049 size: 64\n",
      "Batch 2050 size: 64\n",
      "Batch 2051 size: 64\n",
      "Batch 2052 size: 64\n",
      "Batch 2053 size: 64\n",
      "Batch 2054 size: 64\n",
      "Batch 2055 size: 64\n",
      "Batch 2056 size: 64\n",
      "Batch 2057 size: 64\n",
      "Batch 2058 size: 64\n",
      "Batch 2059 size: 64\n",
      "Batch 2060 size: 64\n",
      "Batch 2061 size: 64\n",
      "Batch 2062 size: 64\n",
      "Batch 2063 size: 64\n",
      "Batch 2064 size: 64\n",
      "Batch 2065 size: 64\n",
      "Batch 2066 size: 64\n",
      "Batch 2067 size: 64\n",
      "Batch 2068 size: 64\n",
      "Batch 2069 size: 64\n",
      "Batch 2070 size: 64\n",
      "Batch 2071 size: 64\n",
      "Batch 2072 size: 64\n",
      "Batch 2073 size: 64\n",
      "Batch 2074 size: 64\n",
      "Batch 2075 size: 64\n",
      "Batch 2076 size: 64\n",
      "Batch 2077 size: 64\n",
      "Batch 2078 size: 64\n",
      "Batch 2079 size: 64\n",
      "Batch 2080 size: 64\n",
      "Batch 2081 size: 64\n",
      "Batch 2082 size: 64\n",
      "Batch 2083 size: 64\n",
      "Batch 2084 size: 64\n",
      "Batch 2085 size: 64\n",
      "Batch 2086 size: 64\n",
      "Batch 2087 size: 64\n",
      "Batch 2088 size: 64\n",
      "Batch 2089 size: 64\n",
      "Batch 2090 size: 64\n",
      "Batch 2091 size: 64\n",
      "Batch 2092 size: 64\n",
      "Batch 2093 size: 64\n",
      "Batch 2094 size: 64\n",
      "Batch 2095 size: 64\n",
      "Batch 2096 size: 64\n",
      "Batch 2097 size: 64\n",
      "Batch 2098 size: 64\n",
      "Batch 2099 size: 64\n",
      "Batch 2100 size: 64\n",
      "Batch 2101 size: 64\n",
      "Batch 2102 size: 64\n",
      "Batch 2103 size: 64\n",
      "Batch 2104 size: 64\n",
      "Batch 2105 size: 64\n",
      "Batch 2106 size: 64\n",
      "Batch 2107 size: 64\n",
      "Batch 2108 size: 64\n",
      "Batch 2109 size: 64\n",
      "Batch 2110 size: 64\n",
      "Batch 2111 size: 64\n",
      "Batch 2112 size: 64\n",
      "Batch 2113 size: 64\n",
      "Batch 2114 size: 64\n",
      "Batch 2115 size: 64\n",
      "Batch 2116 size: 64\n",
      "Batch 2117 size: 64\n",
      "Batch 2118 size: 64\n",
      "Batch 2119 size: 64\n",
      "Batch 2120 size: 64\n",
      "Batch 2121 size: 64\n",
      "Batch 2122 size: 64\n",
      "Batch 2123 size: 64\n",
      "Batch 2124 size: 64\n",
      "Batch 2125 size: 64\n",
      "Batch 2126 size: 64\n",
      "Batch 2127 size: 64\n",
      "Batch 2128 size: 64\n",
      "Batch 2129 size: 64\n",
      "Batch 2130 size: 64\n",
      "Batch 2131 size: 64\n",
      "Batch 2132 size: 64\n",
      "Batch 2133 size: 64\n",
      "Batch 2134 size: 64\n",
      "Batch 2135 size: 64\n",
      "Batch 2136 size: 64\n",
      "Batch 2137 size: 64\n",
      "Batch 2138 size: 64\n",
      "Batch 2139 size: 64\n",
      "Batch 2140 size: 64\n",
      "Batch 2141 size: 64\n",
      "Batch 2142 size: 64\n",
      "Batch 2143 size: 64\n",
      "Batch 2144 size: 64\n",
      "Batch 2145 size: 64\n",
      "Batch 2146 size: 64\n",
      "Batch 2147 size: 64\n",
      "Batch 2148 size: 64\n",
      "Batch 2149 size: 64\n",
      "Batch 2150 size: 64\n",
      "Batch 2151 size: 64\n",
      "Batch 2152 size: 64\n",
      "Batch 2153 size: 64\n",
      "Batch 2154 size: 64\n",
      "Batch 2155 size: 64\n",
      "Batch 2156 size: 64\n",
      "Batch 2157 size: 64\n",
      "Batch 2158 size: 64\n",
      "Batch 2159 size: 64\n",
      "Batch 2160 size: 64\n",
      "Batch 2161 size: 64\n",
      "Batch 2162 size: 64\n",
      "Batch 2163 size: 64\n",
      "Batch 2164 size: 64\n",
      "Batch 2165 size: 64\n",
      "Batch 2166 size: 64\n",
      "Batch 2167 size: 64\n",
      "Batch 2168 size: 64\n",
      "Batch 2169 size: 64\n",
      "Batch 2170 size: 64\n",
      "Batch 2171 size: 64\n",
      "Batch 2172 size: 64\n",
      "Batch 2173 size: 64\n",
      "Batch 2174 size: 64\n",
      "Batch 2175 size: 64\n",
      "Batch 2176 size: 64\n",
      "Batch 2177 size: 64\n",
      "Batch 2178 size: 64\n",
      "Batch 2179 size: 64\n",
      "Batch 2180 size: 64\n",
      "Batch 2181 size: 64\n",
      "Batch 2182 size: 64\n",
      "Batch 2183 size: 64\n",
      "Batch 2184 size: 64\n",
      "Batch 2185 size: 64\n",
      "Batch 2186 size: 64\n",
      "Batch 2187 size: 64\n",
      "Batch 2188 size: 64\n",
      "Batch 2189 size: 64\n",
      "Batch 2190 size: 64\n",
      "Batch 2191 size: 64\n",
      "Batch 2192 size: 64\n",
      "Batch 2193 size: 64\n",
      "Batch 2194 size: 64\n",
      "Batch 2195 size: 64\n",
      "Batch 2196 size: 64\n",
      "Batch 2197 size: 64\n",
      "Batch 2198 size: 64\n",
      "Batch 2199 size: 64\n",
      "Batch 2200 size: 64\n",
      "Batch 2201 size: 64\n",
      "Batch 2202 size: 64\n",
      "Batch 2203 size: 64\n",
      "Batch 2204 size: 64\n",
      "Batch 2205 size: 64\n",
      "Batch 2206 size: 64\n",
      "Batch 2207 size: 64\n",
      "Batch 2208 size: 64\n",
      "Batch 2209 size: 64\n",
      "Batch 2210 size: 64\n",
      "Batch 2211 size: 64\n",
      "Batch 2212 size: 64\n",
      "Batch 2213 size: 64\n",
      "Batch 2214 size: 64\n",
      "Batch 2215 size: 64\n",
      "Batch 2216 size: 64\n",
      "Batch 2217 size: 64\n",
      "Batch 2218 size: 64\n",
      "Batch 2219 size: 64\n",
      "Batch 2220 size: 64\n",
      "Batch 2221 size: 64\n",
      "Batch 2222 size: 64\n",
      "Batch 2223 size: 64\n",
      "Batch 2224 size: 64\n",
      "Batch 2225 size: 64\n",
      "Batch 2226 size: 64\n",
      "Batch 2227 size: 64\n",
      "Batch 2228 size: 64\n",
      "Batch 2229 size: 64\n",
      "Batch 2230 size: 64\n",
      "Batch 2231 size: 64\n",
      "Batch 2232 size: 64\n",
      "Batch 2233 size: 64\n",
      "Batch 2234 size: 64\n",
      "Batch 2235 size: 64\n",
      "Batch 2236 size: 64\n",
      "Batch 2237 size: 64\n",
      "Batch 2238 size: 64\n",
      "Batch 2239 size: 64\n",
      "Batch 2240 size: 64\n",
      "Batch 2241 size: 64\n",
      "Batch 2242 size: 64\n",
      "Batch 2243 size: 64\n",
      "Batch 2244 size: 64\n",
      "Batch 2245 size: 64\n",
      "Batch 2246 size: 64\n",
      "Batch 2247 size: 64\n",
      "Batch 2248 size: 64\n",
      "Batch 2249 size: 64\n",
      "Batch 2250 size: 64\n",
      "Batch 2251 size: 64\n",
      "Batch 2252 size: 64\n",
      "Batch 2253 size: 64\n",
      "Batch 2254 size: 64\n",
      "Batch 2255 size: 64\n",
      "Batch 2256 size: 64\n",
      "Batch 2257 size: 64\n",
      "Batch 2258 size: 64\n",
      "Batch 2259 size: 64\n",
      "Batch 2260 size: 64\n",
      "Batch 2261 size: 64\n",
      "Batch 2262 size: 64\n",
      "Batch 2263 size: 64\n",
      "Batch 2264 size: 64\n",
      "Batch 2265 size: 64\n",
      "Batch 2266 size: 64\n",
      "Batch 2267 size: 64\n",
      "Batch 2268 size: 64\n",
      "Batch 2269 size: 64\n",
      "Batch 2270 size: 64\n",
      "Batch 2271 size: 64\n",
      "Batch 2272 size: 64\n",
      "Batch 2273 size: 64\n",
      "Batch 2274 size: 64\n",
      "Batch 2275 size: 64\n",
      "Batch 2276 size: 64\n",
      "Batch 2277 size: 64\n",
      "Batch 2278 size: 64\n",
      "Batch 2279 size: 64\n",
      "Batch 2280 size: 64\n",
      "Batch 2281 size: 64\n",
      "Batch 2282 size: 64\n",
      "Batch 2283 size: 64\n",
      "Batch 2284 size: 64\n",
      "Batch 2285 size: 64\n",
      "Batch 2286 size: 64\n",
      "Batch 2287 size: 64\n",
      "Batch 2288 size: 64\n",
      "Batch 2289 size: 64\n",
      "Batch 2290 size: 64\n",
      "Batch 2291 size: 64\n",
      "Batch 2292 size: 64\n",
      "Batch 2293 size: 64\n",
      "Batch 2294 size: 64\n",
      "Batch 2295 size: 64\n",
      "Batch 2296 size: 64\n",
      "Batch 2297 size: 64\n",
      "Batch 2298 size: 64\n",
      "Batch 2299 size: 64\n",
      "Batch 2300 size: 64\n",
      "Batch 2301 size: 64\n",
      "Batch 2302 size: 64\n",
      "Batch 2303 size: 64\n",
      "Batch 2304 size: 64\n",
      "Batch 2305 size: 64\n",
      "Batch 2306 size: 64\n",
      "Batch 2307 size: 64\n",
      "Batch 2308 size: 64\n",
      "Batch 2309 size: 64\n",
      "Batch 2310 size: 64\n",
      "Batch 2311 size: 64\n",
      "Batch 2312 size: 64\n",
      "Batch 2313 size: 64\n",
      "Batch 2314 size: 64\n",
      "Batch 2315 size: 64\n",
      "Batch 2316 size: 64\n",
      "Batch 2317 size: 64\n",
      "Batch 2318 size: 64\n",
      "Batch 2319 size: 64\n",
      "Batch 2320 size: 64\n",
      "Batch 2321 size: 64\n",
      "Batch 2322 size: 64\n",
      "Batch 2323 size: 64\n",
      "Batch 2324 size: 64\n",
      "Batch 2325 size: 64\n",
      "Batch 2326 size: 64\n",
      "Batch 2327 size: 64\n",
      "Batch 2328 size: 64\n",
      "Batch 2329 size: 64\n",
      "Batch 2330 size: 64\n",
      "Batch 2331 size: 64\n",
      "Batch 2332 size: 64\n",
      "Batch 2333 size: 64\n",
      "Batch 2334 size: 64\n",
      "Batch 2335 size: 64\n",
      "Batch 2336 size: 64\n",
      "Batch 2337 size: 64\n",
      "Batch 2338 size: 64\n",
      "Batch 2339 size: 64\n",
      "Batch 2340 size: 64\n",
      "Batch 2341 size: 64\n",
      "Batch 2342 size: 64\n",
      "Batch 2343 size: 64\n",
      "Batch 2344 size: 64\n",
      "Batch 2345 size: 64\n",
      "Batch 2346 size: 64\n",
      "Batch 2347 size: 64\n",
      "Batch 2348 size: 64\n",
      "Batch 2349 size: 64\n",
      "Batch 2350 size: 64\n",
      "Batch 2351 size: 64\n",
      "Batch 2352 size: 64\n",
      "Batch 2353 size: 64\n",
      "Batch 2354 size: 64\n",
      "Batch 2355 size: 64\n",
      "Batch 2356 size: 64\n",
      "Batch 2357 size: 64\n",
      "Batch 2358 size: 64\n",
      "Batch 2359 size: 64\n",
      "Batch 2360 size: 64\n",
      "Batch 2361 size: 64\n",
      "Batch 2362 size: 64\n",
      "Batch 2363 size: 64\n",
      "Batch 2364 size: 64\n",
      "Batch 2365 size: 64\n",
      "Batch 2366 size: 64\n",
      "Batch 2367 size: 64\n",
      "Batch 2368 size: 64\n",
      "Batch 2369 size: 64\n",
      "Batch 2370 size: 64\n",
      "Batch 2371 size: 64\n",
      "Batch 2372 size: 64\n",
      "Batch 2373 size: 64\n",
      "Batch 2374 size: 64\n",
      "Batch 2375 size: 64\n",
      "Batch 2376 size: 64\n",
      "Batch 2377 size: 64\n",
      "Batch 2378 size: 64\n",
      "Batch 2379 size: 64\n",
      "Batch 2380 size: 64\n",
      "Batch 2381 size: 64\n",
      "Batch 2382 size: 64\n",
      "Batch 2383 size: 64\n",
      "Batch 2384 size: 64\n",
      "Batch 2385 size: 64\n",
      "Batch 2386 size: 64\n",
      "Batch 2387 size: 64\n",
      "Batch 2388 size: 64\n",
      "Batch 2389 size: 64\n",
      "Batch 2390 size: 64\n",
      "Batch 2391 size: 64\n",
      "Batch 2392 size: 64\n",
      "Batch 2393 size: 64\n",
      "Batch 2394 size: 64\n",
      "Batch 2395 size: 64\n",
      "Batch 2396 size: 64\n",
      "Batch 2397 size: 64\n",
      "Batch 2398 size: 64\n",
      "Batch 2399 size: 64\n",
      "Batch 2400 size: 64\n",
      "Batch 2401 size: 64\n",
      "Batch 2402 size: 64\n",
      "Batch 2403 size: 64\n",
      "Batch 2404 size: 64\n",
      "Batch 2405 size: 64\n",
      "Batch 2406 size: 64\n",
      "Batch 2407 size: 64\n",
      "Batch 2408 size: 64\n",
      "Batch 2409 size: 64\n",
      "Batch 2410 size: 64\n",
      "Batch 2411 size: 64\n",
      "Batch 2412 size: 64\n",
      "Batch 2413 size: 64\n",
      "Batch 2414 size: 64\n",
      "Batch 2415 size: 64\n",
      "Batch 2416 size: 64\n",
      "Batch 2417 size: 64\n",
      "Batch 2418 size: 64\n",
      "Batch 2419 size: 64\n",
      "Batch 2420 size: 64\n",
      "Batch 2421 size: 64\n",
      "Batch 2422 size: 64\n",
      "Batch 2423 size: 64\n",
      "Batch 2424 size: 64\n",
      "Batch 2425 size: 64\n",
      "Batch 2426 size: 64\n",
      "Batch 2427 size: 64\n",
      "Batch 2428 size: 64\n",
      "Batch 2429 size: 64\n",
      "Batch 2430 size: 64\n",
      "Batch 2431 size: 64\n",
      "Batch 2432 size: 64\n",
      "Batch 2433 size: 64\n",
      "Batch 2434 size: 64\n",
      "Batch 2435 size: 64\n",
      "Batch 2436 size: 64\n",
      "Batch 2437 size: 64\n",
      "Batch 2438 size: 64\n",
      "Batch 2439 size: 64\n",
      "Batch 2440 size: 64\n",
      "Batch 2441 size: 64\n",
      "Batch 2442 size: 64\n",
      "Batch 2443 size: 64\n",
      "Batch 2444 size: 64\n",
      "Batch 2445 size: 64\n",
      "Batch 2446 size: 64\n",
      "Batch 2447 size: 64\n",
      "Batch 2448 size: 64\n",
      "Batch 2449 size: 64\n",
      "Batch 2450 size: 64\n",
      "Batch 2451 size: 64\n",
      "Batch 2452 size: 64\n",
      "Batch 2453 size: 64\n",
      "Batch 2454 size: 64\n",
      "Batch 2455 size: 64\n",
      "Batch 2456 size: 64\n",
      "Batch 2457 size: 64\n",
      "Batch 2458 size: 64\n",
      "Batch 2459 size: 64\n",
      "Batch 2460 size: 64\n",
      "Batch 2461 size: 64\n",
      "Batch 2462 size: 64\n",
      "Batch 2463 size: 64\n",
      "Batch 2464 size: 64\n",
      "Batch 2465 size: 64\n",
      "Batch 2466 size: 64\n",
      "Batch 2467 size: 64\n",
      "Batch 2468 size: 64\n",
      "Batch 2469 size: 64\n",
      "Batch 2470 size: 64\n",
      "Batch 2471 size: 64\n",
      "Batch 2472 size: 64\n",
      "Batch 2473 size: 64\n",
      "Batch 2474 size: 64\n",
      "Batch 2475 size: 64\n",
      "Batch 2476 size: 64\n",
      "Batch 2477 size: 64\n",
      "Batch 2478 size: 64\n",
      "Batch 2479 size: 64\n",
      "Batch 2480 size: 64\n",
      "Batch 2481 size: 64\n",
      "Batch 2482 size: 64\n",
      "Batch 2483 size: 64\n",
      "Batch 2484 size: 64\n",
      "Batch 2485 size: 64\n",
      "Batch 2486 size: 64\n",
      "Batch 2487 size: 64\n",
      "Batch 2488 size: 64\n",
      "Batch 2489 size: 64\n",
      "Batch 2490 size: 64\n",
      "Batch 2491 size: 64\n",
      "Batch 2492 size: 64\n",
      "Batch 2493 size: 64\n",
      "Batch 2494 size: 64\n",
      "Batch 2495 size: 64\n",
      "Batch 2496 size: 64\n",
      "Batch 2497 size: 64\n",
      "Batch 2498 size: 64\n",
      "Batch 2499 size: 64\n",
      "Batch 2500 size: 64\n",
      "Batch 2501 size: 64\n",
      "Batch 2502 size: 64\n",
      "Batch 2503 size: 64\n",
      "Batch 2504 size: 64\n",
      "Batch 2505 size: 64\n",
      "Batch 2506 size: 64\n",
      "Batch 2507 size: 64\n",
      "Batch 2508 size: 64\n",
      "Batch 2509 size: 64\n",
      "Batch 2510 size: 64\n",
      "Batch 2511 size: 64\n",
      "Batch 2512 size: 64\n",
      "Batch 2513 size: 64\n",
      "Batch 2514 size: 64\n",
      "Batch 2515 size: 64\n",
      "Batch 2516 size: 64\n",
      "Batch 2517 size: 64\n",
      "Batch 2518 size: 64\n",
      "Batch 2519 size: 64\n",
      "Batch 2520 size: 64\n",
      "Batch 2521 size: 64\n",
      "Batch 2522 size: 64\n",
      "Batch 2523 size: 64\n",
      "Batch 2524 size: 64\n",
      "Batch 2525 size: 64\n",
      "Batch 2526 size: 64\n",
      "Batch 2527 size: 64\n",
      "Batch 2528 size: 64\n",
      "Batch 2529 size: 64\n",
      "Batch 2530 size: 64\n",
      "Batch 2531 size: 64\n",
      "Batch 2532 size: 64\n",
      "Batch 2533 size: 64\n",
      "Batch 2534 size: 64\n",
      "Batch 2535 size: 64\n",
      "Batch 2536 size: 64\n",
      "Batch 2537 size: 64\n",
      "Batch 2538 size: 64\n",
      "Batch 2539 size: 64\n",
      "Batch 2540 size: 64\n",
      "Batch 2541 size: 64\n",
      "Batch 2542 size: 64\n",
      "Batch 2543 size: 64\n",
      "Batch 2544 size: 64\n",
      "Batch 2545 size: 64\n",
      "Batch 2546 size: 64\n",
      "Batch 2547 size: 64\n",
      "Batch 2548 size: 64\n",
      "Batch 2549 size: 64\n",
      "Batch 2550 size: 64\n",
      "Batch 2551 size: 64\n",
      "Batch 2552 size: 64\n",
      "Batch 2553 size: 64\n",
      "Batch 2554 size: 64\n",
      "Batch 2555 size: 64\n",
      "Batch 2556 size: 64\n",
      "Batch 2557 size: 64\n",
      "Batch 2558 size: 64\n",
      "Batch 2559 size: 64\n",
      "Batch 2560 size: 64\n",
      "Batch 2561 size: 64\n",
      "Batch 2562 size: 64\n",
      "Batch 2563 size: 64\n",
      "Batch 2564 size: 64\n",
      "Batch 2565 size: 64\n",
      "Batch 2566 size: 64\n",
      "Batch 2567 size: 64\n",
      "Batch 2568 size: 64\n",
      "Batch 2569 size: 64\n",
      "Batch 2570 size: 64\n",
      "Batch 2571 size: 64\n",
      "Batch 2572 size: 64\n",
      "Batch 2573 size: 64\n",
      "Batch 2574 size: 64\n",
      "Batch 2575 size: 64\n",
      "Batch 2576 size: 64\n",
      "Batch 2577 size: 64\n",
      "Batch 2578 size: 64\n",
      "Batch 2579 size: 64\n",
      "Batch 2580 size: 64\n",
      "Batch 2581 size: 64\n",
      "Batch 2582 size: 64\n",
      "Batch 2583 size: 64\n",
      "Batch 2584 size: 64\n",
      "Batch 2585 size: 64\n",
      "Batch 2586 size: 64\n",
      "Batch 2587 size: 64\n",
      "Batch 2588 size: 64\n",
      "Batch 2589 size: 64\n",
      "Batch 2590 size: 64\n",
      "Batch 2591 size: 64\n",
      "Batch 2592 size: 64\n",
      "Batch 2593 size: 64\n",
      "Batch 2594 size: 64\n",
      "Batch 2595 size: 64\n",
      "Batch 2596 size: 64\n",
      "Batch 2597 size: 64\n",
      "Batch 2598 size: 64\n",
      "Batch 2599 size: 64\n",
      "Batch 2600 size: 64\n",
      "Batch 2601 size: 64\n",
      "Batch 2602 size: 64\n",
      "Batch 2603 size: 64\n",
      "Batch 2604 size: 64\n",
      "Batch 2605 size: 64\n",
      "Batch 2606 size: 64\n",
      "Batch 2607 size: 64\n",
      "Batch 2608 size: 64\n",
      "Batch 2609 size: 64\n",
      "Batch 2610 size: 64\n",
      "Batch 2611 size: 64\n",
      "Batch 2612 size: 64\n",
      "Batch 2613 size: 64\n",
      "Batch 2614 size: 64\n",
      "Batch 2615 size: 64\n",
      "Batch 2616 size: 64\n",
      "Batch 2617 size: 64\n",
      "Batch 2618 size: 64\n",
      "Batch 2619 size: 64\n",
      "Batch 2620 size: 64\n",
      "Batch 2621 size: 64\n",
      "Batch 2622 size: 64\n",
      "Batch 2623 size: 64\n",
      "Batch 2624 size: 64\n",
      "Batch 2625 size: 64\n",
      "Batch 2626 size: 64\n",
      "Batch 2627 size: 64\n",
      "Batch 2628 size: 64\n",
      "Batch 2629 size: 64\n",
      "Batch 2630 size: 64\n",
      "Batch 2631 size: 64\n",
      "Batch 2632 size: 64\n",
      "Batch 2633 size: 64\n",
      "Batch 2634 size: 64\n",
      "Batch 2635 size: 64\n",
      "Batch 2636 size: 64\n",
      "Batch 2637 size: 64\n",
      "Batch 2638 size: 64\n",
      "Batch 2639 size: 64\n",
      "Batch 2640 size: 64\n",
      "Batch 2641 size: 64\n",
      "Batch 2642 size: 64\n",
      "Batch 2643 size: 64\n",
      "Batch 2644 size: 64\n",
      "Batch 2645 size: 64\n",
      "Batch 2646 size: 64\n",
      "Batch 2647 size: 64\n",
      "Batch 2648 size: 64\n",
      "Batch 2649 size: 64\n",
      "Batch 2650 size: 64\n",
      "Batch 2651 size: 64\n",
      "Batch 2652 size: 64\n",
      "Batch 2653 size: 64\n",
      "Batch 2654 size: 64\n",
      "Batch 2655 size: 64\n",
      "Batch 2656 size: 64\n",
      "Batch 2657 size: 64\n",
      "Batch 2658 size: 64\n",
      "Batch 2659 size: 64\n",
      "Batch 2660 size: 64\n",
      "Batch 2661 size: 64\n",
      "Batch 2662 size: 64\n",
      "Batch 2663 size: 64\n",
      "Batch 2664 size: 64\n",
      "Batch 2665 size: 64\n",
      "Batch 2666 size: 64\n",
      "Batch 2667 size: 64\n",
      "Batch 2668 size: 64\n",
      "Batch 2669 size: 64\n",
      "Batch 2670 size: 64\n",
      "Batch 2671 size: 64\n",
      "Batch 2672 size: 64\n",
      "Batch 2673 size: 64\n",
      "Batch 2674 size: 64\n",
      "Batch 2675 size: 64\n",
      "Batch 2676 size: 64\n",
      "Batch 2677 size: 64\n",
      "Batch 2678 size: 64\n",
      "Batch 2679 size: 64\n",
      "Batch 2680 size: 64\n",
      "Batch 2681 size: 64\n",
      "Batch 2682 size: 64\n",
      "Batch 2683 size: 64\n",
      "Batch 2684 size: 64\n",
      "Batch 2685 size: 64\n",
      "Batch 2686 size: 64\n",
      "Batch 2687 size: 64\n",
      "Batch 2688 size: 64\n",
      "Batch 2689 size: 64\n",
      "Batch 2690 size: 64\n",
      "Batch 2691 size: 64\n",
      "Batch 2692 size: 64\n",
      "Batch 2693 size: 64\n",
      "Batch 2694 size: 64\n",
      "Batch 2695 size: 64\n",
      "Batch 2696 size: 64\n",
      "Batch 2697 size: 64\n",
      "Batch 2698 size: 64\n",
      "Batch 2699 size: 64\n",
      "Batch 2700 size: 64\n",
      "Batch 2701 size: 64\n",
      "Batch 2702 size: 64\n",
      "Batch 2703 size: 64\n",
      "Batch 2704 size: 64\n",
      "Batch 2705 size: 64\n",
      "Batch 2706 size: 64\n",
      "Batch 2707 size: 64\n",
      "Batch 2708 size: 64\n",
      "Batch 2709 size: 64\n",
      "Batch 2710 size: 64\n",
      "Batch 2711 size: 64\n",
      "Batch 2712 size: 64\n",
      "Batch 2713 size: 64\n",
      "Batch 2714 size: 64\n",
      "Batch 2715 size: 64\n",
      "Batch 2716 size: 64\n",
      "Batch 2717 size: 64\n",
      "Batch 2718 size: 64\n",
      "Batch 2719 size: 64\n",
      "Batch 2720 size: 64\n",
      "Batch 2721 size: 64\n",
      "Batch 2722 size: 64\n",
      "Batch 2723 size: 64\n",
      "Batch 2724 size: 64\n",
      "Batch 2725 size: 64\n",
      "Batch 2726 size: 64\n",
      "Batch 2727 size: 64\n",
      "Batch 2728 size: 64\n",
      "Batch 2729 size: 64\n",
      "Batch 2730 size: 64\n",
      "Batch 2731 size: 64\n",
      "Batch 2732 size: 64\n",
      "Batch 2733 size: 64\n",
      "Batch 2734 size: 64\n",
      "Batch 2735 size: 64\n",
      "Batch 2736 size: 64\n",
      "Batch 2737 size: 64\n",
      "Batch 2738 size: 64\n",
      "Batch 2739 size: 64\n",
      "Batch 2740 size: 64\n",
      "Batch 2741 size: 64\n",
      "Batch 2742 size: 64\n",
      "Batch 2743 size: 64\n",
      "Batch 2744 size: 64\n",
      "Batch 2745 size: 64\n",
      "Batch 2746 size: 64\n",
      "Batch 2747 size: 64\n",
      "Batch 2748 size: 64\n",
      "Batch 2749 size: 64\n",
      "Batch 2750 size: 64\n",
      "Batch 2751 size: 64\n",
      "Batch 2752 size: 64\n",
      "Batch 2753 size: 64\n",
      "Batch 2754 size: 64\n",
      "Batch 2755 size: 64\n",
      "Batch 2756 size: 64\n",
      "Batch 2757 size: 64\n",
      "Batch 2758 size: 64\n",
      "Batch 2759 size: 64\n",
      "Batch 2760 size: 64\n",
      "Batch 2761 size: 64\n",
      "Batch 2762 size: 64\n",
      "Batch 2763 size: 64\n",
      "Batch 2764 size: 64\n",
      "Batch 2765 size: 64\n",
      "Batch 2766 size: 64\n",
      "Batch 2767 size: 64\n",
      "Batch 2768 size: 64\n",
      "Batch 2769 size: 64\n",
      "Batch 2770 size: 64\n",
      "Batch 2771 size: 64\n",
      "Batch 2772 size: 64\n",
      "Batch 2773 size: 64\n",
      "Batch 2774 size: 64\n",
      "Batch 2775 size: 64\n",
      "Batch 2776 size: 64\n",
      "Batch 2777 size: 64\n",
      "Batch 2778 size: 64\n",
      "Batch 2779 size: 64\n",
      "Batch 2780 size: 64\n",
      "Batch 2781 size: 64\n",
      "Batch 2782 size: 64\n",
      "Batch 2783 size: 64\n",
      "Batch 2784 size: 64\n",
      "Batch 2785 size: 64\n",
      "Batch 2786 size: 64\n",
      "Batch 2787 size: 64\n",
      "Batch 2788 size: 64\n",
      "Batch 2789 size: 64\n",
      "Batch 2790 size: 64\n",
      "Batch 2791 size: 64\n",
      "Batch 2792 size: 64\n",
      "Batch 2793 size: 64\n",
      "Batch 2794 size: 64\n",
      "Batch 2795 size: 64\n",
      "Batch 2796 size: 64\n",
      "Batch 2797 size: 64\n",
      "Batch 2798 size: 64\n",
      "Batch 2799 size: 64\n",
      "Batch 2800 size: 64\n",
      "Batch 2801 size: 64\n",
      "Batch 2802 size: 64\n",
      "Batch 2803 size: 64\n",
      "Batch 2804 size: 64\n",
      "Batch 2805 size: 64\n",
      "Batch 2806 size: 64\n",
      "Batch 2807 size: 64\n",
      "Batch 2808 size: 64\n",
      "Batch 2809 size: 64\n",
      "Batch 2810 size: 64\n",
      "Batch 2811 size: 64\n",
      "Batch 2812 size: 64\n",
      "Batch 2813 size: 64\n",
      "Batch 2814 size: 64\n",
      "Batch 2815 size: 64\n",
      "Batch 2816 size: 64\n",
      "Batch 2817 size: 64\n",
      "Batch 2818 size: 64\n",
      "Batch 2819 size: 64\n",
      "Batch 2820 size: 64\n",
      "Batch 2821 size: 64\n",
      "Batch 2822 size: 64\n",
      "Batch 2823 size: 64\n",
      "Batch 2824 size: 64\n",
      "Batch 2825 size: 64\n",
      "Batch 2826 size: 64\n",
      "Batch 2827 size: 64\n",
      "Batch 2828 size: 64\n",
      "Batch 2829 size: 64\n",
      "Batch 2830 size: 64\n",
      "Batch 2831 size: 64\n",
      "Batch 2832 size: 64\n",
      "Batch 2833 size: 64\n",
      "Batch 2834 size: 64\n",
      "Batch 2835 size: 64\n",
      "Batch 2836 size: 64\n",
      "Batch 2837 size: 64\n",
      "Batch 2838 size: 64\n",
      "Batch 2839 size: 64\n",
      "Batch 2840 size: 64\n",
      "Batch 2841 size: 64\n",
      "Batch 2842 size: 64\n",
      "Batch 2843 size: 64\n",
      "Batch 2844 size: 64\n",
      "Batch 2845 size: 64\n",
      "Batch 2846 size: 64\n",
      "Batch 2847 size: 64\n",
      "Batch 2848 size: 64\n",
      "Batch 2849 size: 64\n",
      "Batch 2850 size: 64\n",
      "Batch 2851 size: 64\n",
      "Batch 2852 size: 64\n",
      "Batch 2853 size: 64\n",
      "Batch 2854 size: 64\n",
      "Batch 2855 size: 64\n",
      "Batch 2856 size: 64\n",
      "Batch 2857 size: 64\n",
      "Batch 2858 size: 64\n",
      "Batch 2859 size: 64\n",
      "Batch 2860 size: 64\n",
      "Batch 2861 size: 64\n",
      "Batch 2862 size: 64\n",
      "Batch 2863 size: 64\n",
      "Batch 2864 size: 64\n",
      "Batch 2865 size: 64\n",
      "Batch 2866 size: 64\n",
      "Batch 2867 size: 64\n",
      "Batch 2868 size: 64\n",
      "Batch 2869 size: 64\n",
      "Batch 2870 size: 64\n",
      "Batch 2871 size: 64\n",
      "Batch 2872 size: 64\n",
      "Batch 2873 size: 64\n",
      "Batch 2874 size: 64\n",
      "Batch 2875 size: 64\n",
      "Batch 2876 size: 64\n",
      "Batch 2877 size: 64\n",
      "Batch 2878 size: 64\n",
      "Batch 2879 size: 64\n",
      "Batch 2880 size: 64\n",
      "Batch 2881 size: 64\n",
      "Batch 2882 size: 64\n",
      "Batch 2883 size: 64\n",
      "Batch 2884 size: 64\n",
      "Batch 2885 size: 64\n",
      "Batch 2886 size: 64\n",
      "Batch 2887 size: 64\n",
      "Batch 2888 size: 64\n",
      "Batch 2889 size: 64\n",
      "Batch 2890 size: 64\n",
      "Batch 2891 size: 64\n",
      "Batch 2892 size: 64\n",
      "Batch 2893 size: 64\n",
      "Batch 2894 size: 64\n",
      "Batch 2895 size: 64\n",
      "Batch 2896 size: 64\n",
      "Batch 2897 size: 64\n",
      "Batch 2898 size: 64\n",
      "Batch 2899 size: 64\n",
      "Batch 2900 size: 64\n",
      "Batch 2901 size: 64\n",
      "Batch 2902 size: 64\n",
      "Batch 2903 size: 64\n",
      "Batch 2904 size: 64\n",
      "Batch 2905 size: 64\n",
      "Batch 2906 size: 64\n",
      "Batch 2907 size: 64\n",
      "Batch 2908 size: 64\n",
      "Batch 2909 size: 64\n",
      "Batch 2910 size: 64\n",
      "Batch 2911 size: 64\n",
      "Batch 2912 size: 64\n",
      "Batch 2913 size: 64\n",
      "Batch 2914 size: 64\n",
      "Batch 2915 size: 64\n",
      "Batch 2916 size: 64\n",
      "Batch 2917 size: 64\n",
      "Batch 2918 size: 64\n",
      "Batch 2919 size: 64\n",
      "Batch 2920 size: 64\n",
      "Batch 2921 size: 64\n",
      "Batch 2922 size: 64\n",
      "Batch 2923 size: 64\n",
      "Batch 2924 size: 64\n",
      "Batch 2925 size: 64\n",
      "Batch 2926 size: 64\n",
      "Batch 2927 size: 64\n",
      "Batch 2928 size: 64\n",
      "Batch 2929 size: 64\n",
      "Batch 2930 size: 64\n",
      "Batch 2931 size: 64\n",
      "Batch 2932 size: 64\n",
      "Batch 2933 size: 64\n",
      "Batch 2934 size: 64\n",
      "Batch 2935 size: 64\n",
      "Batch 2936 size: 64\n",
      "Batch 2937 size: 64\n",
      "Batch 2938 size: 64\n",
      "Batch 2939 size: 64\n",
      "Batch 2940 size: 64\n",
      "Batch 2941 size: 64\n",
      "Batch 2942 size: 64\n",
      "Batch 2943 size: 64\n",
      "Batch 2944 size: 64\n",
      "Batch 2945 size: 64\n",
      "Batch 2946 size: 64\n",
      "Batch 2947 size: 64\n",
      "Batch 2948 size: 64\n",
      "Batch 2949 size: 64\n",
      "Batch 2950 size: 64\n",
      "Batch 2951 size: 64\n",
      "Batch 2952 size: 64\n",
      "Batch 2953 size: 64\n",
      "Batch 2954 size: 64\n",
      "Batch 2955 size: 64\n",
      "Batch 2956 size: 64\n",
      "Batch 2957 size: 64\n",
      "Batch 2958 size: 64\n",
      "Batch 2959 size: 64\n",
      "Batch 2960 size: 64\n",
      "Batch 2961 size: 64\n",
      "Batch 2962 size: 64\n",
      "Batch 2963 size: 64\n",
      "Batch 2964 size: 64\n",
      "Batch 2965 size: 64\n",
      "Batch 2966 size: 64\n",
      "Batch 2967 size: 64\n",
      "Batch 2968 size: 64\n",
      "Batch 2969 size: 64\n",
      "Batch 2970 size: 64\n",
      "Batch 2971 size: 64\n",
      "Batch 2972 size: 64\n",
      "Batch 2973 size: 64\n",
      "Batch 2974 size: 64\n",
      "Batch 2975 size: 64\n",
      "Batch 2976 size: 64\n",
      "Batch 2977 size: 64\n",
      "Batch 2978 size: 64\n",
      "Batch 2979 size: 64\n",
      "Batch 2980 size: 64\n",
      "Batch 2981 size: 64\n",
      "Batch 2982 size: 64\n",
      "Batch 2983 size: 64\n",
      "Batch 2984 size: 64\n",
      "Batch 2985 size: 64\n",
      "Batch 2986 size: 64\n",
      "Batch 2987 size: 64\n",
      "Batch 2988 size: 64\n",
      "Batch 2989 size: 64\n",
      "Batch 2990 size: 64\n",
      "Batch 2991 size: 64\n",
      "Batch 2992 size: 64\n",
      "Batch 2993 size: 64\n",
      "Batch 2994 size: 64\n",
      "Batch 2995 size: 64\n",
      "Batch 2996 size: 64\n",
      "Batch 2997 size: 64\n",
      "Batch 2998 size: 64\n",
      "Batch 2999 size: 64\n",
      "Batch 3000 size: 64\n",
      "Batch 3001 size: 64\n",
      "Batch 3002 size: 64\n",
      "Batch 3003 size: 64\n",
      "Batch 3004 size: 64\n",
      "Batch 3005 size: 64\n",
      "Batch 3006 size: 64\n",
      "Batch 3007 size: 64\n",
      "Batch 3008 size: 64\n",
      "Batch 3009 size: 64\n",
      "Batch 3010 size: 64\n",
      "Batch 3011 size: 64\n",
      "Batch 3012 size: 64\n",
      "Batch 3013 size: 64\n",
      "Batch 3014 size: 64\n",
      "Batch 3015 size: 64\n",
      "Batch 3016 size: 64\n",
      "Batch 3017 size: 64\n",
      "Batch 3018 size: 64\n",
      "Batch 3019 size: 64\n",
      "Batch 3020 size: 64\n",
      "Batch 3021 size: 64\n",
      "Batch 3022 size: 64\n",
      "Batch 3023 size: 64\n",
      "Batch 3024 size: 64\n",
      "Batch 3025 size: 64\n",
      "Batch 3026 size: 64\n",
      "Batch 3027 size: 64\n",
      "Batch 3028 size: 64\n",
      "Batch 3029 size: 64\n",
      "Batch 3030 size: 64\n",
      "Batch 3031 size: 64\n",
      "Batch 3032 size: 64\n",
      "Batch 3033 size: 64\n",
      "Batch 3034 size: 64\n",
      "Batch 3035 size: 64\n",
      "Batch 3036 size: 64\n",
      "Batch 3037 size: 64\n",
      "Batch 3038 size: 64\n",
      "Batch 3039 size: 64\n",
      "Batch 3040 size: 64\n",
      "Batch 3041 size: 64\n",
      "Batch 3042 size: 64\n",
      "Batch 3043 size: 64\n",
      "Batch 3044 size: 64\n",
      "Batch 3045 size: 64\n",
      "Batch 3046 size: 64\n",
      "Batch 3047 size: 64\n",
      "Batch 3048 size: 64\n",
      "Batch 3049 size: 64\n",
      "Batch 3050 size: 64\n",
      "Batch 3051 size: 64\n",
      "Batch 3052 size: 64\n",
      "Batch 3053 size: 64\n",
      "Batch 3054 size: 64\n",
      "Batch 3055 size: 64\n",
      "Batch 3056 size: 64\n",
      "Batch 3057 size: 64\n",
      "Batch 3058 size: 64\n",
      "Batch 3059 size: 64\n",
      "Batch 3060 size: 64\n",
      "Batch 3061 size: 64\n",
      "Batch 3062 size: 64\n",
      "Batch 3063 size: 64\n",
      "Batch 3064 size: 64\n",
      "Batch 3065 size: 64\n",
      "Batch 3066 size: 64\n",
      "Batch 3067 size: 64\n",
      "Batch 3068 size: 64\n",
      "Batch 3069 size: 64\n",
      "Batch 3070 size: 64\n",
      "Batch 3071 size: 64\n",
      "Batch 3072 size: 64\n",
      "Batch 3073 size: 64\n",
      "Batch 3074 size: 64\n",
      "Batch 3075 size: 64\n",
      "Batch 3076 size: 64\n",
      "Batch 3077 size: 64\n",
      "Batch 3078 size: 64\n",
      "Batch 3079 size: 64\n",
      "Batch 3080 size: 64\n",
      "Batch 3081 size: 64\n",
      "Batch 3082 size: 64\n",
      "Batch 3083 size: 64\n",
      "Batch 3084 size: 64\n",
      "Batch 3085 size: 64\n",
      "Batch 3086 size: 64\n",
      "Batch 3087 size: 64\n",
      "Batch 3088 size: 64\n",
      "Batch 3089 size: 64\n",
      "Batch 3090 size: 64\n",
      "Batch 3091 size: 64\n",
      "Batch 3092 size: 64\n",
      "Batch 3093 size: 64\n",
      "Batch 3094 size: 64\n",
      "Batch 3095 size: 64\n",
      "Batch 3096 size: 64\n",
      "Batch 3097 size: 64\n",
      "Batch 3098 size: 64\n",
      "Batch 3099 size: 64\n",
      "Batch 3100 size: 64\n",
      "Batch 3101 size: 64\n",
      "Batch 3102 size: 64\n",
      "Batch 3103 size: 64\n",
      "Batch 3104 size: 64\n",
      "Batch 3105 size: 64\n",
      "Batch 3106 size: 64\n",
      "Batch 3107 size: 64\n",
      "Batch 3108 size: 64\n",
      "Batch 3109 size: 64\n",
      "Batch 3110 size: 64\n",
      "Batch 3111 size: 64\n",
      "Batch 3112 size: 64\n",
      "Batch 3113 size: 64\n",
      "Batch 3114 size: 64\n",
      "Batch 3115 size: 64\n",
      "Batch 3116 size: 64\n",
      "Batch 3117 size: 64\n",
      "Batch 3118 size: 64\n",
      "Batch 3119 size: 64\n",
      "Batch 3120 size: 64\n",
      "Batch 3121 size: 64\n",
      "Batch 3122 size: 64\n",
      "Batch 3123 size: 64\n",
      "Batch 3124 size: 64\n",
      "Batch 3125 size: 64\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(Dataloaders['Test']):\n",
    "    batch_size = len(batch[0])  # Assuming the first element of the batch is the data\n",
    "    print(f\"Batch {i+1} size: {batch_size}\")\n",
    "    if batch_size != 64:\n",
    "        print(f\"Batch {i+1} does not contain 64 records.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mZH7kgxF9p00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1670395868510,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "mZH7kgxF9p00",
    "outputId": "3b5f2da8-cb90-43b1-ea26-b5a9fad4dd31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 size: 64\n",
      "Batch 2 size: 64\n",
      "Batch 3 size: 64\n",
      "Batch 4 size: 64\n",
      "Batch 5 size: 64\n",
      "Batch 6 size: 64\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(Dataloaders[160][0]):\n",
    "    batch_size = len(batch[0])  # Assuming the first element of the batch is the data\n",
    "    print(f\"Batch {i+1} size: {batch_size}\")\n",
    "    if batch_size != 64:\n",
    "        print(f\"Batch {i+1} does not contain 64 records.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4c70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29.8828125, 100.0, 71.6796875]\n",
      "[50.520833333333336, 29.622395833333332, 100.0]\n",
      "[68.42447916666667, 48.50260416666667, 29.752604166666668]\n",
      "[100.0, 71.09375, 51.5625]\n",
      "[29.296875, 100.0, 69.921875]\n",
      "[48.95833333333333, 28.255208333333332, 100.0]\n",
      "[69.46614583333333, 49.73958333333333, 29.557291666666668]\n",
      "[100.0, 70.24739583333334, 49.479166666666664]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "for CLUSTER in range (1, 9):\n",
    "    DEVICE_PERCENTAGE = []\n",
    "    for DEVICE__ in range(0, 12):\n",
    "        for i, batch in enumerate(Dataloaders[CLUSTER][DEVICE__]):\n",
    "            _, labels = batch\n",
    "            class_counts = Counter(labels.numpy())\n",
    "            total_records = sum(class_counts.values())\n",
    "            class_0_count = class_counts.get(0, 0)\n",
    "            percentage_class_0 = (class_0_count / total_records) * 100\n",
    "            DEVICE_PERCENTAGE.append(percentage_class_0)\n",
    "            # print(f\"Batch {i+1}: {dict(class_counts)}\")\n",
    "            # print(f\"Percentage of class 0: {percentage_class_0:.2f}%\\n\")\n",
    "    # print(DEVICE_PERCENTAGE)        \n",
    "    chunk_size = 6\n",
    "    averages = [sum(DEVICE_PERCENTAGE[i:i + chunk_size]) / chunk_size for i in range(0, len(DEVICE_PERCENTAGE), chunk_size)]\n",
    "    # print(\"Averages of every device:\")\n",
    "    # print(averages)\n",
    "    chunk_size_4 = 4\n",
    "    averages = [sum(averages[i:i + chunk_size_4]) / chunk_size_4 for i in range(0, len(averages), chunk_size_4)]\n",
    "    # print(\"Averages of every 4 devices:\")\n",
    "    print(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3a112-2124-4571-802b-61850c0be001",
   "metadata": {},
   "outputs": [],
   "source": [
    "del TrafficData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1f66c2-dd6f-458f-93e3-85b93ef1fb50",
   "metadata": {
    "id": "gjVnC-rj9gC4",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='Red'>***Neural Network***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fLD8dUBg9pyY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1670813208940,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "fLD8dUBg9pyY",
    "outputId": "6df101c4-37c9-4660-f5c7-3a42be8b8951"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()        \n",
    "        self.layer_1 = nn.Linear(98, 64)\n",
    "        self.layer_2 = nn.Linear(64, 32)\n",
    "        self.layer_out = nn.Linear(32, 15) \n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_out(x)        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc624e0-c16a-413d-bb70-c7fc7743def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random  = Net()\n",
    "# for param_tensor in Random.state_dict():\n",
    "#     print(param_tensor, \"\\t\", Random.state_dict()[param_tensor].size())\n",
    "# torch.save(Random.state_dict(), \"0_Input_Random_model_Net.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SKRdGrET9pvn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1670813234860,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "SKRdGrET9pvn",
    "outputId": "fe9c8747-dad6-4606-f067-021d127d259c"
   },
   "outputs": [],
   "source": [
    "def train(net, trainloader, epochs: int, verbose=True):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    net.train()\n",
    "    prediction_matrix = []\n",
    "    actual_matrix= []\n",
    "    acc_matrix = []\n",
    "    loss_matrix=[]\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(net(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "            predictions = torch.max(outputs.data, 1)[1]\n",
    "            prediction_matrix.append(predictions.tolist())\n",
    "            actual_matrix.append(labels.tolist())\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        loss_matrix.append(epoch_loss.tolist())\n",
    "        acc_matrix.append(epoch_acc)\n",
    "    return prediction_matrix, actual_matrix, acc_matrix, loss_matrix\n",
    "def test(net, testloader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    prediction_matrix = []\n",
    "    actual_matrix= []\n",
    "    acc_matrix = []\n",
    "    loss_matrix=[]\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            prediction_matrix.append(predicted.tolist())\n",
    "            actual_matrix.append(labels.tolist())\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    loss_matrix.append(loss)\n",
    "    acc_matrix.append(accuracy)    \n",
    "    print(f\"Evaluation: eval loss {loss}, eval accuracy {accuracy}\")\n",
    "    return loss, accuracy, prediction_matrix, actual_matrix, acc_matrix, loss_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otE9jhmS-IXU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1670813239232,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "otE9jhmS-IXU",
    "outputId": "ec01b70c-2185-4293-87ec-b3eaa8b6d92a"
   },
   "outputs": [],
   "source": [
    "prediction_dict= {}\n",
    "actual_dict= {}\n",
    "accuracy_dict= {}\n",
    "loss_dict= {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2GrOePXWWEzi",
   "metadata": {
    "id": "2GrOePXWWEzi",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='Brown'>***Federated Learning Classes***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fk8W4F6K8Sy1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1670813254332,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "fk8W4F6K8Sy1",
    "outputId": "a8c6b6fe-b416-4a5f-f41f-68a76993e47a",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_parameters\u001b[39m(net) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mList\u001b[49m[np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [val\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m _, val \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mstate_dict()\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mset_parameters\u001b[39m(net, parameters: List[np\u001b[38;5;241m.\u001b[39mndarray]):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WMjtRo9r8Sv7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 155,
     "status": "ok",
     "timestamp": 1670813544785,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "WMjtRo9r8Sv7",
    "outputId": "056c1096-a023-47b7-9668-409abbf3b50f"
   },
   "outputs": [],
   "source": [
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, FL_Update):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.FL_Update = FL_Update\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "    def fit(self, parameters, config):\n",
    "        local_epochs = config[\"local_epochs\"]\n",
    "        print(f\"[Client {self.cid}, round {self.FL_Update}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        _1, _2, _3, _4 = train(self.net, self.trainloader, epochs=local_epochs)\n",
    "        prediction_dict[f'C{self.cid}R{self.FL_Update}'] = _1\n",
    "        actual_dict[f'C{self.cid}R{self.FL_Update}'] = _2\n",
    "        accuracy_dict[f'C{self.cid}R{self.FL_Update}'] = _3\n",
    "        loss_dict[f'C{self.cid}R{self.FL_Update}'] = _4\n",
    "        # Save model updates (parameters)\n",
    "        # update_filename = f'EdgeCooperation/Performance/Results/C{self.cid}R{self.FL_Update}_update.pkl'\n",
    "        # with open(update_filename, 'wb') as update_outfile:\n",
    "        #     pickle.dump(get_parameters(self.net), update_outfile)        \n",
    "\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}\")\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0abdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_sort_client_updates(global_model, round_number, client_ids):\n",
    "    client_updates = {}\n",
    "    for cid in client_ids:\n",
    "        update_filename = f'EdgeCooperation/Performance/Results/C{cid}R{round_number}_update.pkl'\n",
    "        with open(update_filename, 'rb') as update_file:\n",
    "            client_update = pickle.load(update_file)\n",
    "            client_updates[cid] = client_update\n",
    "    client_contributions = {cid: calculate_weight_magnitude(global_model, update) for cid, update in client_updates.items()}\n",
    "    sorted_clients = sorted(client_contributions.items(), key=lambda x: x[1], reverse=True)\n",
    "    least_contributing_clients = sorted_clients[-3:]\n",
    "    return sorted_clients, least_contributing_clients\n",
    "\n",
    "def calculate_weight_magnitude(global_model, client_update):\n",
    "    \"\"\"\n",
    "    Calculate the L2 norm of the weight difference between the global model and client's updated model.\n",
    "    \n",
    "    Args:\n",
    "    global_model (nn.Module): The global model before client update.\n",
    "    client_update (list): List of numpy arrays representing client's updated model parameters.\n",
    "\n",
    "    Returns:\n",
    "    float: The L2 norm of the weight difference.\n",
    "    \"\"\"\n",
    "    weight_diff = 0.0\n",
    "    global_parameters = [param.detach().cpu().numpy() for param in global_model.parameters()]\n",
    "    for global_param, client_param in zip(global_parameters, client_update):\n",
    "        weight_diff += np.linalg.norm(global_param - client_param)\n",
    "    return weight_diff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edfb4fa-2a88-48f9-a070-e42770855b74",
   "metadata": {
    "id": "2GrOePXWWEzi",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='Brown'>***Clients Functions***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe8ff6-416d-4e64-9fbf-441497bd4fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def General_Client():\n",
    "    def client_fn(cid: int, Round: int) -> FlowerClient:\n",
    "        clients_ids_list = TrainingListPerRound[Round]\n",
    "        if int(cid) in clients_ids_list:\n",
    "            net = Net().to(DEVICE)\n",
    "            trainloader = Dataloaders[Round][int(cid)]\n",
    "            arg_ = Round\n",
    "            return FlowerClient(cid, net, trainloader, arg_)\n",
    "        else:\n",
    "            raise ValueError(f\"Client ID {cid} not found in the list for round {Round}\")\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c63cf7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<font color='Brown'>***FL Strategy***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "590c9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Global_Models = {}\n",
    "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
    "    def __init__(self, additional_argument, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.additional_argument = additional_argument\n",
    "    def aggregate_fit(self, rnd: int, results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]], failures: List[BaseException]) -> Optional[fl.common.NDArrays]:\n",
    "        aggregated_parameters_tuple = super().aggregate_fit(rnd, results, failures)\n",
    "        aggregated_parameters, _ = aggregated_parameters_tuple\n",
    "        if aggregated_parameters is not None:\n",
    "            # print(f\"Saving round {rnd} aggregated_parameters...\")\n",
    "            # Convert `Parameters` to `List[np.ndarray]`\n",
    "            aggregated_weights: List[np.ndarray] = fl.common.parameters_to_ndarrays(aggregated_parameters)\n",
    "            # Convert `List[np.ndarray]` to PyTorch`state_dict`\n",
    "            Global_Models[self.additional_argument] = Net()\n",
    "            params_dict = zip(Global_Models[self.additional_argument].state_dict().keys(), aggregated_weights)\n",
    "            state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "            Global_Models[self.additional_argument].load_state_dict(state_dict, strict=True)\n",
    "            torch.save(Global_Models[self.additional_argument].state_dict(), f\"{PATH}/GlobalModel_{self.additional_argument}.pth\")\n",
    "        return aggregated_parameters_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e959fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_config(server_round: int):\n",
    "    \"\"\"Return training configuration dict for each round.\n",
    "    Perform two rounds of training with one local epoch, increase to two local\n",
    "    epochs afterwards.\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"current_round\": server_round,  # The current round of federated learning\n",
    "        \"local_epochs\": EPOCHS #1 if rnd < 2 else 2,  # \n",
    "    }\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d181247",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "***Running the Generalized FL Round***\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834689ad-179a-4cf7-860e-2ffb1008c6fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing / Loading Global Model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Ensure initial global model exists or load from checkpoint\u001b[39;00m\n\u001b[1;32m     10\u001b[0m Global_Models \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 12\u001b[0m start_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mPATH\u001b[49m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGlobalModel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTRT_ROUND\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(start_path):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResuming from checkpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PATH' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Loading Initial Global Model\")\n",
    "Global_Models[0] = Net()\n",
    "Global_Models[0].load_state_dict(torch.load(\"0_Input_Random_model_Net.pth\"))\n",
    "Global_Models[0].train()\n",
    "\n",
    "TrainingListPerRound = {}\n",
    "for Round in range(1, ROUNDS+1):\n",
    "    TrainingListPerRound[Round] = []     \n",
    "    for CLIENT in range (NUM_CLIENTS):\n",
    "        TrainingListPerRound[Round].append(int(CLIENT))\n",
    "\n",
    "for Round in range(1, ROUNDS+1):\n",
    "    print(\"Starting FL Round: \", Round)\n",
    "    strategy = SaveModelStrategy(\n",
    "            fraction_fit=1.0,  # Sample 100% of available clients for training\n",
    "            fraction_evaluate=0,  # Sample 50% of available clients for evaluation\n",
    "            min_fit_clients=2,  # Never sample less than 10 clients for training\n",
    "            min_evaluate_clients=0,  # Never sample less than 5 clients for evaluation\n",
    "            min_available_clients=2,  # Wait until all 10 clients are available\n",
    "            on_fit_config_fn=fit_config,\n",
    "            initial_parameters=fl.common.ndarrays_to_parameters(get_parameters(Global_Models[Round-1])),\n",
    "            additional_argument = Round\n",
    "    )\n",
    "    # print(f'Current training nodes at round {Round}: ', len(TrainingListPerRound[Round]), ' ', TrainingListPerRound[Round])\n",
    "    client_fn = General_Client()\n",
    "    fl.simulation.start_simulation(\n",
    "        client_fn=lambda cid: client_fn(cid, int(Round)),\n",
    "        num_clients=int(NUM_CLIENTS),\n",
    "        config=fl.server.ServerConfig(num_rounds=int(1)),\n",
    "        client_resources={\"num_cpus\":16, \"num_gpus\":1}, \n",
    "        ray_init_args = {'num_cpus': 16, 'num_gpus': 1},\n",
    "        strategy=strategy\n",
    "    )\n",
    "    print(\"End of FL Round: \", Round)\n",
    "    print(\"Loading Global Model: \", Round)\n",
    "    Global_Models[Round] = Net()\n",
    "    Global_Models[Round].load_state_dict(torch.load(f\"{PATH}/GlobalModel_{Round}.pth\"))\n",
    "    Global_Models[Round].train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555eb63-e297-4c7b-8e1b-5985d2cde0e9",
   "metadata": {
    "id": "l0PgqJTEwwWk",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='Grey'>***Performance Testing***</font>\n",
    "---\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9f39255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n",
      "Loading 20241215_3Epochs/GlobalModel_1.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_2.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_3.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_4.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_5.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_6.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_7.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_8.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_9.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_10.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_11.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_12.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_13.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_14.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_15.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_16.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_17.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_18.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_19.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_20.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_21.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_22.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_23.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_24.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_25.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_26.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_27.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_28.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_29.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_30.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_31.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_32.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_33.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_34.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_35.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_36.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_37.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_38.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_39.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_40.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_41.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_42.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_43.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_44.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_45.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_46.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_47.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_48.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_49.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_50.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_51.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_52.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_53.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_54.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_55.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_56.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_57.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_58.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_59.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_60.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_61.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_62.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_63.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_64.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_65.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_66.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_67.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_68.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_69.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_70.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_71.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_72.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_73.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_74.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_75.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_76.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_77.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_78.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_79.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_80.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_81.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_82.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_83.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_84.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_85.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_86.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_87.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_88.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_89.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_90.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_91.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_92.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_93.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_94.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_95.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_96.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_97.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_98.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_99.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_100.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_101.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_102.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_103.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_104.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_105.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_106.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_107.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_108.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_109.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_110.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_111.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_112.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_113.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_114.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_115.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_116.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_117.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_118.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_119.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_120.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_121.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_122.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_123.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_124.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_125.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_126.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_127.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_128.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_129.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_130.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_131.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_132.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_133.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_134.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_135.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_136.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_137.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_138.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_139.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_140.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_141.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_142.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_143.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_144.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_145.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_146.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_147.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_148.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_149.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_150.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_151.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_152.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_153.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_154.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_155.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_156.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_157.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_158.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_159.pth\n",
      "Loading 20241215_3Epochs/GlobalModel_160.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the directory and file pattern\n",
    "directory = PATH + '/'\n",
    "pattern = \"GlobalModel_*.pth\"\n",
    "\n",
    "# Find all matching files\n",
    "files = glob.glob(os.path.join(directory, pattern))\n",
    "\n",
    "# Extract numbers from file names\n",
    "numbers = []\n",
    "for file in files:\n",
    "    base_name = os.path.basename(file)\n",
    "    num_str = base_name.replace(\"GlobalModel_\", \"\").replace(\".pth\", \"\")\n",
    "    try:\n",
    "        numbers.append(int(num_str))\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "# Determine the maximum number\n",
    "max_num = max(numbers) if numbers else 0\n",
    "print(max_num)\n",
    "\n",
    "# Use the max_num in a loop\n",
    "for num in range(1, max_num + 1):\n",
    "    file_path = f\"{PATH}/GlobalModel_{num}.pth\"\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the file or perform any operation you need\n",
    "        print(f\"Loading {file_path}\")\n",
    "    else:\n",
    "        print(f\"File {file_path} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Q7O6Uj-at0L7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 364,
     "status": "error",
     "timestamp": 1670839612569,
     "user": {
      "displayName": "Mohannad Abu Issa",
      "userId": "09663279510051357174"
     },
     "user_tz": 300
    },
    "id": "Q7O6Uj-at0L7",
    "outputId": "e4d532e2-d5a1-4a0c-b486-f5975af6cd0e",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m loss_test \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      5\u001b[0m G \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[43mmax_num\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      8\u001b[0m     model \u001b[38;5;241m=\u001b[39m Net()\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/GlobalModel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_num' is not defined"
     ]
    }
   ],
   "source": [
    "pred_test = {}\n",
    "actual_test = {}\n",
    "accuracy_test = {}\n",
    "loss_test = {}\n",
    "G = 0\n",
    "\n",
    "for num in range(1, max_num+1):\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load(f\"{PATH}/GlobalModel_{num}.pth\"))\n",
    "    model.eval()\n",
    "    \n",
    "    prediction_matrix = []\n",
    "    actual_matrix= []\n",
    "    acc_matrix = []\n",
    "    loss_matrix=[]\n",
    "    G = G + 1\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in Dataloaders['Test']:\n",
    "            outputs = model(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            prediction_matrix.append(predicted.tolist())\n",
    "            actual_matrix.append(labels.tolist())\n",
    "    loss /= len(Dataloaders['Test'].dataset)\n",
    "    accuracy = correct / total\n",
    "    loss_matrix.append(loss)\n",
    "    acc_matrix.append(accuracy) \n",
    "\n",
    "    pred_test[f'Global_{G}'] = prediction_matrix\n",
    "    actual_test[f'Global_{G}'] = actual_matrix\n",
    "    accuracy_test[f'Global_{G}'] = acc_matrix\n",
    "    loss_test[f'Global_{G}'] = loss_matrix \n",
    "\n",
    "    filename = f'{PATH}/Global_{G}_pred'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(pred_test[f'Global_{G}'],outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    filename = f'{PATH}/Global_{G}_actual'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(actual_test[f'Global_{G}'],outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    filename = f'{PATH}/Global_{G}_accurracy'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(accuracy_test[f'Global_{G}'],outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    filename = f'{PATH}/Global_{G}_loss'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(loss_test[f'Global_{G}'],outfile)\n",
    "    outfile.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "bcoA2oGB6Kyi",
    "-hiMkZhLVT63",
    "Fg6LBRPK-E6_",
    "gjVnC-rj9gC4",
    "KNYdybJu5uhn",
    "LwpshLHduBDL",
    "Df92buFDvAl8",
    "D8dfI-OyvOBt",
    "s02Uob4Bvebm",
    "3H-0K0GXvuQo",
    "Y3LLG0dvwAFl",
    "thyLEKcswNst",
    "scNSl39rwhZu"
   ],
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
